# 工学博士学位论文

# 基于视觉导引的微小型无人机目标跟踪与飞行控制系统研究



# Research on Target Tracking and Flight Control System of Small UAV Based on Vision Guidance



# 目录

# 第一章绪论

1.1 研究背景与意义1.1.1 研究背景1.1.2 微小型无人机简介1.1.3 研究意义1.2 无人机视觉导引系统的应用场景1.2.1 民用场景1.2.2 军用场景1.3 基于视觉的目标检测与跟踪技术研究现状1.3.1 合作目标的检测与跟踪方法1.3.2 非合作目标的检测与跟踪方法1.3.3 基于深度学习的目标检测方法1.4 无人机低成本飞行控制系统研究现状1.4.1 基于多传感器融合的无人机位姿估计方法1.4.2 低成本开源飞行控制器发展现状1.4.3 无人机编队飞行控制系统研究进展1.5 论文研究内容与章节安排第二章 微小型无人机目标跟踪与飞行控制系统框架设计2.1 引言2.2 应用背景2.2.1 无人机低空精确打击目标任务2.2.2 目标跟踪流程2.3 摄像机选型与标定2.3.1 摄像机针孔模型2.3.2 镜头畸变模型2.3.3 光电成像传感器2.3.4 摄像机选型2.3.5 摄像机标定2.4 嵌入式图像采集处理系统2.4.1 图像采集与数据传输2.4.2 嵌入式图像处理系统2.5 基于机载视觉的目标跟踪系统框架设计2.5.1 功能定位2.5.2 硬件框架

# 2.5.3 软件框架 40

2.5.4 MAVLink 协议与外部控制模式 44

2.5.5 基于视觉的四旋翼无人机穿越圆环算例 45

2.6 小结 50

第三章 基于合作目标的微小型无人机自主降落系统研究 52

3.1 引言 52

3.2 基于合作目标的车载式微小型无人机降落引导系统设计 52

3.2.1 合作二维码的识别 52

3.2.2 基于嵌套二维码的降落板标志设计 56

3.2.3 车载式微小型多旋翼无人机降落引导系统设计 62

3.3 基于合作二维码标志的无人机位姿估计 65

3.3.1 无人机与降落板相对运动模型 65

3.3.2 二维码标志位姿估计方法 68

3.3.3 多二维码数据融合方法 70

3.4 四旋翼无人机动力学模型 71

3.4.1 坐标系与坐标转换 71

3.4.2 四旋翼无人机刚体动力学模型 72

3.4.3 螺旋桨理论模型 72

3.4.4 四旋翼的布局和控制效率模型 74

3.4.5 动力系统模型 75

3.4.6 气动阻力模型 76

3.5 四旋翼无人机控制系统 76

3.5.1 底层飞行控制框架 76

3.5.2 线性化模型 77

3.5.3 位置控制器 78

3.5.4 姿态控制器 79

3.6 四旋翼无人机自主降落飞行试验及结果分析 79

3.6.1 飞行平台和试验环境搭建 79

3.6.2 定点悬停试验与结果分析 82

3.6.3 自主降落试验与结果分析 83

3.7 考虑视角和落角约束的偏置比例导引律 85

3.7.1 捷联式摄像头视角和落角约束问题 85

3.7.2 偏置比例导引律 88

3.7.3 追踪地面移动平台模拟仿真 89

3.8 小结 93

第四章 基于深度学习的非合作目标并行检测与跟踪方法 95

4.1 引言 95

4.2 基于尺度自适应核相关滤波器的目标跟踪 95

4.2.1 相关滤波器基础理论 ..... 954.2.2 基于核相关滤波器的目标跟踪方法 ..... 974.2.3 尺度自适应的核相关滤波器 ..... 1024.3 基于深度学习的非合作目标检测算法 ..... 1064.3.1 基于改进YOLO的目标检测算法 ..... 1064.3.2 针对特定目标的训练和检测结果 ..... 1084.4 基于深度学习的并行检测与跟踪框架 ..... 1134.4.1 应用场景分析 ..... 1134.4.2 YOLO- MSKCF 并行检测与跟踪算法 ..... 1144.4.3 算法验证与结果分析 ..... 1164.5 小结 ..... 119第五章 基于机载视觉导引稳定平台的目标跟踪技术研究 ..... 1215.1 引言 ..... 1215.2 机载视觉导引稳定平台的动力学模型与控制系统 ..... 1215.2.1 稳定平台的结构形式 ..... 1215.2.2 坐标系定义与运动学关系 ..... 1225.2.3 稳定平台的跟踪控制 ..... 1275.2.4 运动目标自主跟踪策略 ..... 1295.3 运动目标自主跟踪系统与飞行试验验证 ..... 1315.3.1 基于三轴稳定平台的运动目标跟踪测试 ..... 1315.3.2 飞行试验系统搭建 ..... 1345.3.3 空中运动目标自主跟踪飞行试验 ..... 1355.4 基于机载视觉导引稳定平台的目标定位技术 ..... 1385.4.1 基于无人机多点测向与测距的目标定位方法 ..... 1385.4.2 地面移动目标跟踪与定位模拟仿真 ..... 1415.4.3 实物演示试验与结果分析 ..... 1435.5 小结 ..... 146第六章 微小型无人机编队飞行控制系统研究 ..... 1476.1 引言 ..... 1476.2 基于一致性理论的无人机编队飞行系统 ..... 1476.2.1 图论的相关知识 ..... 1476.2.2 无人机编队飞行运动学模型 ..... 1486.2.3 编队目标 ..... 1496.2.4 控制器设计 ..... 1516.2.5 仿真试验 ..... 1526.3 无人机编队队形保持控制方法 ..... 1536.3.1 无人机质心运动模型 ..... 1536.3.2 无人机纵向运动控制 ..... 155

6.3.3 无人机横向运动控制 1576.4 编队飞行试验与结果分析 1586.4.1 编队飞行试验系统搭建 1586.4.2 三机编队飞行试验与数据分析 1606.4.3 五机编队飞行试验与数据分析 1656.5 小结 169第七章 结论与展望 1717.1 论文主要研究成果 1717.2 论文主要创新点 1737.3 下一步工作展望 175参考文献 177

# 表目录

表2.1 摄像头参数 ..... 33表2.2 创建工作空间指令 ..... 41表2.3 ROS系统调试工具 ..... 42表2.4 MAVLink协议LOCAL_POSITION_NED(#32)指令 ..... 44表2.5 MAVLink协议SET_POSITION_TARGET_LOCAL_NED(#84)指令 ..... 45表3.1 降落板上方1m高度定点悬停的位姿估计均方根误差 ..... 83表3.2 制导律参数 ..... 90表3.3 仿真模型相关参数 ..... 90表3.4 跟踪非机动移动平台（状态一）的各制导方法对比 ..... 93表3.5 跟踪机动移动平台（状态二）的各制导方法对比 ..... 93表4.1 各目标跟踪方法性能的统计数据 ..... 102表4.2 算法运行环境 ..... 103表4.3 目标检测算法运行环境 ..... 110表4.4 网络训练参数设置 ..... 110表4.5 各目标检测精度指标统计 ..... 111表5.1 不同测试环境下稳定平台跟踪目标的均方根误差 ..... 133表5.2 与目标定位相关的主要传感器测量误差 ..... 142表5.3 多点测向测距数据以及目标位置估计值 ..... 144表6.1 五架无人机的初始状态 ..... 152表6.2 编队队形保持的位置控制精度 ..... 169

# 图目录

图 1.33 FLIR D300 转台 ..... 11图 1.34 “H” 图案降落板 ..... 12图 1.35 摄像机视角 ..... 12图 1.36 无人直升机自主降落在标志上 ..... 12图 1.37 降落板布置在车顶 ..... 12图 1.38 无人机跟踪并自主降落在预定目标上 ..... 12图 1.39 一些基于合作标志的降落板 ..... 13图 1.40 几类常用的合作二维码标志 ..... 13图 1.41 由四个红外点组成的图案 ..... 14图 1.42 可发射红外光的合作目标 ..... 14图 1.43 利用机载视觉系统引导无人机自主着舰及其实验设备 ..... 15图 1.44 主流开源飞行控制器 ..... 19图 1.45 美国国防部测试无人机蜂群作战系统 ..... 20图 1.46 论文组织结构图 ..... 24图 2.1 一种基于多无人机协同的低空精确打击地面目标任务 ..... 26图 2.2 一种基于微小型无人机的空面精确打击系统 ..... 27图 2.3 无人机目标跟踪典型任务过程 ..... 28图 2.4 无人机执行目标跟踪任务各阶段转换关系 ..... 29图 2.5 摄像机相关坐标系定义 ..... 30图 2.6 低成本摄像头 ..... 32图 2.7 自制的棋盘格标定板 ..... 34图 2.8 选择坐标系原点 ..... 34图 2.9 摄像机（固定）与靶标的几何关系 ..... 35图 2.10 摄像机与靶标（固定）的几何关系 ..... 35图 2.11 平均投影误差 ..... 35图 2.12 摄像机图像采集、存储、降噪和边缘检测算例 ..... 36图 2.13 基于 ARM- Linux 的嵌入式图像处理系统软硬件框架 ..... 37图 2.14 基于 NVIDIA Jetson TX2 的机载计算机 ..... 38图 2.15 无人机目标跟踪系统硬件框架 ..... 39图 2.16 四旋翼无人机飞行平台 ..... 40图 2.17 ROS 系统实现的层级架构 ..... 40图 2.18 文件系统级的 ROS 架构 ..... 41图 2.19 计算图级的 ROS 架构 ..... 42图 2.20 基于 ROS 设计的无人机目标跟踪系统软件框架 ..... 43

图2.21 基于ROS/Gazebo的四旋翼穿越圆环仿真算例节点与话题关系图 46图2.22 Gazebo环境下四旋翼无人机穿越圆环目标过程 48图2.23 无人机穿越圆环算例仿真数据 50图3.1 来自不同字典的ArUco码 53图3.2 合作ArUco码内部编码示意图 53图3.3 合作ArUco码的检测与识别算法流程 54图3.4 部署在机载计算机中的ArUco码算法实际运行过程 55图3.5 单个ArUco码在不同距离下的检测结果 57图3.6 基于嵌套ArUco码的降落板标志设计 58图3.7 降落板标志在摄像机视野中的可视范围 59图3.8 室外环境中降落板在不同距离下的检测结果 61图3.9 室内环境中降落板在不同距离下的检测结果 62图3.10 多旋翼无人机自主降落过程 63图3.11 多旋翼无人机自主降落引导系统控制流程 64图3.12 车载式多旋翼无人机自主降落系统节点关系图 65图3.13 无人机与降落板相对运动关系和相关坐标系定义 66图3.14 降落板中心点与各ArUco码的位置关系 67图3.15 已知4个共面点对（3D- 2D） 68图3.16 APC1046桨拉力曲线与实验数据对比 73图3.17 APC1046桨转矩曲线与实验数据对比 73图3.18 四旋翼的布局形式 74图3.19 四旋翼动力系统从遥控器指令到拉力和力矩的信号传递 75图3.20 四旋翼底层飞行控制的闭环结构 77图3.21 室内环境下的无人机飞行试验平台搭建 80图3.22 室内飞行试验系统硬件架构 81图3.23 室内环境下的无人机自主降落试验 82图3.24 无人机定点悬停于降落板正上方1m高度时的飞行试验数据 82图3.25 无人机从1.9m高度进行自主降落的飞行试验数据 84图3.26 无人机进行自主起飞、悬停、移动、降落全过程的飞行试验数据 84图3.27 采用捷联式摄像头的无人机与目标相对运动关系 85图3.28 采用纯追踪法和真比例导引法的仿真结果 87图3.29 无人机追踪非机动移动平台的仿真结果 91图3.30 无人机追踪机动移动平台的仿真结果 92图4.1 通过循环变化产生的样本 98

图 4.2 KCF 跟踪器与其它主流跟踪器的性能对比（OTB- 2013） 102图 4.3 目标尺度预测 103图 4.4 CSK（蓝色框）、KCF（绿色框）与 MSKCF（红色框）在 CarScale 视频上的测试结果 104图 4.5 MSKCF、CSK、KCF 跟踪器性能对比（OTB- 2013） 105图 4.6 28 个尺度变化视频序列的 MSKCF、CSK、KCF 跟踪器成功率指标对比 106图 4.7 YOLOv4- CBAM 算法网络框架 107图 4.8 CBAM 模块 107图 4.9 通过网络下载和数据增强得到的训练样本 108图 4.10 采用 Labeling 软件标注样本图片 109图 4.11 标注生成的配置文件 109图 4.12 对特定目标检测的 P- R 曲线 111图 4.13 针对特定目标的 YOLOv4 与 YOLOv4- CBAM 检测效果对比 112图 4.14 传统目标跟踪算法流程 113图 4.15 YOLO- MSKCF 并行检测与跟踪框架 114图 4.16 在 ROS 中搭建的 YOLO- MSKCF 并行检测与跟踪算法框架 116图 4.17 静态多目标检测与跟踪测试 117图 4.18 基于 YOLO- MSKCF 的运动目标跟踪结果 118图 4.19 基于 KCF 算法的运动目标跟踪结果 119图 5.1 典型的稳定平台结构形式 122图 5.2 三轴稳定的机载视觉导引平台系统 123图 5.3 稳定平台相关部件内部结构 123图 5.4 三轴稳定平台相关坐标系定义 124图 5.5 惯性空间中的失调角示意图 127图 5.6 机载视觉导引稳定平台的工作流程 128图 5.7 机载视觉导引稳定平台的控制系统架构 128图 5.8 无人机与目标相对运动关系 129图 5.9 无人机自主跟踪策略 130图 5.10 地面支架搭载稳定平台对四旋翼无人机目标进行跟踪测试 131图 5.11 利用无人机搭载稳定平台对慢速移动的行人目标进行跟踪测试 132图 5.12 间歇性快速移动目标跟踪测试数据 132图 5.13 慢速移动行人目标跟踪测试数据 132图 5.14 四旋翼无人机目标跟踪测试数据 133图 5.15 本文搭建的全自主目标跟踪飞行试验平台 134

图5.16 自主跟踪飞行试验平台主要机载设备 134图5.17 无人机目标跟踪程序的ROS节点与话题关系 135图5.18 针对空中运动目标的无人机全自主跟踪飞行试验 136图5.19 无人机与稳定平台姿态、无人机飞行速度相关数据 137图5.20 无人机跟踪目标的飞行轨迹 138图5.21 基线长度对三角测量的影响 139图5.22 利用机载视觉导引稳定平台对目标进行搜索、锁定、定位和抓取的应用场景[243] 139图5.23 基于机载视觉导引稳定平台多点测向与测距的目标定位原理 140图5.24 地面运动目标跟踪与定位结果 143图5.25 自主搭建的微小型机载视觉导引稳定平台 143图5.26 多点测向与测距户外试验 144图5.27 地面运动目标跟踪与定位结果 145图6.1 图论中的有向图和无向图 148图6.2 五架无人机组成的通信拓扑图 149图6.3 无人机编队期望队形示意图 151图6.4 五架无人机编队飞行仿真试验结果 153图6.5 无人机纵向运动受力分析 153图6.6 领航者与跟随者相对运动状态 155图6.7 L1 制导律示意图 157图6.8 根据前后航点计算制导指令 158图6.9 用于编队飞行试验的小型固定翼无人机 159图6.10 编队飞行试验系统硬件框架 159图6.11 编队飞行队形变换示意图 160图6.12 三机编队飞行试验过程 160图6.13 三机编队飞行试验结果 163图6.14 三机编队飞行试验时的编队位置误差 164图6.15 五机编队飞行试验过程 165图6.16 五机编队飞行试验结果 167图6.17 五机编队飞行试验时的编队位置误差 168

# 摘要

近年来，微小型无人机凭借其体积小、重量轻、易于携带、效费比高等特点广泛应用于局部军事冲突、反恐乱、交通监控、电力巡检、应急救援等领域。在众多实际应用中，对用户感兴趣的特定目标进行搜索和持续跟踪已经成为微小型无人机最重要的任务之一。微小型无人机目标跟踪系统就是利用机载视觉传感器获取图像，通过跟踪算法锁定目标并将目标位置等信息提供给制导控制系统，从而辅助无人机完成对目标的打击等任务。由于微小型无人机的尺寸和重量都受到严格约束，只能搭载微型化、轻量化和低功耗的机载设备，因而导致其计算资源非常有限，这对于系统集成度和算法效率提出了更高的要求。本文以微小型无人机目标跟踪与飞行控制系统为研究对象，对合作目标识别、无人机位姿估计、长时间目标跟踪、目标定位、编队飞行控制等相关技术进行了深入研究，主要研究内容和成果如下：

建立了满足微型化、轻量化和低功耗要求的微小型无人机目标跟踪与飞行控制系统框架。对目标跟踪系统的应用场景进行分析，提出了一种基于多无人机协同的低空精确打击地面目标任务。对机载摄像机、图像处理系统、飞行控制器等设备进行选型和分析，提出了一种面向实际应用的无人机机载视觉导引与目标跟踪系统软硬件框架。在采用低成本摄像头、微型机载计算机和低成本开源控制器的基础上，搭建了一套完整的微小型无人机飞行试验平台及仿真环境，最后通过无人机穿越圆环目标的仿真算例验证了该系统的可行性。

提出了一种基于合作二维码标志的微小型无人机自主降落导引系统。针对微小型无人机自主降落问题，提出了一种通过优化降落板图案来提高自主降落精度的方法。对合作二维码目标位姿估计方法进行了研究，搭建了室内环境下的飞行试验系统并成功实现了对地面平台的自主降落。建立了一种适用于车载式微小型无人机自主降落的任务流程和系统框架，提出了考虑视角和落角约束的偏置比例导引律，仿真试验结果证明了该导引律满足任务要求。

提出了一种基于深度学习的非合作目标并行检测与跟踪算法。在长时间跟踪非合作运动目标的过程中，传统目标跟踪算法易受背景干扰，在目标存在机动或发生形变时容易出现跟踪失败、跟踪漂移以及目标尺度估计不准确等问题。针对长时间目标跟踪任务，深入研究了核相关滤波器的基本原理，建立了基于尺度预测的非合作目标跟踪算法，并通过公开的数据集进行了测试和对比分析。研究了基于卷积神经网络的目标检测方法，提出了非合作目标并行检测与跟踪框架，通过静态目标和运动目标跟踪测试验证了该算法的可行性，提高了复杂环境下微小型无人机对非合作目标的跟踪效果。

提出了一种基于三轴稳定平台的空中运动目标自主跟踪方法和基于无人机多点测向与测距的目标定位方法。以无人机跟踪运动目标为应用背景，对机载视觉

导引稳定平台的结构形式、运动学关系和动力学模型进行了深入研究，采用低成本无刷电机、编码器和单片机自主设计并搭建了微小型三轴稳定平台。研究了稳定平台的跟踪控制系统，提出了基于纯追踪法和比例导引律的无人机目标跟踪策略，成功开展了空中运动目标自主跟踪飞行试验。提出了一种基于无人机空中多点测向与测距的目标定位方法，在户外环境下对该方法进行了试验验证，结果表明其具有较强的工程实用性。

建立了基于一致性理论的多无人机编队飞行控制系统和微小型无人机队形保持控制系统。深入研究了分布式一致性理论，提出了一种适用于微小型无人机的编队队形和通信拓扑结构，设计了多无人机编队飞行协同控制算法，通过仿真试验证明了该算法可以使得编队实现预期的目标。针对微小型无人机编队的队形保持控制问题，基于总能量控制方法和L1制导律分别对无人机的纵向和横向轨迹跟踪进行了深入研究，基于Pixhawk开源飞行控制器搭建了无人机编队飞行试验系统，开展了三机和五机编队飞行试验，并对飞行数据进行分析，结果表明该系统具有较高的编队稳定性和位置控制精度。

关键词：微小型无人机；机载视觉；目标跟踪；自主降落；编队飞行控制。

# Abstract

In recent years, with the characteristics of small size, light weight, easy to carry and high efficiency cost ratio, micro UAV is widely used in military conflict, counter- terrorism operation, traffic monitoring, power inspection, emergency response and other fields. In many practical applications, searching and tracking specific targets of interest to users has become one of the most important tasks of micro UAV. The micro UAV target tracking system uses the onboard vision sensor to obtain the image, locks the target by using the tracking algorithm, and provides the target position and other information to the guidance and control system, so as to assist the UAV to complete the tasks of attack. Because the size and weight of micro UAV are strictly constrained, it can only carry miniaturized, lightweight and low- power onboard equipment. So the onboard computing resources are very limited, which puts forward higher requirements for system integration and algorithm efficiency. To solve the task of target tracking and flight control based on onboard vision, this paper studies on cooperative target recognition, UAV pose estimation, long- time target tracking, target positioning, formation flight control and other related technologies. The main research contents and results are as follows:

A micro UAV onboard vision guidance and target tracking system framework is established to meet the requirements of miniaturization, lightweight and low power consumption. Based on the analysis of the actual application scenario of micro UAV target tracking, a precision attack task based on multi UAV cooperation is proposed. The onboard camera, image processing system, flight controller and other equipment are selected and analyzed, and a software and hardware framework of UAV visual guidance and target tracking system for practical application is proposed. Based on the use of low- cost camera, microcomputer and low- cost open source controller, a complete flight test and simulation environment of micro multi rotor UAV are built. Finally, the feasibility of the system is verified by the simulation of UAV passing through a ring target.

A micro UAV autonomous landing system based on cooperative QR code is proposed. According to the actual mission requirements of landing the UAV on the recovery platform, a method to improve the landing accuracy through the optimal design of landing pad is proposed. The position and attitude estimation method by using cooperative QR code target is deeply studied. The flight test system in indoor environment is built, and the autonomous landing on the ground fixed platform is successfully realized. The biased proportional guidance law considering angle of view and angle of fall constraints is studied, and a task flow and framework suitable for autonomous landing of micro UAV are proposed.

Parallel target detection and tracking algorithm based on deep learning is proposed. In the process of tracking non- cooperative moving targets for a long time, the traditional target tracking algorithm is vulnerable to background interference. When the target is maneuvering or deformed, it may fail, drift or provide inaccurate target scale estimate. For the long- time target tracking task, the basic principle of kernel correlation filter is deeply studied, and a kernel correlation filter target tracking algorithm based on scale adaptation is established. The actual operation effect of the algorithm is tested through the public data set. The target detection method based on convolutional neural network and large number of sample training is studied, and a non- cooperative target parallel detection and tracking framework based on deep learning is proposed. The feasibility of the algorithm is verified by static target and moving target tracking tests, which improves the tracking effect of micro UAV on non- cooperative targets in complex environment.

An autonomous tracking method of aerial moving target based on three- axis stabilized gimbal system and a target positioning method are proposed. Based on the application background of UAV tracking moving targets, the structure, kinematics and dynamic model of onboard visual guidance platform are studied. A micro three- axis stabilized platform is independently designed and built by using low- cost brushless motor, encoder and microcontroller, and the control system of the platform is studied. The UAV target tracking strategy based on pure pursuit method and proportional guidance law is proposed. The autonomous tracking flight test of aerial moving target is successfully carried out, and a target positioning method is proposed. The method is tested and verified in outdoor environment, and has strong engineering practicability.

A multi UAV formation control system based on distributed consensus theory and a micro UAV formation maintenance control system are established. The distributed consensus theory is studied. A formation and communication topology suitable for micro UAVs is proposed, and a multi UAV formation flight control algorithm is designed. The simulation results show that the algorithm is able to achieve the expected goal. Aiming at the formation maintenance control of micro UAV formation, the trajectory tracking of UAV is studied based on the total energy control method and L1 guidance law. The formation flight test system is built based on Pixhawk open source flight controller. The autonomous flight test of three and five UAVs is carried out, and the flight data are analyzed. The results show that the system has high formation stability and control accuracy.

Key words: Micro UAV, Onboard Vision, Target Tracking, Autonomous Landing, Formation Flight Control.

# 第一章 绪论

# 1.1 研究背景与意义

# 1.1.1 研究背景

无人驾驶飞行器简称无人机（UnmannedAerialVehicle，UAV），在过去20年里已经成为许多行业应用中至关重要的角色，其在未来军事作战中有着极其重要的作用，目前也是世界各国科研机构和工业部门大力发展的技术领域[1- 3]。无人机的历史最早可以追溯到1917年，英国的两位将军向英国军事航空学会提出无人机的概念，成立了“空中目标”（AerialTarget，AT）项目组，由ArchibaldLow主要负责，对装有无线电遥控系统的飞机进行了多次试验，这是世界第一架有动力且可控制的无人机，距今已有百余年[4]。在相当长的一段时间内，由于通信、导航与控制等技术不成熟，无人机并没有得到广泛的应用。20世纪后期，精确制导武器和空中力量的应用在越南战争、海湾战争和科索沃战争中起到了决定性作用[5- 8]，世界军事领域也兴起了一场深刻的变革，称为"新军事变革"(Revolutionin MilitaryAffairs,RMA）[9- 11]，传统战争理论已不再适应未来以信息技术为核心的作战方式，而无人机正是在这样的时代背景下逐渐在军事领域崭露头角，至今已成为世界各国军队最关注的武器装备之一[12]。

最初，无人机主要作为靶机在训练防空部队时使用，并用于执行侦察、电子干扰、诱骗等作战任务[13]。随着微机电系统（Micro- Electro- Mechanical System,MEMS）[14]、全球导航卫星系统（Global Navigation Satellite System,GNSS）[15]等领域的技术突破，无人机的潜在能力被不断挖掘。2012年，位于中国深圳的大疆创新科技有限公司发布了世界首架一体式航拍无人机（Phantom1）[16]，从而拉开了民用无人机发展的序幕。目前，无人机在民用领域已广泛应用于影视航拍、航空测绘、地质勘探、搜索救援（SearchandRescue，SAR）、农林植保领域，并正在逐步探索使得无人机电力巡检、交通监控、物流运输、森林防火等一系列新型应用成为现实[17- 26]，无人机领域的创新将持续不断地推动许多传统行业向高质量和高效率的发展模式转型升级。

随着人工智能（ArtificialIntelligence，AI）[27,28]领域加速发展并与无人机紧密结合，在同时定位与构图（SimultaneousLocalization andMapping,SLAM）[29]、无人机集群编队飞行[30]、在线路径规划[31]等研究方向上都取得了丰硕的研究成果。在军事领域，受战争的牵引和人工智能技术的推动，以无人机为代表的无人作战系统正在向自主化、智能化和协同化的方向快速发展[32]。2020年1月3日，美军

在巴格达的一次空袭行动中，利用无人机发射导弹，将伊朗伊斯兰革命卫队少将苏莱曼尼及其保镖乘坐的两辆汽车击毁，苏莱曼尼当场身亡[33- 35]。参与此次暗杀行动的MQ- 9无人机在数千米高空飞行，其携带4枚精确制导导弹，并配备了多种目标探测、跟踪和监视设备，可以在最佳时机对目标进行打击[36,37]。

近年来，在利用无人机对特定目标进行“外科手术式”打击的军事行动中，视觉导引技术发挥了重要的作用。视觉导引技术通过计算机对目标图像进行处理，并将无人机引向指定目标所在位置或与目标相隔一定距离的某个位置，从而实现对目标的侦察和打击等。目前，在无人机自主降落[38- 40]、目标识别与跟踪[41]、无人机“自杀式”攻击[42]等任务中，视觉导引都是关键的技术环节，也是目前国内外无人机领域研究的重点问题[43]。

# 1.1.2 微小型无人机简介

微型无人机（MicroAerialVehicle，MAV）和小型无人机（SmallUnmannedAerialVehicle，SUAV）的发展最早由美国国防高级研究中心（DefenseAdvancedResearchProjectsAgency，DARPA)在20世纪90年代末至21世纪初提出并逐步重视起来[44]。受益于微处理器[45]、惯性测量单元（InertialMeasurementUnit，IMU）以及其它机载传感器朝着微型化、轻量化的方向加速发展，无人机在尺寸和重量方面得以极大地减小[46]。其中小型无人机的最大尺寸一般在  $3\mathrm{m}$  以内，重量在  $50\mathrm{kg}$  以内；而微型无人机的最大尺寸一般小于  $0.5\mathrm{m}$  ，重量小于  $2\mathrm{kg}$  。在过去20年里，微小型无人机以其成本低、重量轻、体积小、便于携带、起降灵活、响应快速等特点，在军用和民用领域中都有着不可替代的竞争优势，得到了持续的发展并取得了许多技术突破[47]。

根据飞行原理的不同，微小型无人机一般可分为微小型扑翼机、微小型旋翼无人机和微小型固定翼无人机[48]。

微小型扑翼机是一种模仿鸟类、蝴蝶、蜜蜂等生物的飞行原理而研制的超微型无人机，其具有隐蔽性好、噪音小等特点，例如荷兰代尔夫特理工大学研究团队设计的DelFlyMicro，重量仅  $3\mathrm{g}$  ，翼展仅  $10\mathrm{cm}^{[49]}$  ；法国BionicBird团队推出的MetaFly 消费级扑翼机，重量仅  $10\mathrm{g}$  ，可用手机遥控飞行  $8\mathrm{min}$  ，最远控制距离为 $100\mathrm{m}^{[50]}$  ；美国哈佛大学研究团队设计的RoboBee，翼展仅  $8\mathrm{cm}$  ，是世界上最小的仿昆虫无人机[51]；伊利诺伊大学研究团队研制的BatBot无人机，其高度模仿蝙蝠的形态特征，重量仅  $93\mathrm{g}^{[52]}$  0

微小型旋翼无人机一般以单个或多个电机带动螺旋桨转动而提供飞行动力，例如由挪威ProxDynamics公司设计的重量仅  $16\mathrm{g}$  的PD- 100BlackHornet微型直升机，在阿富汗战场被英军用于执行侦察任务[53]；美国斯坦福大学研究团队设计的

Mesicopter微型四旋翼无人机，其尺寸接近一枚硬币[54]；加拿大AeryonLabs公司的Scout小型侦察四旋翼无人机，总重  $1.3\mathrm{kg}$ ，有效载重约  $250\mathrm{g}$ ，可以在  $3\mathrm{km}$  范围内作业，续航时间为  $25\mathrm{min}^{[55]}$ ；以色列的Elbit Systems防务公司推出的MAGNI小型战术四旋翼无人机，重量小于  $3\mathrm{kg}$ ，可搭载可见光和红外相机等机载微型设备，可配置给排、连级部队以及单兵使用[56]。

微小型固定翼无人机依靠机翼提供升力，具有较快的飞行速度和较好的巡航经济性，可以携带更大的载荷，一般采用滑跑、手抛、弹射等方式进行起飞，例如美国AeroVironment公司的RQ- 11Raven无人机就是一种美国士兵可以手抛起飞的小型战术无人机，起飞重量约  $1.9\mathrm{kg}$ ，工作半径约  $10\mathrm{km}$ ，可在战场快速部署并执行侦察等任务[57]；以色列航空工业公司设计的Bird- Eye400无人机，起飞重量约  $4.1\mathrm{kg}$ ，有效载重约  $1.2\mathrm{kg}$ ，翼展约  $2.2\mathrm{m}$ ，作战半径达  $15\mathrm{km}$  并可以持续留空  $80\mathrm{min}^{[58]}$ ；瑞士SenseFly公司设计的eBeeX测绘无人机，采用飞翼布局，主要结构为泡沫，翼展约  $1.16\mathrm{m}$ ，起飞重量仅  $1.1\mathrm{kg}$ ，巡航时间达  $90\mathrm{min}$ ，单次测绘面积可达  $40\mathrm{km}^{[59]}$ ；法国LehmannAviation公司生产的L- A系列无人机，主要采用碳纤维和EPP泡沫材料制成，可手抛起飞并进入预定航线，主要应用于高精度测绘等领域[60]。以上介绍的微小型无人机如图1.1所示。

![](images/7ae26f0789dc4bac43578589853a7fd75340541a25e714bf307ac9d45a798f9e.jpg)  
图1.1一些典型的微小型无人机

# 1.1.3 研究意义

微小型无人机的工作环境复杂多变，在执行目标跟踪任务时，微小型无人机需要实时地感知飞行环境和搜索用户感兴趣的目标，在获得目标信息后，自主地决策并完成对目标的锁定、指示等。众多实际应用对无人机机载视觉导引与飞行控制系统的智能化水平提出了更高的要求。目前主要面临的挑战如下：

（1）相比中大型无人机，微小型无人机的机身体积大幅缩小，重量也大幅减轻，其任务载荷在重量和尺寸方面都受到严格限制。因此，微小型无人机机载设备如摄像机、图像处理器、伺服机构、控制器等必须朝微型化、轻量化和低功耗的方向设计。在微小型无人机总体指标的约束下，如何设计满足实时目标跟踪任务要求的机载视觉导引系统软硬件框架，仍面临较大挑战。

（2）微小型无人机采用的低成本传感器在精度、可靠性等方面与精密传感器存在较大差距，比如其导航系统基于MEMS的陀螺仪、加速度计、磁力计等传感器，随时间、温度等的变化会产生较大的漂移量，特别是在全球导航卫星系统拒止环境下，微小型无人机的定位结果会出现较大偏差。此外，机载摄像机采用的CMOS传感器一般比较廉价，存在图像背景噪声大、分辨率低、灵敏度低等不足。面对实际跟踪任务，在传感器精度受限的情况下，如何提高无人机飞行控制精度并实现预期的目标跟踪效果，仍存在较大技术难度。

（3）在执行非合作目标跟踪任务时，传统跟踪算法极其容易出现跟踪发散、跟踪失败等异常情况。这些情况一般发生在画面中目标产生形变、被遮挡、较大机动或执行长时间跟踪任务时。由于缺少目标先验信息，传统跟踪算法在出现跟踪异常后，也无法有效地找回丢失的目标。如何提高长时间目标跟踪的精度、稳定性，或者在跟踪出现异常时如何重新校正或初始化跟踪器，以保持对目标的实时锁定，这些仍是实际任务中亟待解决的问题。

（4）在执行对运动目标的跟踪任务时，目标位置快速变化且运动状态未知，如果只考虑图像层面的跟踪，当目标超出摄像机的探测距离或者移动到视野盲区时，则会导致跟踪任务失败。如何在传统方法的基础上改进无人机跟踪策略，快速锁定目标并生成自主跟随目标运动的飞行控制指令，使无人机与目标保持期望的相对位置关系，仍然是当前需要解决的问题。此外，微小型无人机的机载计算能力十分有限，若要满足实际跟踪任务要求，还需进一步提升算法的效率、实时性以及系统响应速度等。

（5）单架微小型无人机的能力十分有限，需依靠多架微小型无人机协作完成复杂的任务。数量众多的微小型无人机如果不能有效地组织，则无法实现预期效果甚至会发生空中相撞等情况。在面对实际任务时，众多微小型无人机之间的协作模式仍缺乏进一步研究。针对快速飞行的微小型无人机编队，如何在通信质量

和导航精度受限的情况下，提高编队飞行控制精度，从而实现预期的编队队形和编队效果，仍具有较大的难度。

为解决上述问题，本文从合作目标跟踪与非合作目标跟踪两个方面来研究微小型无人机目标跟踪问题。在传感器精度受限的条件下，提出了一种多层级嵌套图案的合作降落板目标，能够实时地估计无人机六自由度相对位姿，并辅助微小型无人机在全球导航卫星系统拒止环境下实现了高精度的自主降落。研究了基于深度学习的目标检测和尺度自适应的目标跟踪算法，解决了在长时间目标跟踪过程中容易出现的跟踪发散和失败等问题。提出并设计了一种基于低成本器件的机载视觉导引稳定平台，用于辅助无人机完成自主降落和目标定位、抓取、追捕等实际任务。针对微小型无人机编队队形保持问题，研究了基于低成本控制器的编队飞行控制系统，并实现了预期的编队飞行效果。上述研究成果可以应用于众多军用和民用领域中，本文选题方向的研究意义重大，相关技术成果具有较高的工程应用价值。下面将针对无人机视觉导引系统的应用场景进行详细的介绍。

# 1.2 无人机视觉导引系统的应用场景

# 1.2.1 民用场景

包裹投送是当前无人机在民用领域的主要应用场景之一，一些物流企业纷纷开始研发无人机系统来解决“最后一公里”运输问题。而仅仅依靠卫星导航系统的定位结果无法满足无人机在城市中飞行的需要，容易出现定位精度差、数据不稳定以及信号被干扰等情况，如何将包裹精准地投送到指定区域是目前需要重点解决的问题。亚马逊公司[61]，在其Prime Air项目中提出了用合作标志来引导运输包裹的无人机降落，如图1.2和图1.3所示。这些标志经过特殊设计后，可以被唯一地识别，具有较高的可靠性和鲁棒性。图1.4为京东公司的无人机成功降落在背景是红色区域的“H”标志上，随后工作人员从其机身下方取出快递包裹[62]。图1.5为顺丰公司的无人机运输物资后，精准地降落在预先布置好的圆环状图案区域内[63]。

![](images/9f6924e08ecfc9b7fcd439a0050d5f33392396c14fa2cbcaa01b13235bc28fb4.jpg)  
图1.2亚马逊PrimeAir多旋翼无人机

![](images/effcde69b49e6c39f8b7a1e47ce8aab4946af2ef2256d676709893c2a5df0421.jpg)  
图1.3亚马逊PrimeAir垂直起降无人机

![](images/e1b10b51669836949847694515a6e2438b4ae3bee14470b5e780b736cfe0275f.jpg)  
图1.4 京东无人机投递包裹

![](images/39dd0ff37125f69fc21a5142be399065c083db97566a69f22094555db80b68db.jpg)  
图1.5 顺丰无人机运输医疗物资

微小型无人机由于其起飞重量小，不能携带大容量的电池，因此续航时间非常有限，一般在  $15\sim 30\mathrm{min}$  以内。在执行任务的过程中，需要多次手动更换电池，导致作业效率低。国内某公司提出了固定式无人机自动机场和车载式无人机自动机场，从而解决无人机充电换电问题，如图1.6、图1.7、图1.8和图1.9所示[64]。该自动机场内置有小型的停机坪，可以为无人机提供起降平台，通过GNSS实时动态载波相位差分定位技术（Real- time kinematic, RTK）和视觉辅助的自主起降技术对无人机进行回收，在装置内更换电池并对电池进行充电，对延长无人机的作业时间和提高工作效率起着重要的作用。

![](images/e30233a7fe81acbf0a4f80b12218e5ba5c29c020a8db32762032a6cdcce560a0.jpg)  
图1.6 固定式无人机自动机场

![](images/0009cf43af6a7ab022fdbfa74390fbcccc505dac00e4b54d607f98a84cb17f34.jpg)  
图1.7 车载式无人机自动机场

![](images/101667d6a4fe5cc4762c26de4f0147252331b6445bee91268c5e02418931349d.jpg)  
图1.8 无人机自动机场用于电力巡检

![](images/9ec9a5989516abd0667cc39f6d63e2ff4dd882cb95d5b31f2ffb8757b6b66372.jpg)  
图1.9 无人机自动机场用于海事巡逻

无人机室内自主飞行，一直以来都是行业内研究的热点问题。VICON光学运动捕捉系统利用多个相机对目标物体进行拍摄，来解算目标在室内的位置，可实现厘米级精度[65]。VICON系统提供的数据通常作为最高精度级别的位置和角度信

息用于完成科研实验[56, 67]，图 1.10 和图 1.11 分别为基于 VICON 系统的无人机室内自主飞行和多无人机协同搬运任务的场景。

![](images/8e7c63831960a3cc1dbe0a26444813c2eb79863a916c5f3f85ab9dad8d2017c4.jpg)  
图 1.10 基于 VICON 的无人机室内飞行

![](images/88b461acffca71a912d228cb44a385a7d64d6e9f51bdc1c51627d05ebf840c5a.jpg)  
图 1.11 基于 VICON 的无人机协同搬运

一些研究学者将视觉和惯性导航系统的信息融合，从而实现室内自主定位和无人机的飞行控制。其中，Bumblebee[68]、RealSense T265[69]等视觉开发套件得到大量应用，如图 1.12 和图 1.13 所示。Shen 等人[70]和 Carrillo 等人[71]分别利用单目和双目视觉对室内导航问题进行了研究，并搭建了无人机飞行试验平台来验证算法，如图 1.14 和图 1.15 所示。

![](images/ad68cadaab7b5a903e7e37f556cef988c3b9ec4074be7b257533b5ea621ecad8.jpg)  
图 1.12 PointGrey 公司的 Bumblebee 相机

![](images/67970e1c2ff96e297c3cf87c6c3a935e743d960abe0b0d15fc9037a8e3ef22f5.jpg)  
图 1.13 Intel 公司的 T265 产品

![](images/69f6239b9fcafded3b4912d6cd88bdcc266f7e77a47b4213b877e4e1b325f50f.jpg)  
图 1.14 基于单目视觉 SLAM 的室内飞行

![](images/ddfd4b4b35f5f52d6b07db108e5fabdc76b824060c7830cfba19eba68edae564.jpg)  
图 1.15 基于双目视觉的室内导航

城市高层建筑一旦起火形成“烟囱效应”，会使火灾加速蔓延，因此高层建筑的灭火救援一直以来都是世界性的难题。针对该问题，国内外多个研发团队提出多旋翼无人机携带灭火弹的方案，利用机载视觉系统自动地瞄准失火房间，精准地将灭火弹发射至建筑内，从而可以有效地完成高层建筑的灭火任务。图 1.16 和图 1.17 是目前国内某研究单位研制的灭火无人机平台以及该无人机发射灭火弹的场景。

![](images/fb0a7d9b63d9747a2c1b95a3e87f32d1e993299d0a6828d98af289c167597e00.jpg)  
图1.16 灭火无人机

![](images/c408589db6d511d167781934f9ba17a8064cd14d9c68b29d0c604df5a1c51ac3.jpg)  
图1.17 无人机发射灭火弹

# 1.2.2 军用场景

近年来，军事领域对无人机的需求更为迫切。AeroVironment公司于2008年开始为美国陆军提供Switchblade（“弹簧刀”）巡飞弹，如图1.18所示。与传统的导弹不同的是，“弹簧刀”可由士兵随身携带，遇到突发情况便可发射，其头部装有导引头，可在空中搜索并锁定目标后进行攻击。目前，“弹簧刀”巡飞弹已在阿富汗战场大量部署并应用于实战环境[72]，如图1.19所示。

![](images/7c677ab5ad16ccfe544131d2c5d1a392e2f1fe6a1eee43f36e6cfc879e734cea.jpg)  
图1.18 “弹簧刀”巡飞弹

![](images/e186f7407d94daa616d121085883b7548109fd3c3b538ad87d78241a2ca8f752.jpg)  
图1.19 “弹簧刀”巡飞弹应用于实战

2020年9月爆发的“纳卡冲突”是人类历史上首次有组织的大规模无人机与反无人机作战，阿塞拜疆军队使用的TB2无人机利用机载视觉导引系统，对地面目标进行瞄准，并发射小型制导弹药打击目标，在这场冲突中摧毁了亚美尼亚的大量坦克、装甲车、防空导弹阵地等有生力量，如图1.20和图1.21所示[73,74]。

![](images/3f32a10ed2de5e325ca16ccbc0d2f7bd6551647c6cbe3f07a4f4dd96310e1ef1.jpg)  
图1.20 土耳其BayraktarTB2无人机

![](images/f9f55b57607ee26669cbed540e1ba8d97fd0faf7e75d286b613416bf7d8caa0d.jpg)  
图1.21 无人机瞄准地面军事目标

图1.22所示的小型无人直升机搭载了武器系统，该武装无人机可由地面操作人员遥控，利用机载视觉导引系统瞄准目标，最后通过空投炸弹、发射火箭弹和枪榴弹等方式精准毁伤目标[75]，其打击目标的过程如图1.23、图1.24和图1.25所示。

![](images/6349731a1ee9d81fe52d566f2bb34e931ed2b68362c2529dc814d2ab34d565eb.jpg)  
图1.22 “河豚”A3无人机

![](images/c80bce9ebfd9f4284902f1bf925b31d825d1b0b311497171baa62172470eb13f.jpg)  
图1.23 无人机瞄准目标后投弹

![](images/8a762afa482f6dd6ee3bc524a26e2139adbba95fdcd98a0aa18f03f5bb8e61ef.jpg)  
图1.24 无人机空投迫击炮弹

![](images/a73f7ec2b630f5a9b047fcd85eaafc933d232fefe6da2df2b51e6eeb2afea572.jpg)  
图1.25 无人机空中发射火箭弹

MQ- 1捕食者无人机由美军在阿富汗战场中首次投入使用，此后中空长航时察打一体无人机便在多次军事行动中展示其作战能力，引起了世界各国的关注。MQ- 9无人机在MQ- 1无人机的基础上发展而来，在性能上已经有了不小的提升[76]，如图1.26所示。MQ- 9无人机装备了美国Raytheon公司生产的AN/AAS- 52型多光谱瞄准系统（Multi- Spectral Targeting System, MTS）[77]，这套系统集成了红外相机、可见光相机、激光测距仪、指示器和照明器等传感器，可以自动跟踪并瞄准运动目标，指示目标的精确地理位置，引导机载导弹对目标进行打击，如图1.27所示。从“猎杀”苏莱曼尼事件来分析，美军的定点清除行动已从单纯的“空中狙击”形式走向多无人机机协同的作战模式，去中心化的分布式指挥控制体系成为主要发展方向。

图1.28为英国《每日邮报》绘制的美军对苏莱曼尼采取军事打击的过程示意图[78]。图1.29为MQ- 9无人机作战案例分析，其中提到的任务是从空中打击一名受到严密保护的目标，而最后的攻击必须在“15s任务窗口”内完成，该项任务中，四架无人机协同作战，各机之间快速交换信息，最终在极短的时间内完成对目标的致命打击[79]。

![](images/69e26cb7df54a5122a2032db988bebb62c21c520ee5c8774a36a8b796f3d3ca5.jpg)  
图1.26 MQ-9无人机

![](images/a0fedafcbe1b4b04f0c25b5bedadded1cf91c52d882db122b98e37930f17da1e.jpg)

![](images/c362234348df0fbad8b1ff6c24e247b0edd317b74d33f4183a363b8b4caed142.jpg)  
图1.27 AN/AAS-52目标瞄准系统

![](images/8a80ce55a0f3d49de6d4cef532d3b95367274cf69250b8aff0222ee00e43a3e2.jpg)  
图1.28美军“猎杀”苏莱曼尼行动示意图  
图1.29美军MQ-9无人机作战案例分析

无人机自主着舰一直以来都是各国高度重视的技术领域。Northrop Grumman公司与美军方开展的“无人空战系统验证机”（Unmanned Combat Air System Demonstration, UCAS- D）项目中，X- 47B舰载无人机于2011年2月完成首飞，2013年7月完成其首次着舰飞行，随后进行了多次在航空母舰上的全自主起降测试[80]，如图1.30和图1.31所示。在X- 47B无人机接近甲板时，降落引导系统为其提供了精确的导航信息，以辅助无人机挂上拦阻索，实现了自主降落。

![](images/0aa342ca8dd50af3103d43fde150edd1b24700d77c9e966603ba8e22f25e45e6.jpg)  
图1.30成功挂上拦阻索的X-47B无人机

![](images/e7509e7ba4db27f5aa54b0ec22bf52e4299b26ea23f17b8afa290aebcccf2034.jpg)  
图1.31X-47B从航空母舰上起飞

孔维玮等[81]，针对无人机自主着舰问题提出了一种分离式长基线伺服双目引导系统，其由两个引导单元组成分别布置在跑道的两侧，如图1.32所示。每个引导单元由一个二自由度转台驱动可见光、红外相机等多个传感器，如图1.33所示。这套系统利用双目视觉对目标位置进行估计，并将数据提供给无人机机载端的飞行控制器，从而引导无人机自主降落，在实际测试中取得了较好的效果。

![](images/c1a7ea1dce552e935b41032f010d8cae19701870404353d30397bc0199b759d9.jpg)  
图1.32分离式长基线双目视觉导引系统

![](images/c935f426cba965c5e75b9d4fc56e6b1bd8cf2d5f8fa3b3f2a41f138d8a95cbec.jpg)  
图1.33FLIRD300转台

# 1.3 基于视觉的目标检测与跟踪技术研究现状

目标检测与跟踪一直以来都是计算机视觉（Computer Vision, CV）领域内的重要研究方向[82]。随着微小型无人机技术的快速发展，目标检测与跟踪技术逐渐融入到无人机的众多实际应用中。本节对现有的目标检测与跟踪方法进行归纳，针对合作与非合作目标的跟踪问题分别进行阐述，主要结合视觉在微小型无人机自

主降落、运动目标追踪等问题上的研究情况进行了分析，对深度学习方法在目标检测领域的发展情况进行了介绍。

# 1.3.1 合作目标的检测与跟踪方法

合作目标检测一般是利用已知的目标尺寸、图案及形状特征等信息，来判别某个检测到的样本是否与预设目标一致。当视觉系统识别目标后，可通过已知的目标信息来实现对目标的位置与姿态进行估计。

最常见的合作目标是由一个圆圈包围的字母“H”图案，主要用于直升机停机坪[83]，如图1.34所示。Saripalli等人[84]，利用机载视觉系统对地面的“H”降落板进行检测和识别，并成功引导无人直升机精准地降落在降落板上，多次试验的方向误差仅为  $5^{\circ}$  左右，距离误差在  $28cm$  左右，如图1.35和图1.36所示。其实验过程中，降落板主要是静止放置在地面或者间歇性的移动，没有研究动平台的自主降落。2017年，在阿布扎比举行的国际机器人挑战赛（Mohamed Bin Zayed International Robotics Challenge, MBZIRC）“无人机移动目标侦测及自主起降”项目中，要求无人机识别比赛方规定的合作标志并自主降落在一辆以  $15\mathrm{km / h}$  速度行驶的汽车上，如图1.37所示。Baca等人[85]，提出了一种对该合作目标的快速检测方法，采用了视场角  $185^{\circ}$  的鱼眼镜头和机载电脑对图像进行处理，并基于模型预测控制方法成功实现无人机对运动车辆的跟踪和自主降落，如图1.38所示。

![](images/56e737563b683d54b22444ba416924d5a058b082a6088aa42cb0d2415886ee44.jpg)  
图1.34 “H”图案降落板

![](images/580634307218d74b89eed80480a6b324ba8571170f74ae420979b3b565f1e6f9.jpg)  
图1.35 摄像机视角

![](images/e52988db09e9380803844c08b84dac0172c6a1568abdc2b448c935a2c36941ad.jpg)  
图1.36 无人直升机自主降落在标志上

![](images/d2e8b1c55f0877d78f000b808d7deebdcada35afb8def73e20db177d4a365b2d.jpg)  
图1.37 降落板布置在车顶

![](images/e066f7106009f3890873acd0745b82cb6e911d1f987e49abfd7a8746411fa2ca.jpg)  
图1.38 无人机跟踪并自主降落在预定目标上

朱建明[86]对传统的“H”标志进行改进，将其上端封闭，解决了该标志在图像

中缺乏方向性的缺点，使其更适用于无人机自主降落任务。当无人机离降落板比较近时，传统“H”可能在图像中会缺失一部分，这是由于定焦相机的有限视场角所造成的，Jung等人[87]，基于传统“H”标志进行了改进，利用三个同心圆构成的图案，可以在不同的距离估计无人机的相对位置。Yang等人[88]，针对杂乱背景环境下的“H”标志检测和识别问题进行了研究，并利用单目视觉实现无人机六自由度位姿估计，实现了自主起飞、定点悬停与自主降落等任务。Lange等人[89]，设计了一种多个同心圆的合作标志，在卫星定位数据缺失时，通过视觉算法估计无人机的状态，从而实现无人机的自主控制。Chen等人[90]，设计了一种由多种颜色构成的图案，提出了一种基于视觉的自主定位与目标跟踪方法，实现了无人机在运动平台上的自主降落。一些典型降落板如图1.39所示。

![](images/90d61bdbffd8cc62106989a7f58cf2cbcda6e181c7ac3c0b80e305f4de1ae4f2.jpg)  
图1.39一些基于合作标志的降落板

利用合作二维码来提高目标识别的可靠性，同时估计摄像机的相对位置和姿态，是目前国内外广泛使用的一种方法。这些二维码标志一般设计成非常容易被检测的形状，每个标志内部包含二进制编码，因此具有独立的“身份号”以便被唯一地识别。目前这种含有丰富信息、效率高、可靠性好的二维码标志已广泛应用在基于视觉的无人机应用中，比较常见的如QR code[91]、Matrix[92]、ARTag[93]、ARToolKit[94]、ARToolKitPlus[95]、AprilTag[96]、ArUco[97]等，如图1.40所示。

![](images/c2801a32c8b16d12d0d47acc09f8661126cc226dfb982a454e5cda278b9c63cf.jpg)  
图1.40几类常用的合作二维码标志

Borowczyk 等人[108]，在其所设计的无人机自主降落系统中采用了 AprilTag 标志，把标志与一部手机放置车辆顶部，利用大疆 M100 无人机搭载的云台相机对车辆进行跟踪，同时将手机的 IMU 数据和全球定位系统（Global Positioning System, GPS）数据传输给无人机。该系统采用了一种卡尔曼滤波器（Kalman Filter, KF）对无人机、云台相机和手机的测量数据进行融合，从而实现了对无人机与标志的相对位置、速度和加速度的估计，其估计频率为  $100\mathrm{Hz}$  。在飞行试验中，无人机成功降落在以  $50\mathrm{km / h}$  速度行驶的汽车上，证明了该系统能够应用于高速运动条件下的无人机自主降落任务。

Sanchez- Lopez 等人[99]，将可变数量的传感器和定位算法以松耦合的方式进行组合，提出了一种基于二维码标志的多传感器融合状态估计方法，其利用相机对二维码标志进行测量，从而提高其估计精度。该方法采用了扩展卡尔曼滤波器（Extended Kalman Filter, EKF），其包含了延时测量补偿等特性，能够利用 IMU 和相机信息对无人机的状态进行估计。经过试验验证，该方法的平均位置误差小于  $4\mathrm{cm}$ ，而平均姿态误差小于  $1^{\circ}$ ，实现了较高的估计精度。

可见光背景下的目标检测容易受到光照等环境因素的影响，而用红外标志灯组作为合作目标能够较好地解决这一问题，在实际应用中也得到了验证。Wenzel 等人[100]，提出了一种由四个红外点组成的降落板图案，利用成本非常低的消费级相机来识别该图案，最终实现了无人机的自主悬停、跟踪和降落，如图 1.41 所示。徐贵力等人[101]，提出了一种可发射红外光的地面合作目标，能够在机场跑道上布置，如图 1.42 所示。通过理论分析和实验得出，当目标与背景的温差在  $170\sim 200^{\circ}\mathrm{C}$  时成像质量最好，优于可见光视觉系统所检测到的目标图像，基于此系统可以实现全天候的无人机自主着陆。

![](images/66ae62e7939d5646df8f0dea5d5c9307d383835ea6c06c2fa9c0d7ecd49cfbfc.jpg)  
图 1.41 由四个红外点组成的图案

![](images/3c4a4e423a5f2363eb5fcccddc5ce0173403dc1945e840507f3502ab85811caf.jpg)  
图 1.42 可发射红外光的合作目标

桂阳、于起峰等人[102,103]，选择理想着舰点为世界坐标系的原点，在理想着舰点后方布置红外标志灯组，全站仪用于测量标志灯中心在世界坐标系中的坐标，着舰时利用像机对标志灯进行检测，并估计无人机与理想着舰点的相对位置和姿态，利用位姿估计值实现无人机的飞行控制。该方法成本低且精度较高，工程实用性较强，其试验所用的无人机、像机、红外标志灯、全站仪等设备如图 1.43 所示。

![](images/1999748c5d06ad8402d4408315ca830316b65c1f4b50348f2e9878a6e3afe6d0.jpg)  
图1.43 利用机载视觉系统引导无人机自主着舰及其实验设备

# 1.3.2 非合作目标的检测与跟踪方法

与跟踪合作目标不同，在跟踪非合作目标时没有使用关于目标的任何先验信息，跟踪器（Tracker）一般只能把第一帧图像中给定的目标框作为唯一的训练样本，完成跟踪器的初始化，从而建立能够描述目标的模型或者能够将目标从图像中提取出来的分类器，最终实现对目标运动的连续估计并给出目标的位置预测值。在目标跟踪的过程中，若算法需要加入检测环节来实现跟踪，则该目标跟踪方法可以称为判别式方法，若无检测环节参与，则称之为生成式方法[104]。下面对两种不同类型目标跟踪方法的特点和发展情况分别进行介绍，对国内外学者的研究成果进行总结。

生成式跟踪方法采用模板匹配思路，比较典型的为MeanShift[105- 107]方法。Bradski等人[108]，在MeanShift方法基础上提出了CAMShift(Continuously Adaptive Mean Shift）方法，在目标发生形变时能够更好地保持跟踪的成功率，且该方法的实时性非常强。Levy等人[109]与Brand[110]，采用了增量奇异值分解(Singular Value Decomposition, SVD)的方法，获取子空间学习的解，并将其应用于目标跟踪。Khan等人[111]，提出了一种基于Grassmann流形的贝叶斯（Bayesian）在线学习和目标跟踪算法，通过Grassmann流形对目标的外形进行在线估计和非线性动态建模，能较好地解决目标部分被遮挡或发生非平面姿态变化时的跟踪问题。Mei等人[112]，

提出了L1跟踪器，使得目标跟踪算法的鲁棒性得到提升。Li等人[113]，在Mei等人的方法基础上，引入压缩感知（CompressiveSensing，CS）理论，提高了算法运行效率。Zhang等人[114]，提出了一种结构稀疏跟踪器（Structural Sparse Tracking,SST），充分利用了候选目标与局部区域的内在联系和空间布局结构。

判别式跟踪方法的工作原理则是通过寻找目标与背景之间的边界来实现对目标的提取和估计，其跟踪过程与检测环节相互联系，且又同步进行，因此一般也称其为基于检测的跟踪算法，比较成熟的例如OAB[115]、Struck[116]、MIL[117]、TLD[118]等方法。

近年来，基于相关滤波的目标跟踪算法凭借其出众的性能，在领域内引起了极大的关注。Bolme等人[119]，提出了基于相关滤波的MOSSE(Minimum Output Sum of Squared Error）滤波器，在跟踪目标时对光照、尺度和形状变化具有鲁棒性，能够以669帧每秒（Frames Per Second，FPS）的速度运行，但跟踪精度比较一般。Henriques等人[120]，首先在MOSSE的基础上通过循环矩阵来实现密集采样，并引入核方法，提出了CSK（Circulant Structure of Tracking- by- detection with Kernel）跟踪算法。并在随后的文章中把输入图像从单通道灰度特征换为多通道特征，并支持输入方向梯度直方图(Histogram of Oriented Gradient, HOG)特征，这就是领域内轰动一时的核相关滤波（Kernelized Correlation Filter, KCF）跟踪算法[121]，其通过OTB- 2013数据集[122]测试后，在运行速度和精度上都超越了同时期最先进的跟踪算法。最初版本的KCF算法没有考虑目标在画面中的尺寸变化，因此在跟踪时的目标预测框与最初给定的目标框大小始终保持一致，缺少对目标尺度的预测，也容易出现跟踪发散等情况。Li等人[123]和Montero等人[124]，对跟踪目标的尺度变化进行考虑，分别研究了基于尺度自适应的KCF跟踪算法，有效地解决了尺度预测的问题。

生成式跟踪方法使用了包含目标的图像信息来拟合目标的表观模型，然而在实际应用中，目标通常不是以固定的表现形式存在于图像中。而由于该类方法没有充分利用背景信息，当目标表现形式变化或图像背景中出现相似物体时，跟踪效果就会明显下降。判别式方法在跟踪过程中考虑了背景信息，在这一点上克服了生成式跟踪方法的不足，当目标被部分遮挡或发生形变时具有较强的鲁棒性。不过，训练样本的选取对判别式方法的跟踪性能影响很大，而通常在对非合作目标的跟踪任务中，目标样本数量是非常有限的，这也增加了跟踪的难度。

# 1.3.3 基于深度学习的目标检测方法

在很多应用场景中，当我们发现一个目标后，不仅仅需要知道这个目标在哪，还希望进一步地识别这个目标是什么。在深度学习算法出现之前，采用基于支持

向量机（Support Vector Machine, SVM）[125]的算法在目标检测领域是应用最为成熟的。近年来，在理论和硬件的支持下，卷积神经网络（Convolutional Neural Networks, CNN）得到了迅速的发展，在计算机视觉的许多领域都取得了巨大的成果，并且网络深度和复杂性都不断提高。2015年，在ImageNet挑战赛（ImageNet Large Scale Visual Recognition Challenge, ILSVRC）中[126]，采用152层深度神经网络的ResNet（Residual Network）在识别挑战中的错误率仅为  $3.57\%$  ，获得了当年的冠军。基于深度学习的目标检测算法具有较强的分层自学习能力，可以深入挖掘数据间的潜在联系，目前逐渐发展了以R- CNN（Region- CNN）、YOLO（You Only Look Once）和SSD（Single Shot Multi- Box Detector）为代表的三个主要框架[127]。

（1）R-CNN算法由Girshick等人[128]在2014年提出，是第一个成功把深度学习应用于目标检测领域的算法，能够在物体定位的同时检测出其类别。其主要的思路是先生成若干个候选区域，再提取特征并进行类别区分。Girshick等人[129]之后提出的Fast R-CNN算法和Ren等人[130]提出的Faster R-CNN算法，主要是基于R-CNN的思路在检测速度和精度上进行提升，但是速度上依然不能达到实时性的要求。

（2）YOLO算法由Redmon等人[131]提出，其将检测任务视为目标区域预测和类别预测的回归问题，属于单阶段检测器。之后提出的YOLOv2[132]在网络结构等方面进行了改进，在检测精度上相比YOLOv1有了进一步的提高。YOLOv3[133]在YOLOv2的基础上网络复杂度进一步增加，但检测速度仍然比较快，并加入了多尺度预测，对小目标的检测效果有了明显提升。

（3）SSD算法由Liu等人[134]提出，其结合了R-CNN和YOLO算法的优点，继承了YOLO中将检测转化为回归问题的思想，对于一张图片只需使用一次网络结构，此外使用了Faster R-CNN的预先框（Prior Box）思路，加入基于特征金字塔（Pyramidal Feature Hierarchy）的检测方式，在准确率和实时性两方面都有所改进。

基于深度学习的目标检测应用前景广阔，但依然存在需要解决的问题，比如速度和精度不能被兼顾、依赖大量的数据集来对网络进行训练以及对硬件配置要求比较高等。

# 1.4 无人机低成本飞行控制系统研究现状

无人机集群编队系统是近年来的研究热点，这种工作模式受大自然中的狼群、蚁群、蜂群等群体行为启发，其优势在于可以实现单架无人机的能力拓展，利用多无人机协同完成复杂的实际任务[135,136]。这种无人机集群一旦应用于军事行动，将会颠覆传统作战方式，由于不再单一地依赖某架无人机进行作战，进攻一方的

打击成功率将得以保证，而守方的防御成本则会因为无人机目标数量众多而大幅上升。在工程实际中如果给大量的无人机配备精密且昂贵的传感器和控制器，必然会造成成本上升，效费比大幅降低。因此，研究基于低成本飞行控制的无人机集群编队系统具有重要的意义。

本节对目前无人机低成本飞行控制器的发展现状进行了介绍，梳理了一些主流开源飞行控制器的发展历史，分析了微小型无人机的导航方式以及主要的位姿估计方法。最后，对无人机编队飞行控制方法进行了详细介绍，并对该领域的研究现状进行了综述。

# 1.4.1 基于多传感器融合的无人机位姿估计方法

惯性导航一直以来都是导弹、无人机等主要的导航方式之一。但由于惯性元件的特性，即便使用了高精度的传感器，惯性导航系统（Inertial Navigation System, INS）的定位误差也会随时间累积而增加。近年来，以四旋翼无人机为代表的微小型无人机发展迅速，其主要采用了INS/GNSS组合导航系统，既保持了惯导系统完全自主、不受外界信号干扰、能提供丰富的导航参数等优点，又利用卫星导航定位精度高且长期稳定的特点来弥补惯性导航系统的不足。微小型无人机主要采用了基于MEMS的陀螺仪、加速度计、磁力计、气压高度计等测量元件。虽然这些传感器的测量稳定性和精度都不高，但却有各自的特点，而如何利用多传感器数据来实现对无人机状态更准确地估计一直以来都是领域内的研究热点。

1960年，卡尔曼滤波方法的提出在航空航天领域轰动一时，其考虑到系统实际的测量数据是存在干扰的，在已知噪声服从高斯分布时，利用系统状态空间模型和观测模型对系统状态进行最优估计，在解决阿波罗计划的轨道预测中发挥了关键作用[137,138]。Carlson提出了联邦滤波（Federated Filtering, FF）方法[139]。无迹卡尔曼滤波（Unscented Kalman Filter, UKF）方法由Julier等人[140]提出，其利用无迹变换（Unscented Transformation, UT）过程处理均值和协方差的非线性传递问题，在非线性情形下表现更好。

Mahony等人[141- 143]，针对姿态解算和陀螺仪偏差估计的问题，基于无人机低成本惯性测量单元，提出了两种不同的非线性互补滤波器（Complementary Filter, CF），即直接互补滤波器和无源非线性互补滤波器，两种滤波器通过特殊正交群SO(3)进行更新并以四元数形式表达，实现了自适应陀螺仪偏差估计和姿态解算。针对无人机各姿态传感器的采样频率和延迟不一致的问题，Khosravian等人[144]，基于输出预测器和姿态滤波器组合，考虑了传感器测量数据不连续以及延时随时间变化的情况，设计了一种通用的递归预测器，在仿真环境下得到了较好的姿态解算结果。上述方法后来被广泛应用于微小型无人机飞行控制系统中。

Kumar 等人[145]，将基于 EKF 的姿态解算方法应用于由 MEMS 传感器组成的低成本惯性导航系统，对 3 个正交安装的加速度计和 1 个两轴磁力计的数据进行融合，引入了传感器模型，对速率陀螺积分得到的姿态进行校正，最终将滚转和俯仰的姿态解算精度保持在  $\pm 5^{\circ}$  的水平。Higgins 等人[146]，对 CF 和 KF 进行了详细地介绍和对比分析，通过方法推导认为 CF 比 KF 更加简化，计算量更小。Chowdhary 等人[147]，针对在线气动参数辨识问题，对 EKF、增广 UKF 和简化 UKF 的估计结果进行了对比分析，三种方法对某固定翼无人机的参数估计结果并无太大差异，而对某旋翼无人机的参数估计结果显示，增广 UKF 略优于其它两种方法。Marina 等人[148]，提出了一种基于 UKF 的航姿测量系统（Attitude Heading Reference System, AHRS），并使用三轴姿态确定方法（Three- axis Attitude Determination, TRIAD）作为其观测模型，搭建了基于 XPlane 9 的仿真框架，通过仿真试验证明了 UKF 的性能优于 EKF。

# 1.4.2 低成本开源飞行控制器发展现状

得益于无人机位姿估计方法与低成本组合导航系统的发展，一些基于开源硬件（Open- source Hardware, OSH）[149]和开源软件（Open- source Software, OSS）[150]协议的微型飞行控制器不断出现，比如 Arduino[151]、Paparazzi[152]、OpenPilot[153]、Mikrokopter[154]、MultiwiiCopter[155]、KK[156]等。目前，应用广泛且开发非常活跃的主流开源飞行控制器是 ArduPilot Mega[157]和 Pixhawk[158]，也可以简称之为 APM 和 PX4，如图 1.44 所示。

![](images/e4124d56778666b59d5141f26064acf4703da37d9b441ed7da0d096ea6c354eb.jpg)  
图 1.44 主流开源飞行控制器

APM 最早由 DIY Drones 无人机社区基于 Arduino 平台开发，后来隶属于 3DRobotics 公司，在经过多年的不断完善和发展后，已经能够支持多旋翼、直升机、固定翼、垂直起降无人机以及无人车等多种构型，并支持使用 Mission Planner 地面站软件来连接飞行控制器进行功能及参数调试等操作。

Pixhawk 是由苏黎世联邦理工大学计算机视觉与几何实验室（Computer Vision and Geometry Lab）、自主系统实验室（Autonomous Systems Lab）联合发起的开

源飞行控制器。Pixhawk 为开发者提供了二次开发平台，是一个扩展性非常强的通用软硬件框架，在无人机领域内有着极其重要的影响力[159]。在熟悉掌握 Pixhawk 的代码框架后，开发者可以根据任务的需要，自定义控制模型并编写课题研究需要的算法，用于一些新概念无人机的研究和控制实现。

# 1.4.3 无人机编队飞行控制系统研究进展

编队飞行的灵感源于自然界，人们观察到鸟群在长途飞行时会编成“人”字队形以减少飞行阻力[60,161]。近年来，由于微小型无人机的实际任务复杂性日益增加，单架无人机的侦察范围、搜索效率、打击能力等都明显不足。受到自然界的群体行为模式启发，如果把众多微小型无人机有效组织成为类似蜂群的系统，将极大地提升微小型无人机执行任务的能力，微小型无人机集群编队系统也逐渐成为当前无人机领域研究的重点方向。

美国国防部为了验证其提出的蜂群作战系统的可行性，利用3架战斗机在空中投放百余架微型无人机，成功完成了编队集结、集体决策、自适应编队飞行等任务，如图1.45所示[162]。

![](images/5d22d8b3f4e8b01740f257eeb5e5fb1b507767ec54e17fea922be61a6a092c2f.jpg)  
图1.45美国国防部测试无人机蜂群作战系统

无人机编队飞行的核心目的是让编队内的无人机单元合理地分布并且有组织地流动，从而适应任务需求的变化，这里包含了任务规划、信息交互策略、编队队形控制等问题。贾高伟等人[163]，对目前主要的无人机集群任务规划方法进行了综述。林倩玉[164]在其硕士论文中，对无人机编队的信息交互策略进行了介绍，并对时延环境下的分布式编队控制方法进行了研究。秦昂等人[165]，针对无人机编队队形保持问题，采用全局渐进稳定控制方法设计了无人机队形保持控制器，通过仿真试验实现了较为理想的响应速度和跟踪精度。

无人机编队系统根据信息交互的策略的不同，可以分为中心化的控制和去中心化的控制。中心化的控制又称为集中式控制，是把控制系统布置在一个中心节点上，此节点可以收到所有无人机单元的状态信息，将这些信息处理得到控制指令后，再由该节点向所有单元发送，从而实现对编队的控制。这种编队方式的信

息集中度较高，编队效果较好，但对主控节点的计算和通信能力要求很高，而且一旦主控节点出现故障，整个编队系统就会失效。去中心化的控制也常被称为分布式控制，由于其降低了对中心控制节点的依赖，编队成员只与邻近无人机进行局部通信，就可以形成期望的队形，减少了计算量，更适合于大规模的群体控制，即便某个或数个成员在行动过程中出现了故障，也不会造成整个系统的失效[166]。相比之下，分布式控制是当前发展的主要方向。

在过去一段时间内，领域内的研究学者对编队队形保持控制方法进行了深入研究，主要包括以下三类。

（1）领航者/跟随者（Leader-Follower）方法。在一些文章中也称之为主从式编队，首先是确定领航者，在无人机飞行编队中一般是位置最靠前的无人机成员为领航者，其它成员为跟随者。领航者通过编队间的通信系统将自身运动状态向所有成员发送，跟随者在收到领航者的状态信息后，不断调整自身的运动状态，按照队形约束与领航者保持一定的距离，从而实现编队队形保持控制。焦林冠等人[167]，建立了两架无人机的领航者/跟随者编队飞行模型，引入模糊控制理论，通过设计模糊控制律来实现跟随者对领航者的跟踪。Challa等人[18]，在领航者/跟随者编队框架下，对编队在做机动时的飞行速度和姿态约束进行了研究，并且通过仿真试验进行了验证。嵇亮亮等人[169]，进一步地对无人机集群协同攻防技术开展了研究。领航者/跟随者的控制结构非常简化，但同时也有一定的局限性，例如领航者故障引起的编队控制失效等情况。

（2）虚拟领航者（Virtual Leader）方法。在一些文章中也称之为虚拟结构方法，其根据队形保持的需要，在每一时刻把编队看作是一个由多个节点组成的结构体，编队内的每个无人机成员跟踪这个虚拟结构体中特定节点进行运动。Seanor等人[170, 171]，采用三架YF-22模型飞机开展了基于虚拟领航者模式的编队飞行研究，得到了多次飞行实验数据并进行了对比分析，验证了该编队系统的可行性。Li等人[172]和邵壮等人[173]，对虚拟结构编队模式展开了深入研究，并进行了仿真验证。虚拟领航者的优点是每个无人机成员跟踪特定点飞行，没有误差传递，具有较高的控制精度，但缺点是没有编队状态的反馈，每个无人机成员不知道邻近成员的状态信息，容易出现碰撞等问题，并且这种编队模式对通信能力要求高，是一种集中式的控制方法，对中心节点的处理能力要求比较高。

（3）基于行为控制（Behavior-Based Control）的方法。基于行为的控制方法是一种完全分布式的编队模式，其通过学习自然界中的生物行为，按照一定的机制来约束编队内成员的相互运动，优点是编队成员之间可以很容易地相互协调，容错性更好，缺点是对系统的建模和分析比较复杂[174, 175]。

近年来，基于分布式一致性理论的编队控制方法引起了学术界的广泛关注，

在国内外也涌现了许多研究成果[176]。Ren 等人[177]，最早提出将一致性理论用于解决编队协同控制问题。一致性理论的基本思想是每个成员通过局部短距离通信来进行交互，最终获得高质量的一致目标信息[178- 180]。

Seo 等人[181- 184]，深入研究了一致性理论，并用该理论来解决编队协同控制问题。窦立谦等人[185]，提出了一种基于状态观测器的分布式有限时间编队控制方法。吴宇等人[186]，针对编队在满足通信拓扑结构的条件下快速收敛时的控制量与飞行状态约束相矛盾的问题，提出了改进一致性方法的编队控制策略。Zhu 等人[187]，对紧密编队集结问题进行了研究。郭伟强等人[188]，对同时具有通信和输入时滞的情况进行了研究。Yan 等人[189]，基于二阶一致性方法，引入位置和速度协调变量，通过相邻无人机成员间的指令交互，提出了一种多无人机编队控制策略。

基于信息一致性理论编队方法的优点在于，它对通信条件要求较低，不需要远距离通信，降低了系统的复杂度和成本，系统的容错性和扩展性较好，对集群的规模大小和成员数量的增减不敏感，目前已成为无人机编队控制领域最为活跃的研究方向。

# 1.5 论文研究内容与章节安排

本文以微小型无人机为对象，以无人机低空精确打击地面目标任务为背景，设计了微小型无人机目标跟踪与飞行控制系统框架，针对实际任务中的无人机自主降落、目标跟踪与定位、指派多架无人机对目标进行打击等问题开展研究。本文对合作目标的跟踪方法进行了研究，建立了微小型无人机自主降落系统；提出了基于深度学习的非合作目标并行检测与跟踪方法，建立了基于机载视觉导引稳定平台的空中运动目标自主跟踪系统，提出了一种基于无人机多点测向与测距的目标定位方法；研究了微小型无人机编队飞行控制方法，基于一致性理论提出了一种五机编队队形以及通信拓扑结构，开展了微小型无人机编队飞行试验。

论文由以下七章组成：

# 第一章 绪论

介绍微小型无人机的特点以及需研究的关键技术，梳理主流的目标跟踪方法，总结微小型无人机视觉导引系统与飞行控制系统研究现状，整理论文研究的主要内容。

# 第二章 微小型无人机目标跟踪与飞行控制系统框架设计

以无人机低空精确打击地面目标任务为背景，设计微小型无人机目标跟踪与飞行控制系统的框架，设计基于多无人机协同的低空精确打击任务流程以及各子系统之间的协调关系。根据微小型无人机的任务特点，对机载视觉导引系统主要硬件设备进行选型和分析，提出面向实际应用的微小型无人机目标跟踪与飞行控

制系统硬件框架，搭建微小型无人机飞行平台，建立微小型无人机目标跟踪与飞行控制系统软件框架，将系统主体算法部署在机载计算机中进行代码测试和可视化飞行仿真，通过仿真算例验证该方法的可行性。

# 第三章 基于合作目标的微小型无人机自主降落系统研究

针对微小型无人机自主降落问题，分析合作目标的检测和识别方法，设计一种优化的合作降落板标志，对微小型无人机与合作目标的相对位姿估计方法进行研究，提出一种适用于微小型无人机自主降落的任务流程和框架，搭建室内环境下的飞行试验系统，开展飞行试验并对数据进行分析，研究考虑视角和落角约束的微小型无人机自主降落导引律。

# 第四章 基于深度学习的非合作目标并行检测与跟踪方法

针对非合作目标的长时间跟踪问题，对非合作目标跟踪算法进行深入分析，采用尺度自适应方法来提高预测目标框与真实目标的吻合度，研究基于深度学习的目标检测算法，提出非合作目标并行检测与跟踪框架，通过静态目标和运动目标跟踪试验测试算法的实际应用效果。

# 第五章 基于机载视觉导引稳定平台的目标跟踪技术研究

针对运动目标自主跟踪问题，研究机载视觉导引稳定平台系统的结构形式、运动学关系和动力学模型，研究平台的稳定与跟踪控制方法，搭建微小型机载视觉导引稳定平台和飞行试验系统，开展空中运动目标自主跟踪飞行试验，提出一种基于无人机多点测向与测距的目标定位方法，对算法的可行性进行验证。

# 第六章 微小型无人机编队飞行控制系统研究

提出一种微小型无人机编队的通信拓扑结构，通过仿真试验证明该算法可以使得编队实现预期的目标。将无人机的轨迹跟踪问题分为横向和纵向两部分，分别采用L1制导律和总能量控制方法来实现横向和纵向的轨迹跟踪。采用低成本的模型飞机和开源飞行控制器，搭建固定翼编队飞行试验系统，开展微小型无人机编队飞行试验。

# 第七章 结论与展望

总结论文的工作内容，梳理论文的主要创新点，在未来几个重要的发展方向上提供研究思路。

论文首先对目标跟踪系统架构进行研究，建立能够用于工程实际的基本系统框架，再对合作与非合作目标的跟踪方法进行深入研究，通过方法的改进来提升目标跟踪效果。随后，论文对平台式视觉导引头的结构和控制进行了分析，将机载视觉导引稳定平台用于微小型无人机对运动目标的跟踪和指示。最后，开展多架微小型无人机自主编队飞行试验来验证编队控制系统的有效性。论文的组织结构关系如图1.46所示。

![](images/b1314116b9f6897563efc6177f9195b5a321c39e97c27ac0821edd1e2d5d710a.jpg)  
图1.46论文组织结构图

# 第二章 微小型无人机目标跟踪与飞行控制系统框架设计

# 2.1 引言

微小型无人机能够在超低空飞行，尺寸与一些鸟类接近，而传统的防空雷达不能有效地识别这一类“低慢小”目标，其在隐蔽性、效费比、便携性等方面具有较大的优势。微小型无人机因其尺寸和重量的约束，无法搭载过于庞杂和笨重的机载设备，而同时又要完成对图像实时采集、计算、决策以及制导控制等多种复杂任务。因此，微小型无人机机载设备既要求小巧轻便，又要具备较高的计算速度和更低的功耗，同时也要具备性价比高和耐用性好等特点。在面对复杂环境下的目标跟踪任务时，如何设计满足轻量化、高效率和低功耗要求的目标跟踪与飞行控制系统框架，仍存在较大挑战。

本章提出了一种微小型无人机低空精确打击地面目标任务，设计了多无人机协同打击目标的流程。对摄像机、嵌入式图像处理系统、飞行控制器等设备进行选型和分析，提出了一种面向实际任务的机载视觉目标跟踪系统框架。在采用低成本摄像头、微型机载计算机和低成本开源控制器的基础上，搭建了一套完整的微小型多旋翼无人机飞行平台，建立了基于机载视觉的目标跟踪系统软件框架。最后，将全部算法部署在机载计算机中进行代码测试和可视化飞行仿真，通过无人机穿越圆环算例验证了该系统的可行性。

# 2.2 应用背景

# 2.2.1 无人机低空精确打击目标任务

微小型无人机包括微小型扑翼机、微小型旋翼机和微小型固定翼三种主要类型，而本文主要研究的是微小型旋翼机中的微小型多旋翼无人机和微小型固定翼无人机。

微小型多旋翼无人机一般采用三个及以上数量的螺旋桨，通过螺旋桨旋转产生与自身重力相平衡的拉力，同时通过调节螺旋桨的转速来控制自身的运动。由于微小型多旋翼无人机独特的构型，其采用直接力进行控制，机动能力强，同时能够处于准静止状态，进行定点悬停，稳定性好。而微小型多旋翼无人机的缺点在于其续航性能比较差，有效载荷有限，在执行持续时间长的任务时需要进行回收和充电，才能够继续工作。

微小型固定翼无人机则主要靠机翼提供与自身重力平衡的升力，动力系统产生的推力则与飞行阻力平衡。这种飞行方式的优势在于巡航经济性好，结构质量

占比可以做到更低的水平，在相同起飞重量的条件下，有效载荷更大，且具有较高的飞行速度，可以按照导弹的飞行方式对目标进行打击。固定翼无人机飞行时必须保持一定的速度，对起飞和降落的场地要求比较高，飞行状态也容易受到扰动气流的影响。

2020年9月爆发的“纳卡”冲突的过程与结果表明，未来高技术条件下的局部战争将大量应用无人化的武器系统，其中大规模的无人机与反无人机作战是主要的战争形式之一。战场中的运兵车、补给车辆、导弹阵地、雷达阵地等时间敏感目标的打击窗口往往转瞬即逝，而中空长航时无人机一般需要从后方机场起飞，在目标区域上空数千米巡弋，目前军用雷达对这一高度的飞行目标探测技术成熟，无人机容易遭到防空导弹的拦截。微小型无人机的优点在于其便于携带、飞行高度低、传统雷达难以探测、维护简单、成本低、效费比高等，可装备到班排级小规模部队，灵活部署在战场前线，响应速度更快。因此，在未来战争中，微小型无人机的灵活使用可以有效地提高作战行动的精准度，提升对突发情况的应急处置能力，在提升打击精度的同时大大降低了进攻的成本。

微小型多旋翼无人机稳定性好、易于操纵和回收，而微小型固定翼无人机续航性能好、有效载荷大和飞行速度快。结合两者的特点，本文提出了一种基于多无人机协同的低空精确打击地面目标任务，如图2.1所示。

![](images/a1c45f2099aae6b817fb21363d30a41b00d5c52ac4a14b2c4fb942c193cf9275.jpg)  
图2.1一种基于多无人机协同的低空精确打击地面目标任务

基于对未来战争中无人机应用场景的分析，本文针对无人机低空精确打击地面目标任务，提出了基于微小型无人机的空面精确打击系统，将数量众多的微小型无人机群有机地结合，从而完成复杂的作战行动。该系统主要由地面控制中心、导引无人机、通信无人机、攻击无人机等分系统组成，其系统框架与信息流程如图2.2所示。

![](images/5dc77a621f87708c28d20c620cf026d586a2e6b9549e5035d040aae388242523.jpg)  
图2.2一种基于微小型无人机的空面精确打击系统

根据任务要求，首先由地面控制中心指派数架微小型多旋翼无人机对目标所在区域进行搜索，其中导引无人机对目标进行跟踪、定位和毁伤评估，通信无人机建立控制中心和导引无人机之间的数据通信链路，采用中继通信方式延长通信距离。在执行任务期间，微小型多旋翼无人机电量较低时，则返回控制中心的回收平台更换电池。发射车将数架挂载战斗部的微小型固定翼无人机发射升空，完成编队集结，保持一定的队形在目标区域巡弋，等待攻击指令。导引无人机锁定目标后，对目标进行测向与测距，将实时解算得到的目标位置发送给固定翼无人机编队，并引导固定翼无人机编队对目标进行“自杀式”攻击，攻击完成后由导引无人机对毁伤效果进行评估。

# 2.2.2 目标跟踪流程

本文基于“边检测边跟踪”的策略，首先利用基于深度学习的目标检测算法对视野内的所有目标进行提取和分类，在框选出跟踪对象后，将该目标在图像中的位置和大小等信息传递给跟踪算法，并作为初始样本来训练跟踪器，使其能够在序列图像中持续预测目标的位置，随后相隔固定帧对图像进行重新检测，并将检测结果用于校正跟踪器，再由跟踪器继续预测目标位置和尺度。

根据任务要求，无人机对存在目标的可疑区域进行侦察。无人机在上电启动之后，地面控制中心给出起飞指令，无人机离地一定高度定点悬停，待地面控制中心给出下一步指令后飞往预定区域，按照所规划的路径对目标进行搜索，同时

机载视觉系统对所获得的图像执行目标检测，整个飞行过程中无人机的状态数据和图像都实时地传输到地面控制中心。

当发现与预设类型一致的目标后，启用跟踪程序并将目标锁定在图像中心位置。此时，机载视觉系统解算出与目标参数并传递给飞行控制器，而飞行控制器根据机载视觉系统提供的信息计算制导控制指令，将控制指令发送给执行机构，使其在完全自主的模式下实现对目标的跟踪，并且保持目标始终清晰地出现在图像画面中。在跟踪过程中，机载视觉系统一直将目标锁定在画面中心并通过获得更多的实时图像信息，更新目标轮廓模型，当目标出现形变或较大的机动时，保持跟踪结果准确稳定。图2.3所示为微小型无人机执行对地面运动目标跟踪任务的过程。

![](images/326e1ff021ea231efc74e1da43437dbca2aec4de23380108ff17e14d58a37a93.jpg)  
图2.3无人机目标跟踪典型任务过程

为了精确地获取目标的位置，可以采用平台式的导引头，将摄像头与激光测距仪共同安装在平台的内框上，利用电机驱动平台，在摄像头对准目标后，通过无人机导航系统给出的姿态数据和平台编码器所测的角度数据来解算无人机与目标的惯性系视线角，同时利用激光测距仪测量无人机与目标的相对距离，从而实现对目标地理位置的估计。

由于微小型多旋翼无人机可以保持在准静止状态，当目标静止时，无人机只需在固定点悬停并将摄像头对准目标即可实现对其持续监控。当目标保持运动状态时，为了能持续将目标锁定在图像中，无人机需根据所获得的目标参数进行制导控制解算，从而实现对运动目标的自主跟踪。当无人机和目标之间有障碍物遮挡而导致目标在图像中丢失时，需返回搜索模式，根据丢失前对目标运动状态的估计，重新捕获目标。当无人机机载电池电量降低并触发报警值时，无人机自动地进入返航模式，回到基地更换电池，再飞向目标区域作业。

微小型无人机执行目标跟踪任务的流程以及各阶段之间的转换关系如图2.4所示。

![](images/f03c5209bc467ed514cdae0f2b8093ea344ab4d960a660bcb7989d78f4c02ed6.jpg)  
图2.4无人机执行目标跟踪任务各阶段转换关系

# 2.3 摄像机选型与标定

# 2.3.1 摄像机针孔模型

摄像机对物体进行成像，这里给出用于描述成像过程的相关坐标系定义如图2.5所示。

（1）世界坐标系  $O_{w} - X_{w}Y_{w}Z_{w}$

可以根据需要选择在空间中的任意位置，并以此来描述摄像机在真实空间中的位姿。

（2）摄像机坐标系  $O_{c} - X_{c}Y_{c}Z_{c}$

原点  $O_{c}$  为摄像机的光心，  $O_{c}Z_{c}$  轴沿光轴指向前方，  $O_{c}X_{c}$  轴指向摄像机正右方，  $O_{c}Y_{c}$  轴指向摄像机正下方。

（3）像平面坐标系  $O_{i} - X_{i}Y_{i}$

原点  $O_{i}$  为摄像机的光轴与成像平面的交点，  $O_{c}O_{i}$  的长度为摄像机的有效焦距

$f_{c}$ ,  $O_{i}X_{i}$  轴和  $O_{i}Y_{i}$  轴平行于图像平面并分别与  $O_{c}X_{c}$  轴和  $O_{c}Y_{c}$  轴的指向一致, 其坐标单位一般为毫米。

（4）图像坐标系

原点  $O_{0}$  为定义在图像矩阵的左上角, 两轴分别与图像平面的两边平行, 图像中的某个点  $p$  的坐标为  $(u, \nu)$ , 其单位为像素。

![](images/06e25e66b9e6c52d6b2b6e2e666b989f7a239da1fc870c80bb71711313096216.jpg)  
图2.5 摄像机相关坐标系定义

假设上述图像中的  $p(u, \nu)$  点在世界坐标系中的坐标为  $p_{w}(x_{w}, y_{w}, z_{w})$ , 其在摄像机坐标系中的坐标为  $p_{c}(x_{c}, y_{c}, z_{c})$ , 其在像平面坐标系的坐标为  $p_{i}(x_{i}, y_{i})$  。根据针孔模型, 可以得到如下等式

$$
\left\{ \begin{array}{l}\frac{x_{c}}{z_{c}} = \frac{x_{i}}{f_{c}} \\ \frac{y_{c}}{z_{c}} = \frac{y_{i}}{f_{c}} \end{array} \right. \tag{2.1}
$$

设  $d_{x}$  和  $d_{y}$  分别为在像平面坐标系  $O_{i}X_{i}$  轴和  $O_{i}Y_{i}$  轴方向上单位像素所对应的物理尺寸, 像平面坐标系的原点  $O_{i}$  所对应的像素坐标为  $(u_{0}, \nu_{0})$  。则在像平面坐标系和图像坐标系之间转换关系可表示如下

$$
\left\{ \begin{array}{l} (u - u_{0})d_{x} = x_{i} \\ (v - \nu_{0})d_{y} = y_{i} \end{array} \right. \tag{2.2}
$$

将上面两式联立并写为矩阵形式, 可得

$$
\left[ \begin{array}{c}u \\ \nu \\ 1 \end{array} \right] = \left[ \begin{array}{ccc}f_{c} / d_{x} & 0 & u_{0} \\ 0 & f_{c} / d_{y} & \nu_{0} \\ 0 & 0 & 1 \end{array} \right]\left[ \begin{array}{c}x_{c} / z_{c} \\ y_{c} / z_{c} \\ 1 \end{array} \right] = M_{in}\left[ \begin{array}{c}x_{c} / z_{c} \\ y_{c} / z_{c} \\ 1 \end{array} \right] \tag{2.3}
$$

其中,  $M_{in}$  为摄像机内参数矩阵, 其含有四个待标定的未知参数, 均与摄像机的分辨率、焦距等指标相关, 即便同一型号和生产批次的摄像机也存在差别, 不同的标定过程解算出的参数也不完全相同。而同一个点在摄像机坐标系和世界坐标系下的坐标可以相互转换, 如下所示

$$
\left[ \begin{array}{c}x_{c} \\ y_{c} \\ z_{c} \\ 1 \end{array} \right] = \left[ \begin{array}{cc}R & T \\ 0 & 1 \end{array} \right]\left[ \begin{array}{c}x_{w} \\ y_{w} \\ z_{w} \\ 1 \end{array} \right] = {}^{c}M_{w}\left[ \begin{array}{c}x_{w} \\ y_{w} \\ z_{w} \\ 1 \end{array} \right] \tag{2.4}
$$

其中,  $^c M_{w}$  为摄像机的外参数矩阵,  $T = \left[ \begin{array}{lll}t_{x} & t_{y} & t_{z} \end{array} \right]^T$  为三维平移向量, 表示世界坐标系的原点  $O_{w}$  在摄像机坐标系  $O_{c} - X_{c}Y_{c}Z_{c}$  中的位置,  $R$  是一个  $3\times 3$  的正交矩阵, 表示如下

$$
R = \left[ \begin{array}{lll}n_{x} & o_{x} & a_{x} \\ n_{y} & o_{y} & a_{y} \\ n_{z} & o_{z} & a_{z} \end{array} \right] \tag{2.5}
$$

其中,  $n = \left[ \begin{array}{lll}n_{x} & n_{y} & n_{z} \end{array} \right]^T$  为  $O_{w}X_{w}$  轴在摄像机坐标系的单位向量,  $o = \left[ \begin{array}{lll}o_{x} & o_{y} & o_{z} \end{array} \right]^T$  为  $O_{w}Y_{w}$  轴在摄像机坐标系下的单位向量,  $a = \left[ \begin{array}{lll}a_{x} & a_{y} & a_{z} \end{array} \right]^T$  为  $O_{w}Z_{w}$  轴在摄像机坐标系下的单位向量[190]。

# 2.3.2 镜头畸变模型

针对摄像头存在的畸变效应[191], 本文采用笛卡尔空间的 Brown 畸变模型, 考虑径向和切向畸变, 如下所示

$$
\left\{ \begin{array}{l}x_{c1d} = x_{c1}(1 + k_{c1}r_{c}^{2} + k_{c2}r_{c}^{4} + k_{c5}r_{c}^{6}) + 2k_{c3}x_{c1}y_{c1} + k_{c4}(x_{c}^{2} + 2x_{c1}^{2}) \\ y_{c1d} = y_{c1}(1 + k_{c1}r_{c}^{2} + k_{c2}r_{c}^{4} + k_{c5}r_{c}^{6}) + 2k_{c4}x_{c1}y_{c1} + k_{c3}(x_{c}^{2} + 2y_{c1}^{2}) \end{array} \right. \tag{2.6}
$$

式中,  $(x_{c1d}, y_{c1d})$  是焦距归一化平面上的成像点畸变后的坐标,  $(x_{c1}, y_{c1})$  是焦距归一化平面上的成像点坐标, 其表达式如下

$$
\left\{ \begin{array}{l}x_{c1} = \frac{x_{c}}{z_{c}} \\ y_{c1} = \frac{y_{c}}{z_{c}} \end{array} \right. \tag{2.7}
$$

而  $k_{c1}$  、  $k_{c2}$  、  $k_{c5}$  分别为2阶、4阶、6阶径向畸变系数，  $k_{c3}$  、  $k_{c4}$  为切向畸变系数，  $r_c$  为成像点到光轴中心线与焦距归一化成像平面交点的距离，表达式如下

$$
r_c^2 = x_{c1}^2 + y_{c1}^2 \tag{2.8}
$$

# 2.3.3 光电成像传感器

光电像管、超正析像管、视像管等成像器件，主要在早期的电影摄像、电视机等领域投入应用，目前基本被淘汰。而电荷耦合器件（Charge- Coupled Device, CCD）[192]的出现，具有划时代意义。CCD传感器具有很高的灵敏度、分辨率以及较大的动态范围，同时可以满足小尺寸、轻质量和低功耗等要求，是一种高性能光电成像器件。

近年来，在智能手机等电子产品的市场需求推动下，基于互补金属氧化物半导体（Complementary Metal- Oxide Semiconductor, CMOS）[193]的图像传感器技术发展迅速，从一定程度上替代了CCD传感器。虽然CMOS比起CCD传感器在成像品质方面存在一定劣势，但CMOS传感器的功耗非常低，适合搭载于本文所研究的微小型无人机，并且CMOS的制造工艺更适合规模化的流水线生产，单个器件成本CMOS较CCD更低。

# 2.3.4 摄像机选型

本文研究的微小型无人机对载荷的尺寸、重量、功耗和成本的要求都是极其严苛的，在综合考虑上述因素后，认为CMOS这一技术路线更适用于微小型无人机的使用场景。在选择摄像机型号时，主要从性价比、成像品质、视角等方面进行挑选。图2.6为本文采用的低成本摄像头。

![](images/0544390362c5c6e756a7b7b82552fdc533bbdce99fbae24578ef4a1c62f054ba.jpg)  
图2.6低成本摄像头

表2.1所示为该型摄像头的参数。

表2.1摄像头参数  

<table><tr><td>特征</td><td>技术指标</td></tr><tr><td>图像传感器规格</td><td>CMOS 感光芯片 1/4 英寸</td></tr><tr><td>像素大小</td><td>3μm×3μm</td></tr><tr><td>像素数</td><td>100 万</td></tr><tr><td>帧率</td><td>30fps</td></tr><tr><td>分辨率</td><td>1280×720</td></tr><tr><td>对焦方式</td><td>定焦</td></tr><tr><td>焦距</td><td>3.0mm</td></tr><tr><td>视场角</td><td>90°</td></tr><tr><td>工作电压</td><td>5V</td></tr><tr><td>功率</td><td>1W</td></tr><tr><td>图像输出接口</td><td>USB 2.0</td></tr><tr><td>兼容系统</td><td>XP/Win7/Win8/Android/Linux/Mac OS</td></tr></table>

该摄像头虽然成像品质一般，但具有小巧、轻便、成本低等优点，非常适合搭载于微小型无人机。由于该摄像头的功耗非常低，其正常运行时的工作电流约为  $100\mathrm{mA}$ ，不需要额外的独立电源对其进行供电。一般情况下，摄像头通过USB接口与机载计算机相连，机载计算机则输出5V的工作电压为摄像头进行供电。同时，机载计算机可通过USB接口实时地读取和处理视频流。

摄像机视场角一般从  $45^{\circ} \sim 180^{\circ}$  之间选择，视场角越大则可视范围越大，但随之而产生的畸变效应也更加严重，会造成较大的测量误差。当视场角接近甚至大于  $180^{\circ}$  时，虽然可视范围会非常大，但同时画面会产生非常严重的变形，很难从中分辨出实际目标的外形轮廓。而视场角太小则视野也会相应减小，如果检测到的目标在画面中进行快速移动，则目标容易在摄像机画面中丢失。因此，本文采用了  $90^{\circ}$  视场角的镜头，是一种平衡利弊后的折中方案。

# 2.3.5 摄像机标定

摄像机内参数模型和畸变模型中的系数都是未知的，即便是同一个批次的产品，其模型参数都存在一定的差别的，因此在使用前对其进行标定。

本文在Matlab环境下对所选的摄像机进行标定，基于张正友提出的二维平面靶标标定方法[194]，自制了一块棋盘格图案的平板，其尺寸为  $800\mathrm{mm} \times 800\mathrm{mm}$ ，每个单元格是正方形，边长为  $66.67\mathrm{mm}$ ，纵向和横向都为12格。选择棋盘格世界坐标系的原点  $O_{w}$  位于标定板的左上角点，  $O_{w}X_{w}$  和  $O_{w}Y_{w}$  分别沿标定板的两边，  $O_{w}Z_{w}$  垂直于标定板。图2.7为自制的棋盘格标定板图片，从不同方位拍摄了大量的图片，挑选了其中拍摄质量较好的12张图片进行标定，图2.8为在标定工具箱中确定世

界坐标系的原点。

![](images/c468df069282292bc6dd04586301e4b70abe4fe8554e96bb39d6a906444acd5b.jpg)  
图2.7自制的棋盘格标定板

![](images/57a1a855e40e6ae0f46433dc364ff82ac303dc04901e6dc5bc5cd14005c09365.jpg)  
图2.8选择坐标系原点

标定时，首先不考虑畸变模型，假设从不同方位拍摄了  $\mathbf{n}$  张靶标图片，每个图片上可以检测到  $\mathbf{m}$  个角点，由于棋盘格的尺寸已知，在选定棋盘格世界坐标系后，可以知道在世界坐标系下第i张图片中的第j个角点  $m_{ij}^{w}$  以及其在图像中所对应的像素点  $m_{ij}$  。

在完成以上操作后，可以求解摄像机的内参数和外参数，然后利用非线性优化方法使得重投影误差最小。可以对如下所示目标函数进行最小化：

$$
\sum_{i = 1}^{n}\sum_{j = 1}^{m}\left\| m_{ij} - \hat{m} (M_{in},R_{i},T_{i},m_{ij}^{w})\right\|^{2} \tag{2.9}
$$

其中，  $M_{in}$  为摄像机内参数矩阵，  $R_{i}$  为第i张图片摄像机外参数模型中的旋转矩阵，  $T_{i}$  为第i张图片的平移向量，  $\hat{m} (M_{in},R_{i},T_{i},m_{ij}^{w})$  为世界坐标系下的点  $m_{ij}^{w}$  在图像中的投影。

在不考虑畸变的情况下，求解得到了初步的摄像机参数后，假设这些参数是准确的，先利用其估算摄像机的畸变系数，这里考虑径向畸变系数  $k_{c1}$  、  $k_{c2}$  和切向畸变系数  $k_{c3}$  、  $k_{c4}$  。然后再对所有这一系列参数进一步优化使得对所有图片解算的平均投影误差最小，可以取如下所示的目标函数对其进行最小化

$$
\sum_{i = 1}^{n}\sum_{j = 1}^{m}\left\| m_{ij} - \hat{m} (M_{in},k_{c1},k_{c2},k_{c3},k_{c4},R_{i},T_{i},m_{ij}^{w})\right\|^{2} \tag{2.10}
$$

其中，  $\hat{m} (M_{in},k_{c1},k_{c2},k_{c3},k_{c4},R_{i},T_{i},m_{ij}^{w})$  为考虑畸变效应后世界坐标系下的点在图像中的投影。

通过对12张靶标图片进行解算，本文得到的摄像机内参数模型和畸变系数如下所示

$$
M_{in} = \left[ \begin{array}{ccc}912.5796 & 0 & 669.2593 \\ 0 & 909.4341 & 322.0985 \\ 0 & 0 & 1 \end{array} \right] \tag{2.11}
$$

$$
\left\{ \begin{array}{l}k_{c1} = 0.0486 \\ k_{c2} = -0.0907 \\ k_{c3} = 0.0003 \\ k_{c4} = -0.0009 \end{array} \right. \tag{2.12}
$$

图2.9和图2.10所示为标定过程中的摄像机和靶标的几何关系，图2.11为每张图片的投影误差，经过优化后的平均投影误差为0.19像素。

![](images/1887c00e34415d6601f1af5edd5dc4b24b5ec16a18cac860377ce415f14ef711.jpg)  
图2.9 摄像机（固定）与靶标的几何关系

![](images/82e1cc05d858d594784fc574f81f43f3a4013b25752f0fea15b97581f7ae3ab7.jpg)  
图2.10 摄像机与靶标（固定）的几何关系

![](images/b51ccaed9bc698be5570259166199bea66d0977dbe5218da140027284716252f.jpg)  
图2.11 平均投影误差

# 2.4 嵌入式图像采集处理系统

# 2.4.1 图像采集与数据传输

图像采集与数据传输是视觉系统中的重要环节，实时图像传输的信息量都非

常大，如果传输环节存在延迟，会对整个视觉系统的性能产生一定的影响。工业摄像机领域常用的数据传输接口有 Camera Link[195]、IEEE 1394[196]、GigE[197]、USB2.0[198]和串行接口[199]等。

本文采用的传输接口是 USB2.0，摄像头接入后，利用开源计算机视觉库（Open Source Computer Vision Library, OpenCV）[200]进行图像的采集、显示、存储等操作。首先，在 Windows7 64 旗舰版操作系统中安装 Visual Studio 2010 环境并下载 OpenCV2.4.9 库，随后完成环境变量、工程包含（include）目录、工程库（lib）目录、链接库等配置工作后，新建工程，编写 cpp 源文件，最后对所编写的代码进行测试。如图 2.12 所示，在 OpenCV2.4.9 环境下读取摄像头图像后，将原图转换为灰度图，采用均值滤波对图像进行模糊化处理，随后进行边缘检测。

![](images/e2b522e3db179a709c35c756ffb47f8796924f976a4807f1cf3011debf08e3c4.jpg)  
图2.12 摄像机图像采集、存储、降噪和边缘检测算例

# 2.4.2 嵌入式图像处理系统

机器人、无人机、无人驾驶等基于移动平台的应用场景对计算效率和实时性的要求非常高。传统计算机或服务器因为其体积大、重量大、功耗高、成本高等诸多缺点不能满足这类新兴应用的要求，而集图像采集、处理于一体的嵌入式图

像处理系统就成为了当下发展和研究的重要对象。

目前，嵌入式图像处理平台主要有数字信号处理（Digital Signal Processing, DSP）、现场可编程门阵列(Field- Programmable Gate Array, FPGA)、高级精简指令集机器（Advanced RISC Machines, ARM）三种架构。考虑到ARM架构在功能扩展性、功耗等方面的优势，本文选择ARM- Linux方案，利用ARM架构硬件平台配置Linux操作系统环境，通过V4L（Video for Linux）接口函数库调用内部的USB HOST驱动来采集图像。基于ARM- Linux的嵌入式图像处理系统是当前比较主流的一种配置方案，李素芬[201]、蒋立丰[202]、吴健[203]等人在其论文中都对此进行了研究和分析，其系统框架如图2.13所示。

![](images/20ed148aa286f842d773eb696287997590b7edf2ca47c2031389325431b2bb86.jpg)  
图2.13基于ARM-Linux的嵌入式图像处理系统软硬件框架

综合考虑尺寸、计算能力、内存容量、功耗和成本等指标参数后，本文选择了Jetson TX2核心板加扩展底板的硬件平台，其采用了图形处理器（Graphics Processing Unit, GPU）来加速图像计算，既能够满足目标跟踪算法运行时的计算效率和实时性要求，又能够满足微小型无人机对机载设备的尺寸、重量和功耗的约

束，同时成本控制在合理的范围之内。

束，同时成本控制在合理的范围之内。如图 2.14 所示，本文为目标跟踪系统配置了基于 Jeston TX2 的机载计算机，在搭配扩展底板之后，还为其配置了风扇，在运行时可以提高散热效果，该嵌入式平台总的尺寸为  $100\mathrm{mm} \times 87\mathrm{mm} \times 55\mathrm{mm}$ ，重量仅为  $208\mathrm{g}$ 。

![](images/eae7fee74150b5835c487b6e0781b693e2ef94153a5e31ff22b05366d92b8074.jpg)  
图2.14 基于NVIDIA Jetson TX2的机载计算机

本文采用的扩展板为用户提供了更丰富的接口，支持计算机接入更多的外部设备，能够满足无人机目标跟踪系统的需要。

# 2.5 基于机载视觉的目标跟踪系统框架设计

# 2.5.1 功能定位

对用户感兴趣的特定目标进行搜索和持续监控，已经成为无人机功能扩展中最重要的方向。在设计目标跟踪系统时，要从机载图像处理平台、目标检测与跟踪算法到无人机的制导律、控制律以及与地面数据通信等方面进行考虑，最终实现一个闭环系统。本文建立的目标跟踪系统在具备工程应用价值之外，还要满足研究的需要，因此飞行全程的试验数据都需要完整地记录。

许多任务要求无人机在对目标进行跟踪的同时，还要对目标位置进行解算，而单架无人机只利用目标的视线信息是无法直接解算出目标地理位置的。在平板地球假设的条件下[204]，通过无人机导航信息获得其与目标的相对高度，则可以在仅有目标视线信息的情况下对目标进行定位，这是一个很强的假设条件，在实际应用中不存在这样理想的情况。因此，实际应用中，可以利用单架无人机多点测

量目标，或者利用多架无人机对目标进行观测，并记录下测量时的方位和位置信息，由此利用测向交叉方法来实现对目标地理位置的估计[205]。针对运动目标的定位问题，在没有测距信息时，则需要已知包含目标运动模型的先验信息，即可以把目标的运动状态近似为匀速直线运动（Constant Velocity, CV）模型、匀加速直线运动（Constant Acceleration, CA）模型、联动式转弯运动（Coordinated Turn, CT）模型等[206]。在设计目标跟踪系统硬件和软件框架时，应充分考虑目标跟踪与目标定位的功能需要。

# 2.5.2 硬件框架

综合考虑机载端和地面端各设备的功耗、尺寸等多方面指标后，本文采用了Pixhawk 最新一代的Pixhawk 4 自动驾驶仪作为机载端的飞行控制器，其相比上一代性能有所提升，更加紧凑小巧。飞行控制器通过串口与机载计算机连接，在发送飞行数据给机载计算机的同时接收控制指令，并将计算得到各执行电机的油门指令发送给电调（Electronic Speed Control, ESC），从而驱动电机转动。Pixhawk 4 连接机载端数传电台，可以将飞行数据包发送给地面站，并且通过连接遥控接收机可以使用遥控器对无人机进行手动操纵。在执行目标跟踪任务时，机载计算机将摄像机的图像采集并进行处理，利用当前的无人机状态信息来计算无人机控制指令，最后发送给飞行控制器来执行。图2.15所示为无人机目标跟踪系统的硬件框架，并详细地给出了各设备连接、通信方式等。

![](images/4c1cbfa0067affc7eeaae7434ebaffa744fadec468b8a5b338ef74a2d1657334.jpg)  
图2.15 无人机目标跟踪系统硬件框架

机载计算机连接着图传模块（机载端），该模块建立无线网络，将机载计算机的运行界面通过远程桌面的方式与连接着图传（地面端）的地面站进行交互。该图传模块工作频段在  $5.1\sim 5.9\mathrm{GHz}$ ，支持无线传输图像和数据，传输距离大于  $2\mathrm{km}$ ，封装后的重量仅为  $146.8\mathrm{g}$ ，尺寸为  $88\mathrm{mm}\times 66\mathrm{mm}\times 20\mathrm{mm}$ ，具有轻巧、低功耗、低延时、低成本等特点。执行任务时，只需在地面站计算机上通过远程桌面控制机载计算机的运行，可支持对目标的甄别、框选，并根据当前任务情况进行决策，实现人在回路控制。

机载端采用一块锂电池接入分电板进行降压、稳压来为各种不同工作电压下的设备进行供电，同时配备一块预留电池在主电池失效时支持无人机紧急迫降至地面。这种供电方式可以有效管理电源使用情况，只需一次上电就能够启动全机设备，在紧急状况下还能提供降落所需的电量。图2.16所示为本文搭建的四旋翼无人机目标跟踪平台实物。

![](images/3a85dbeee395cc4934c2d1221dcf2e47eb910ca933b81ab07bc21d934ddef22c.jpg)  
图2.16 四旋翼无人机飞行平台

# 2.5.3 软件框架

本文基于Linux Ubuntu 18.04操作系统环境下，采用机器人操作系统（Robot Operation System, ROS）[17]框架来搭建微小型无人机目标跟踪系统软件框架。ROS支持与实时代码和嵌入式硬件的集成，其系统实现的层级架构如图2.17所示。

![](images/bad755765b161d9b60be8bb9865132d73558623d7388e1158eba136ee4d2e9c0.jpg)  
图2.17 ROS系统实现的层级架构

本文在ROS环境下通过包（Package）来实现对目标跟踪系统中不同功能模块的区分和管理，每个包中包含关键节点的可执行文件、消息定义文件等。本文按照表2.2中的指令进行操作，创建ros_workspace工作空间，并在该空间下逐步完成各功能包的开发工作。

表2.2创建工作空间指令  

<table><tr><td>操作</td><td>命令</td></tr><tr><td>建立工作空间</td><td>$ mkdir -p ~/.ros_workspace/src</td></tr><tr><td>进入 src文件夹</td><td>$ cd ~/.ros_workspace/src</td></tr><tr><td>初始化</td><td>$ catkin_init_workspace</td></tr><tr><td>回到ros_workspace中</td><td>$ cd ~/.ros_workspace/</td></tr><tr><td>编译工作空间</td><td>$ catkin_make</td></tr><tr><td>用vi编辑器打开 ~/.bashrc文件</td><td>$ vi ~/.bashrc</td></tr><tr><td>设置环境变量</td><td>$ source ~/.ros_workspace/devel/setup.bash</td></tr></table>

src文件夹是代码源文件存放路径，编译通过后即可执行。CMakeList.txt、Package.xml等是一些必要的配置文件，其中CMakeList.txt是编译配置文件，Package.xml是包的清单文件。此外根据目标跟踪系统的需要，可编写自定义消息（msg）、服务类型定义（srv）、可执行脚本文件（scripts）等以完善系统功能，如图2.18所示。

![](images/7075e4d7fd579f4027c48bd1d995c7266cb38a6e647d37b73b24b94d3463afc7.jpg)  
图2.18文件系统级的ROS架构

图2.19所示为计算图级的ROS架构。每个执行进程为一个节点（Node），节

点间通过消息（Message）传递信息。本文使用了服务（Service）和话题（Topic）两种不同原理的通信机制，其适用于不同的场景。服务是一种同步通信机制，客户端与服务端执行的是请求/响应机制。服务端无响应时，客户端的请求会以失败返回，而服务端正常提供服务时，则客户端会进入阻塞等待状态，直到服务端处理完后，客户端才会继续运行。而话题是一种异步通信机制，每个节点都可以将数据打包在消息里，通过发布（Publish）的方式将消息包含在用户指定的话题中，而每个话题都能被实时地传输和读取。发布者不断发送消息，并不会去检验消息是否被接收到，而接收方不断订阅（Subscribe）某话题中的消息。本文在无人机的解锁和上锁等过程中采用了服务机制，以确保无人机的当前状态有效切换，而在飞行数据传输中采用了话题机制，以满足节点间高频的通信要求。

![](images/15dc037c2c9d344d1ff363e5fd372da4e87f9840b14e019a064015c19a3b79d0.jpg)  
图2.19计算图级的ROS架构

此外，本文采用了表2.3中的工具来进行系统状态分析、消息查看、数据记录、图像显示、轨迹可视化等操作，有效地提高了研发效率，在遇到故障时能快速找到故障来源。

表  $2.3\mathrm{ROS}$  系统调试工具  

<table><tr><td>工具</td><td>主要功能</td></tr><tr><td>rqt_graph</td><td>查看当前运行的系统框架。</td></tr><tr><td>rqt_plot</td><td>绘制数据曲线。</td></tr><tr><td>rqt_image_view</td><td>图像可视化，在窗口中展示给定话题的图像。</td></tr><tr><td>rostopic</td><td>显示所有话题，查看话题中的消息数据。</td></tr><tr><td>rosbag</td><td>记录节点通过话题发布的消息数据。</td></tr><tr><td>rviz</td><td>3D可视化，在模型化环境中展示无人机的运动轨迹。</td></tr></table>

本文基于Ubuntu18.04和Melodic版ROS对目标跟踪系统进行搭建，主要针对图像采集与处理、目标跟踪、目标位置估计、无人机跟踪指令的解算、无人机飞行控制等环节建立相应的节点，从而实现闭环控制。

本文设计的微小型无人机目标跟踪系统总体软件框架如图2.20所示。

![](images/1b10ca1b43e4c3b5f80ad14aee112b711e747af320341cf0177a434376a2eb75.jpg)  
图2.20基于ROS设计的无人机目标跟踪系统软件框架

应用层的节点主要是针对整个任务过程进行管理，包括判断无人机在何种情况应处于什么飞行模式和状态、在任务过程中进行决策以及实现无人机制导信息解算等。

视觉计算包主要完成图像采集、目标检测和跟踪等操作，当完成对预定目标的锁定后，通过将标志位Target_Flag置为1（默认值为Target_Flag=0）把目标已捕获的信息传递给应用层，通过判断标志位的变化，无人机自动进入到设定的跟踪模式中。

位姿估计包主要是根据接入的激光雷达、动作捕捉以及视觉Slam模块对无人机位置姿态进行估计，从而更准确地控制无人机飞行。位置控制包则是实现无人机飞行控制的主节点，其不同的控制律通过编写不同的类来实现，并发布期望的位置或速度。

本文基于微小型无人机通信协议（Micro Air Vehicle Link, MAVLink），采用

mavros 节点来实现 ROS 环境下机载计算机与飞行控制器之间的数据交互[208]。通过 mavros 节点，机载计算机能够实时地读取 Pixhawk 飞行控制器的导航数据，同时将计算得到的控制指令不断地发送给 Pixhawk 飞行控制器，从而完成对无人机的控制。下面将介绍本文如何运用 mavros 节点和 MAVLink 协议来完成与飞行控制器的交互。

# 2.5.4 MAVLink 协议与外部控制模式

MAVLink 协议是一个公开的消息传递协议，Pixhawk 飞行控制器支持使用该协议来实现机载端与地面站之间的数据传输。该协议包含了相对完整的无人机飞行数据，并且对这些数据进行了详细的描述和分类，用户可以根据该协议的定义对一些飞行数据进行解析或者打包发送。

本文采用 Pixhawk 开源飞行控制器搭建无人机飞行平台，而 Pixhawk 飞行控制器提供了在脱离遥控器操纵情况下的完全自主飞行模式，即外部控制模式，也称之为 offboard 模式。本文中，机载计算机通过串口与 Pixhawk 飞行控制器连接，并基于 MAVLink 协议进行通信，从而实现在 offboard 模式下，无人机根据系统的设定自主地完成各种飞行任务。mavros 包封装了 MAVLink 协议，用户可以方便地调用。

本文主要调用了 MAVLink 协议其中的 LOCAL_POSITION_NED(#32) 和 SET_POSITION_TARGET_LOCAL_NED(#84) 等指令。LOCAL_POSITION_NED 指令包含了无人机在本地导航坐标系下的位置信息，系统通过读取这些数据，来解算下一步无人机期望的位置或速度。而 SET_POSITION_TARGET_LOCAL_NED 这条指令是机载计算机发送给 Pixhawk 飞行控制器的控制指令，其中包含了无人机在本地导航坐标系下的期望位置和速度[209]。

根据 MAVLink 协议公开的文档资料，其中 LOCAL_POSITION_NED 指令和 SET_POSITION_TARGET_LOCAL_NED 指令的内部数据组成和定义如表 2.4 和表 2.5 所示。

表2.4MAVLink协议LOCAL_POSITION_NED(#32)指令  

<table><tr><td>字段名</td><td>类型</td><td>单位</td><td>描述</td></tr><tr><td>time boot ms</td><td>uint32_t</td><td>ms</td><td>系统启动后的时间</td></tr><tr><td>x</td><td>Float</td><td>m</td><td>X 方向位置（北）</td></tr><tr><td>y</td><td>Float</td><td>m</td><td>Y 方向位置（东）</td></tr><tr><td>z</td><td>Float</td><td>m</td><td>Z 方向位置（地）</td></tr><tr><td>vx</td><td>Float</td><td>m/s</td><td>X 方向速度（北）</td></tr><tr><td>vy</td><td>Float</td><td>m/s</td><td>Y 方向速度（东）</td></tr><tr><td>vz</td><td>Float</td><td>m/s</td><td>Z 方向速度（地）</td></tr></table>

<table><tr><td colspan="4">表2.5 MAVLink协议SET_POSITION_TARGET_LOCAL_NED(#84)指令</td></tr><tr><td>字段名</td><td>类型</td><td>单位</td><td>描述</td></tr><tr><td>time_boot_ms</td><td>uint32_t</td><td>ms</td><td>系统启动后的时间</td></tr><tr><td>target_system</td><td>uint8_t</td><td></td><td>系统 ID</td></tr><tr><td>target_component</td><td>uint8_t</td><td></td><td>固件 ID</td></tr><tr><td>coordinate_frame</td><td>uint8_t</td><td></td><td>LOCAL_OFFSET_NED=1, 
LOCAL_OFFSET_NED=7, 
BODY_NED=8, 
BODY_OFFSET_NED=9</td></tr><tr><td>type_mask</td><td>uint16_t</td><td></td><td>用以决定忽略哪些量</td></tr><tr><td>x</td><td>Float</td><td>m</td><td>X方向期望位置（北）</td></tr><tr><td>y</td><td>Float</td><td>m</td><td>Y方向期望位置（东）</td></tr><tr><td>z</td><td>Float</td><td>m</td><td>Z方向期望位置（地）</td></tr><tr><td>vx</td><td>Float</td><td>m/s</td><td>X方向期望速度（北）</td></tr><tr><td>vy</td><td>Float</td><td>m/s</td><td>Y方向期望速度（东）</td></tr><tr><td>vz</td><td>Float</td><td>m/s</td><td>Z方向期望速度（地）</td></tr><tr><td>afx</td><td>Float</td><td>m/s/s</td><td>X方向期望加速度（北）</td></tr><tr><td>afy</td><td>Float</td><td>m/s/s</td><td>Y方向期望加速度（东）</td></tr><tr><td>afz</td><td>Float</td><td>m/s/s</td><td>Z方向期望加速度（地）</td></tr><tr><td>yaw</td><td>Float</td><td>rad</td><td>期望偏航角</td></tr><tr><td>yaw_rate</td><td>Float</td><td>rad/s</td><td>期望偏航角速度</td></tr></table>

其中type_mask用0和1来决定无人机采用哪些量来进行控制，0代表不忽略，1代表忽略。从第一位到第十二位分别代表期望的x、y、z、vx、vy、vz、afx、afy、afz、yaw、yaw_rate。如果采用位置模式，则考虑忽略期望的速度量，只发送期望的位置给Pixhawk飞行控制器。反之若采用速度模式，则考虑忽略期望的位置量，发送期望的速度给Pixhawk飞行控制器。

# 2.5.5 基于视觉的四旋翼无人机穿越圆环算例

本节将ROS、Gazebo[10]和PX4虚拟仿真环境部署在机载计算机。其中ROS提供了仿真系统运行的软件平台，Gazebo提供了飞行可视化环境，而PX4仿真环境则包含了无人机的动力学模型和控制系统模型。

首先，在Gazebo环境中建立无人机和目标圆环模型，将基于视觉的目标跟踪系统的代码建立在ROS工程包中，使用mavros节点来发送无人机期望的位置或速度指令，通过PX4仿真环境来模拟飞行控制器对传感器数据的滤波、对无人机状态的估计以及输出电机控制指令等过程，最后在Gazebo环境中实现飞行仿真以及数据记录。本文搭建了如图2.21所示的仿真系统框架，通过实际运行充分验证了各节点功能，最终实现了四旋翼无人机穿越圆环的仿真过程。

![](images/0a4a874fb764cc0384e520f3b277b6284ce49b5c7dbd1a9c3f6a34b3b538ee57.jpg)  
图2.21基于ROS/Gazebo的四旋翼穿越圆环仿真算例节点与话题关系图

由上图可知，pos_estimator 节点通过订阅/mavros/extended_state、/mavros/imu/data、/mavros/local_position/pose 等话题，分别得到无人机当前飞行状态、IMU 传感器数据以及本地坐标系下的位姿估计，并将处理得到的无人机状态估计数据发布在/uav/drone_state 话题中。

circle_cross 节点是控制整个任务流程的应用层节点，其通过订阅/uav/object_detection 和/uav/drone_state 话题来获得无人机以及目标的当前状态，随后通过发布/uav/control_command 话题将控制无人机的命令传递给/px4_sender 节点，由 mavros 节点将控制指令传递给 Pixhawk 飞行控制器，从而执行对无人机的位置和姿态控制。

gazebo 节点通过/uav/camera/image_raw 话题来发布无人机机载摄像头的图像数据，并在/uav/ground_truth 话题中发布无人机在三维环境中飞行时的真实状态数据。

无人机初始位置为本地坐标系原点，摄像机位于机头，指向无人机正前方，圆环目标中心点坐标为（7.5,1.0,1.8）点处，圆环直径为  $1\mathrm{m}$ ，无人机首先从地面起飞至离地  $1.5\mathrm{m}$  处定点悬停，target_det 节点运行中检测到目标时，更新目标标志位并基于椭圆检测结果实时输出目标在摄像机坐标系下的位置估计值，circle_cross 节点根据对目标的估计值解算制导指令，最终通过 mavros 节点发送给 pixhawk 飞行控制器执行。图 2.22 所示为该算例运行时，四旋翼无人机穿越圆环目标的整个过程。

![](images/09be5f65841586840e833cc3bca34e3004fa42a1e173af48cd8849dc9837788b.jpg)

![](images/3cc9f96ee62d78830346e531f3837d7bf19351888d9446495a72ef57097b92e4.jpg)  
图2.22 Gazebo环境下四旋翼无人机穿越圆环目标过程

由上图可知，无人机在进入目标跟踪模式后，能够在机载视觉系统的引导下逐步接近圆环目标，在整个跟踪过程中，目标始终保持在摄像机画面中心。基于视觉导引系统解算得到的目标位置数据，飞行控制系统对无人机的状态进行了十分有效的控制，在保持无人机姿态和速度稳定的同时，使其能够朝着期望的目标位置飞行。

当无人机非常接近圆环目标时，因摄像机存在视角限制，已经无法完整识别到整个目标轮廓，此时的目标位置解算会出现发散。因此，即将穿越目标时，无人机锁定当前的速度和飞行方向，继续向前飞行一段距离后，直到视野中圆环目标完全消失，则发送指令使无人机进入返航模式，按照预先设置的航点返回至初始位置。

图2.23所示为本文在Matlab 2019a软件中对ROS和Gazebo记录下的相关飞

行数据（存储于.bag日志文件中）的绘图分析。

![](images/153c5723497c5a9c88a691f614631f5f35f4d54e26b5dd47fdedf608e6544c39.jpg)  
(a)四旋翼无人机穿越圆环目标的飞行轨迹

![](images/190ae986a56e6c191865624eaee01dd64e6068df78a89d8fab94a34d72b75832.jpg)

![](images/61264eaec3508c5373909fc448a3422cfc646ad0afc33c8bdcceba16e696c964.jpg)  
图2.23无人机穿越圆环算例仿真数据

由上图可知，在108s左右，无人机进入目标追踪模式，四旋翼无人机机头下压以获得向前的速度，因无人机姿态快速改变，在摄像机系下解算的目标位置也出现大幅振荡，随着无人机姿态逐渐平稳，对目标位置的估计也更加稳定。从无人机穿越圆环目标的飞行轨迹可以证明，所建立的视觉跟踪系统在仿真环境下能有效地识别目标，无人机控制系统接收到基于椭圆检测得到的目标位置估计值后，能够控制机体准确穿越圆环目标，系统运行速度满足实时性要求，具备较高的实用价值。

# 2.6 小结

本章描述了机载传感器、控制器等设备的选型过程，研究了摄像机模型和嵌入式图像处理系统架构，分析了目标跟踪典型任务流程，提出了一种基于多无人机协同的低空精确打击任务框架，在机载计算机上部署代码和可视化飞行仿真环境，进行了无人机穿越圆环飞行仿真试验。主要研究工作及结论如下：

（1）分析了无人机目标跟踪典型任务流程，提出了一种基于多无人机协同的低空精确打击地面目标任务框架；（2）描述了摄像机成像模型，介绍了图像传感器的工作原理，完成了低成本摄像机的选型和标定，得到了摄像机的内参数模型和畸变系数；（3）建立了基于ARM-Linux的嵌入式图像处理系统软硬件框架，对NVIDIAJetson系列AI计算平台进行对比分析，最终选择NVIDIAJetsonTX2和Pixhawk4作为本文的机载计算机和飞行控制器，搭建了一套微小型多旋翼无人机飞行平台；（4）在ROS环境中建立了机载视觉目标跟踪系统软件框架，将代码部署于机载计算机并在Gazebo可视化仿真环境中进行测试，通过无人机穿越圆环算例成功验证了该系统的可行性。

# 第三章 基于合作目标的微小型无人机自主降落系统研究

# 3.1 引言

微小型多旋翼无人机以其高机动性、高可靠性和易维护性，在军用和民用领域应用广泛。然而，由于有效载荷和电池续航能力有限，微小型多旋翼无人机的持续飞行时间比较短。因此，在大范围区域作业时，微小型多旋翼无人机在电量用尽后，需要引导其降落在特定的回收平台进行充电或更换电池，从而能够继续执行任务。自主降落是无人机在飞行过程中最具挑战也是最危险的一个环节，考虑到微小型无人机的机载传感器都是微型且低成本的，机载端的计算能力也十分有限，在这样的条件下，如何精准引导无人机自主地降落在回收平台上，是本章研究的主要内容。

本章针对车载式微小型无人机自主降落问题，对合作目标的检测和识别方法进行了分析，研究了通过优化降落板图案设计来提高降落精度的方法。在第二章搭建的机载视觉目标跟踪系统框架的基础上，对合作目标的位姿估计方法进行了研究，搭建了室内环境下的飞行试验系统，分析了飞行试验数据，研究了考虑视角和落角约束的制导方法，提出了一种适用于车载式微小型多旋翼无人机自主降落任务的系统框架。

# 3.2 基于合作目标的车载式微小型无人机降落引导系统设计

# 3.2.1 合作二维码的识别

# （1）基本原理

本文主要对合作二维码标志进行研究，并用于实现无人机与降落平台间相对位姿信息的估计。实际应用中，目标所处的环境是复杂的，摄像机采集到的画面背景也包含着许多杂乱的轮廓，因此需利用二维码标志内部的信息来识别该标志是否属于降落平台。

ArUco码是用平面分布的黑白相间方格来表达二进制编码的图案，其外层是一圈黑色方格，内层由黑白区域组成。根据预定义的字典不同，标志内层包含的黑白方格数量也存在不同。OpenCV库中有许多预定义的ArUco字典，比如DICT_4X4_250、DICT_5X5_1000、DICT_6X6_250、DICT_7X7_100等，如图3.1所示。以字典DICT_5X5_1000为例，其每个标志的内层区域大小为  $5\times 5$  ，每个标志都有唯一的身份号（MarkerID），其编号从0至999，也就是一共包含1000个标志，MarkerID决定了每个独立的ArUco码能够被唯一地识别。

![](images/181943fafe64977239e5aa30d133da8ee312b631e7c0d33160b4854f8ad19670.jpg)  
图3.1来自不同字典的ArUco码

每个标志的内部编码采用了海明码（Hamming Code）的原理，其为一串包含特定数据的二进制代码，由数据位和奇偶校验位组成。以内层区域为  $5\times 5$  的标志为例，其内部存在5个序列，每个序列由5位数组成，3位是校验码，2位是数据码，因此每5位码能够表达4种不同的数据，5行编码序列可以表示  $4\wedge 5 = 1024$  种不同的数据。图3.2所示为字典DICT_5X5_1000中标志（MarkerID  $= 101$  ）的编码示意图。

![](images/503ae0b7c3510794790280bba87a3afbfa9219a139308ab06b2b716550307993.jpg)  
图3.2合作ArUco码内部编码示意图

由于无人机机载摄像机和目标之间存在旋转，因此同一个ArUco码存在4种不同的旋转状态，通过计算可以得到候选标志与所有旋转状态ArUco码的海明距离（Hamming Distance）。只有方向正确的二维码标志其海明距离为0，其它不同旋转状态下的海明距离不为0，则根据正确的标志内部编码可以得到其MarkerID号。

# （2）算法流程

针对复杂环境中的合作ArUco码的检测与识别，首先是将RGB格式的原始图像转换为灰度图，随后采用自适应阈值法对灰度图进行阈值分割，即对图像每个局部区域进行处理得到一个阈值，用这个阈值来对该局部区域进行分割，从而快速地提取图像中的有效信息，把目标从背景中分离出来。通过边缘检测和多边形

逼近寻找符合要求的四边形轮廓作为候选目标。对每个候选目标进行透视变换，将旋转状态下的标志转换成规则的正方形。提取候选目标内部的编码，判断其是否为预定义字典中的编码，如果不是则提取下一个候选目标的编码进行判断，如果是则解算得到其 MarkerID 号。在准确识别标志的身份后，对标志四个角的位置坐标进行精细化处理，返回处理后的坐标值。最后，在图像中画出标志的轮廓并用 MarkerID 号对其注释，表示对该标志已成功识别。图 3.3 所示为 ArUco 码标志的检测与识别流程。

![](images/8dfe5b5d4c66664bfaa97b73d4a0cc515ef6fc5db649b2ebf5b38ecabf673c54.jpg)  
图 3.3 合作 ArUco 码的检测与识别算法流程

![](images/504a365bd7124bfe962e1894717f529382cabaef352ac4a13b8afc6a25c59b04.jpg)  
图3.4为将ArUco识别算法部署在机载计算机中的实际运行效果。图3.4部署在机载计算机中的ArUco码算法实际运行过程

通过所识别的合作ArUco码得到摄像机与二维码间的位姿关系，需要求解PnP（Perspective- n- Point）问题。求解之前需要已知摄像机的内参数以及多个2D到3D点对的坐标，一般不少于3对。

由于合作ArUco码目标的尺寸是已知的，可以将世界坐标系的原点设置在ArUco码的中心点或者其它有利于后续求解工作的自定义位置，则其四个角点在

世界坐标系下的坐标也是已知的。假设某个ArUco码标志的边长为  $d_{a}$ ，取其几何中心为世界坐标系原点，则四个角的坐标分别为  $\left(\frac{d_{a}}{2}, \frac{d_{a}}{2}\right)$ 、 $\left(\frac{d_{a}}{2}, - \frac{d_{a}}{2}\right)$ 、 $\left(- \frac{d_{a}}{2}, - \frac{d_{a}}{2}\right)$  和  $\left(- \frac{d_{a}}{2}, \frac{d_{a}}{2}\right)$ 。

与Wenzel提出的红外灯组标志[100]相比，通过视觉来识别二维码更容易通过其内部编码唯一地确定目标，但缺点在于提取特征点时容易受到光照、图像噪声等因素的干扰，对测量精度有一定的影响。而通过红外摄像机提取光斑能够更准确地得到特征点的像素坐标，测量精度更高，但由于目标缺乏唯一性，因此需要采取一些策略来判断这些光斑之间的对应关系。

# 3.2.2 基于嵌套二维码的降落板标志设计

# （1）设计需求

由于本文采用了低成本摄像机，成像品质相对较低，其总像素数是给定的，镜头对焦方式为定焦，摄像机视角是有限的，并且固定安装在无人机的机身底部，朝下拍摄，被拍摄物体的清晰度受距离影响较大。在无人机降落时，摄像机距离二维码标志越近，则标志在成像平面中占据的像素越多，其轮廓和内部图案则更加清晰，在保证识别成功率的同时角点坐标提取更准确，位姿估计精度也更高。反之，如果摄像机距离二维码标志太远，标志在成像平面中占据的像素很少，其轮廓和内部图案则非常模糊，以至于不能够提取到其内部的编码，导致标志无法识别，也无法得到位姿估计值。

为了测试本文采用的低成本摄像机在不同距离处对单个ArUco码的检测效果，将尺寸为  $8 \times 8 \mathrm{~cm}$  的单个ArUco码（DICT_6X6_250, MarkerID=23）放置在与摄像机距离  $0.13 \mathrm{~m}$  、  $0.3 \mathrm{~m}$  、  $0.65 \mathrm{~m}$  、  $1.2 \mathrm{~m}$  、  $3.0 \mathrm{~m}$  、  $4.6 \mathrm{~m}$  处进行检测，其检测结果如图3.5所示。

![](images/6dc7a2f60417508bf9f244677a4dba46754370d2a421e1a10c8d66328652ee52.jpg)

![](images/b336773fad6580f70e6d02ca91fb502dc1f8e4fc0b489e863e80104b4885ac80.jpg)  
图3.5单个ArUco码在不同距离下的检测结果

由上图的检测结果可知，当摄像机与ArUco码距离  $0.13\mathrm{m}$  时，由于摄像机视角的限制，单个ArUco码标志已无法完整地在画面中显示，系统检测不到方形边缘，无法提取候选区域，也就无法对其内部的编码进行解析，导致该标志无法成功识别。而ArUco码在距离摄像机  $0.3\mathrm{m}$  、  $0.65\mathrm{m}$  、  $1.2\mathrm{m}$  和  $3.0\mathrm{m}$  处，都能够被系统成功检测和识别。当摄像机与ArUco码的距离超过  $4.6\mathrm{m}$  后，由于该ArUco码标志在画面中所占的区域太小，其内部图案十分模糊，对该ArUco码标志的检测成功率比较低。

综合考虑上述因素，在设计降落板时，二维码标志的实际尺寸应该充分考虑到机载摄像机在不同高度下的成像质量和视野范围，保证在降落过程中无人机能连续且稳定地识别降落板标志，从而在无人机降落过程中能够持续输出目标位姿的估计值，并完成对无人机的飞行控制。

本文考虑无人机回收平台是一块方形的硬质木板，放置在小车上方，其最大尺寸为  $700\times 700\mathrm{mm}$  。在设计降落板标志时，应考虑到无人机在进入降落阶段初期，距降落平台较远，在这个阶段需要检测出降落板标志的大致轮廓，从而实现无人机对无人小车的自主跟随。因此，在设计降落板时，应充分利用降落板的最大尺

寸来设计降落板标志的轮廓，保证远距离条件下，降落平台在画面中具有比较清晰和凸出的轮廓特征。

（2）降落板标志设计

通过对车载式微小型无人机自主降落系统的应用场景分析，对现有的降落板图案进行优化，本文采用了多层级内部图案的设计思路，将传统的圆环图案与ArUco码相结合，充分利用ArUco码具有独立身份号、能被唯一识别等特点，对不同身份号和不同尺寸的ArUco码进行合理的排列，并且采用嵌套二维码的方式来实现近距离条件下的标志识别与位姿估计，最后提出了如图3.6所示的合作降落板目标。

![](images/10c14faaeb3da12e5b9e49f4648b1a6594f19e852da9129f03457cb7a290ad97.jpg)  
图3.6基于嵌套ArUco码的降落板标志设计

该标志的外轮廓是黑色填充的圆环区域，外圆的直径为  $700\mathrm{mm}$ ，内圆直径为  $680\mathrm{mm}$ ，圆环内部一共由8个尺寸和MarkerID号不同的ArUco码组成。其中MarkerID为23、57、75、209号的ArUco码分别布置在左上角、右上角、右下角和左下角，尺寸大小为  $135\times 135\mathrm{mm}$ 。MarkerID为18和164号的ArUco码布置在左和右，尺寸为  $90\times 90\mathrm{mm}$ 。而标志中心位置则布置一个嵌套二维码，以解决标志近距识别问题。

（3）嵌套二维码设计

位于降落板中心的嵌套二维码标志由一大一小两个ArUco码构成，其中较大的ArUco码（MarkerID=31）尺寸为  $120\times 120\mathrm{mm}$ ，在大码内部的白色网格区域（3

$\times 3$  格）设置了一个尺寸为  $29 \times 29 \mathrm{~mm}$  的 ArUco 码（MarkerID=157）。在识别二维码的过程中，如果一块方格区域的像素超过一半是白色则认为整个区域都是白色的，反之如果整个方格区域超过一半的像素是黑色的则认为是黑色。因此这种嵌套方式不会对大小两个尺寸 ArUco 码的识别产生影响，也正是基于对 MarkerID 为 31 号的 ArUco 码内部特征的充分利用，才能够实现该合作二维码标志的嵌套设计[211]。

无人机在高度下降的过程中，由于机载摄像机存在视角限制，机载摄像机可视范围随着无人机高度的变化而改变。在无人机高度下降时，摄像机的视野会逐渐变小，如图 3.7 所示。

![](images/7614458167dd5a6a38ecd28dbd87c941115aa54c2cfd974d915f0edc2ef51795.jpg)  
图3.7 降落板标志在摄像机视野中的可视范围

由上图可知，在无人机下降高度的过程中，由于机载摄像机视角是有限的，摄像机的可视范围随着无人机高度的降低而减小。由于本文采用的摄像头分辨率较低且视场角固定为  $90^{\circ}$ ，而降落板标志的最大尺寸仅为  $700 \mathrm{~mm}$ ，则当摄像机距离降落板超过  $10 \mathrm{~m}$  时，圆环内部的二维码比较模糊，可以通过椭圆检测来识别降落板，并通过椭圆拟合估计摄像机与降落板的相对位姿[212]，进一步得到视线角速率，用于制导控制指令的解算。

当无人机与降落板距离较近时，由于各个 ArUco 码的位置和尺寸不同，并且

无人机的位置和姿态也不同，因此每一时刻所识别的 ArUco 码编号和总数量也不同，可以同时输出当前时刻所有 ArUco 码的识别结果以及其相应的位姿估计值，以便后期对这些数据加以利用。由于本文机载摄像头的安装方式为固定朝下，无人机的姿态扰动会对摄像机的拍摄角度产生较大的影响，而多个 ArUco 码分布在圆环内部的不同位置，即便摄像头拍摄角度倾斜导致中心位置的 ArUco 码超出了摄像机视野，也能通过识别邻近位置的 ArUco 码，持续锁定降落板目标。

# （4）地面测试

为了测试机载摄像机对该合作降落板目标的识别效果，飞行试验之前对系统进行地面测试，模拟各种实际工况。根据第二章的方法，本文在机载电脑中编写用于检测该降落板目标的节点程序，编译通过后，首先打开一个终端并输入 roscore 指令启动 ROS 一些基本节点和功能，再打开另一个终端通过 rosrun 指令运行该节点程序。

本文采用手持机载计算机和摄像头的方式，对固定放置的降落板进行拍摄，分别在室外和室内两种光照条件下，将降落板与摄像机相隔  $0.13\mathrm{m}$  、  $0.3\mathrm{m}$  、  $0.7\mathrm{m}$  、  $1.2\mathrm{m}$  、  $5.0\mathrm{m}$  、  $10.0\mathrm{m}$  距离下进行测试，并记录下系统实际的检测结果。室外环境中的测试结果如图 3.8 所示。

![](images/8ea764ae4a69ff395439e9e4c69881592b862fb07004b064768509c99d72bab5.jpg)

![](images/a9630a0a3d1828de89a5cf333ad04f98cfed35947d1168fad3e0d43df55142f0.jpg)  
图3.8 室外环境中降落板在不同距离下的检测结果

室外环境中，光照比较强，在检测过程中存在局部过度曝光的情况，对ArUco码的识别不是很稳定，在某些特定的角度受光照影响比较大。在室外执行自主降落任务时，需要对摄像机的一些参数进行调节，使得目标处于合适的光照之下，保证其轮廓和内部纹理是清晰的。而在室内环境中，灯光是可以根据需要手动调节的，相对来说更容易使目标保持在合适的光照条件下。室内环境中的测试结果如图3.9所示。

![](images/20bc83d40125500f0b165fc001e3c3fc0b3921971b0418b36603b88a1108fc36.jpg)

![](images/465d862cf814339920166862b7f0f6ad4f4714eba3eb328d10331a80731b01f6.jpg)  
图3.9 室内环境中降落板在不同距离下的检测结果

根据上图所示的室内外地面测试结果可知，在摄像头接近降落板的过程中，标志外层圆环轮廓最先被检测到，随后检测到位于四个角的ArUco码（MarkerID=23、57、75、209），紧接着检测到位于中心的嵌套二维码中尺寸较大的ArUco码（MarkerID=31）和位于左右位置的ArUco码（MarkerID=18、164），最后检测到嵌套二维码中的小ArUco码（MarkerID=157）。

采用该降落板标志，拓宽了降落板可被检测的距离范围，在无人机不断接近的过程中，尽管摄像头的视角有限且无人机姿态存在扰动，也可以同时输出多个尺寸不同ArUco码的识别结果和位姿估计值。当无人机即将降落时，摄像机与降落板的距离非常近（小于  $0.3\mathrm{m}$ ），此时唯一能稳定检测到是中心位置尺寸最小的二维码（MarkerID=157），通过对该二维码的位姿估计，无人机计算并判断当前状态是否满足会合条件，若不满足则继续控制无人机向降落板中心点飞行，若满足，则断开无人机的动力电源。

由于该降落板采用了多层级的内部图案，在无人机下降高度的过程中始终保持降落板能被识别和定位，在实际应用中，提高了目标识别的可靠性和稳定性，并提供了不同标志的位姿估计数据，有助于进一步的数据融合，从而提高目标的位姿估计精度。

# 3.2.3 车载式微小型多旋翼无人机降落引导系统设计

根据降落板目标位姿估计数据确定地面无人车的位置，可实现车载式微小型多旋翼无人机的自主起降。本章所采用的地面轮式无人小车，其轮距和轴距分别为  $270\mathrm{mm}$  和  $350\mathrm{mm}$ 。在无人机起飞后，无人车和无人机可以根据任务需要在不同的区域作业，当无人机电量即将耗尽时，则向无人车所在区域进行返航，而此时无人车保持匀速运动。当无人机探测到降落板标志时，向无人车发送指令进入降落阶段，无人车在收到指令后以一定的加速度做匀减速直线运动，经过一段时间

后停止。在整个降落过程中，假设无人车的卫星导航系统受到环境的干扰，导致无人机的定位数据是未知的或者不可靠的，则只能依靠无人机的机载视觉系统对目标进行定位。

本文将整个降落过程分为末制导段、目标跟踪段和会合段三个阶段，如图3.10所示。

![](images/2a48f6c1945e06c85fae87ad3440feab0d9e7a510a0a055a62227addf88026ec.jpg)  
图3.10 多旋翼无人机自主降落过程

末制导阶段，无人机距离降落平台较远，无人车保持匀速运动，此时通过检测降落板外层轮廓来解算降落板目标的视线角以及视线角速度，采用基于比例导引的制导算法控制无人机快速接近降落平台。与此同时，在地面车辆收到无人机开始降落的信息后，进入匀减速运动状态。

目标跟踪阶段，无人车已经处于慢速运动状态，无人机距离车载平台的相对高度为  $3\mathrm{m}$  左右，此时无人机已检测到多个ArUco码，通过所识别的ArUco码来估计降落板的位姿，利用该定位信息来控制无人机对降落平台持续跟踪并保持无人机位于降落板中心点正上方，相对高度保持不变。

会合阶段，此时无人车已经停止运动并向无人机发送其状态信息，无人机在接收到系统的会合指令后，缓慢下降至相对高度  $0.08\mathrm{m}$  。当满足会合条件（无人机与降落板中心点水平距离小于  $0.1\mathrm{m}$  且相对高度小于  $0.08\mathrm{m}$  ）时，无人机动力系统自动关闭，无人机落在降落板上实现会合。由于降落板中心位置采用一大一小两个ArUco码的嵌套设计，支持无人机在末端近距离条件下进行会合，因此这段“距离盲区”（Blind Range）带来的影响得以减小，减少了降落时对机载设备的冲击，并且有助于提高无人机的落点精度。

图3.11 所示为无人机自主降落系统流程图。

![](images/63daea9f08fbfd7eee8e451e78df0acac42ac96b06164888444675a5d40601dc.jpg)  
图3.11 多旋翼无人机自主降落引导系统控制流程

按照第二章所建立的无人机机载视觉导引系统，目标检测、识别、位姿估计、降落板跟踪、位置控制等算法都在ROS中部署，最后利用mavros节点将解算的制导指令发送给Pixhawk控制器，从而实现对无人机的飞行控制。一旦无人机自主降落程序启动，多旋翼无人机处于完全自主控制状态，Pixhawk控制器切换至offboard模式，并接收来自机载视觉系统的控制指令。由于无人机在目标跟踪阶段和会合阶段的高度较低，考虑到周围地形、树木、建筑遮挡等因素会对GPS模块产生不小的干扰，在这两个阶段只采用机载视觉系统和惯性导航系统提供导航信息。

本章在/vision节点中编写所有视觉部分代码，实现图像实时采集、目标检测、识别和位姿估计等算法，并通过话题/relative_position_flag和/relative_position发布表示是否成功检测到目标的标志位和目标的位置信息，如图3.12所示。

![](images/172a1e7d88678efeac1b27ca971a64fff45e4c1e51ee64d34d925a0aa70fb98e.jpg)  
图3.12 年载式多旋翼无人机自主降落系统节点关系图

其中，/track_land是该自主降落系统的应用层节点，用于处理视觉部分的信息，产生目标跟踪制导指令，并调控降落过程中的无人机状态。当确认目标后，由/track_land节点将制导指令通过/mavros/Command话题传递给mavros节点，无人机进入目标跟踪模式，并通过/position_control节点计算期望的位置或速度指令，最终由mavros节点传递给Pixhawk控制器进行执行。如果降落过程中目标丢失，则/track_land节点更改/mavros/Command中的关于无人机状态的指令消息，无人机进入悬停控制状态。

# 3.3 基于合作二维码标志的无人机位姿估计

# 3.3.1 无人机与降落板相对运动模型

第二章已对摄像机模型进行了描述，在目标跟踪实际应用中，可以将世界坐标系的原点设置在目标的几何中心，在该定义下所求解的摄像机外参数矩阵就包含了摄像机与目标的相对位置和姿态信息。由于摄像机是固定安装在无人机机身底部，摄像机与无人机的位置关系是已知的，因此本节主要考虑如何获得摄像机与目标的相对位姿估计。

取惯性参考系  $N$  的原点位于环境中的某固定点，无人机机体坐标系  $B$  位于无人机的几何中心，摄像机坐标系  $C$  的原点位于摄像机的光心，降落板世界坐标系  $W$  的原点位于降落板的几何中心，各个ArUco码的世界坐标系  $M_{i}(i = 1,2\dots 8)$  原点设

在其图案的几何中心处，详细的坐标系定义和运动学关系如图 3.13 所示。

![](images/9f10b3f9619cbf59b125199bd98a6b0d30083df495cf7a4fd48b00fca25fee1e.jpg)  
图3.13 无人机与降落板相对运动关系和相关坐标系定义

设  $^c M_{M_i}$  为从  $M_{i}$  系到  $C$  系的变换矩阵，根据摄像机模型，有如下表达式

$$
\begin{array}{r}\left[ \begin{array}{c}x_c^i\\ y_c^i\\ z_c^i\\ 1 \end{array} \right] = {}^c M_{M_i}\left[ \begin{array}{c}x_m^i\\ y_m^i\\ z_m^i\\ 1 \end{array} \right] \end{array} \tag{3.1}
$$

其中  $\left[x_m^i,y_m^i,z_m^i\right]^T$  为参考点  $a_{r}$  在  $M_{i}$  系下的坐标，  $\left[x_c^i,y_c^i,z_c^i\right]^T$  为该点在  $C$  系下的坐标。而变换矩阵  $^c M_{m_i}$  由一个  $3\times 3$  的旋转矩阵  $^c R_{m_i}$  和  $3\times 1$  的向量  $^c t_{m_i}$  组成，表达式如下

$$
{}^c M_{m_i} = \left[ \begin{array}{cc}{}^c R_{m_i} & {}^c t_{m_i}\\ 0^T & 1 \end{array} \right] \tag{3.2}
$$

在实际降落过程中，摄像机坐标系会随着无人机姿态的变化而旋转，而降落板可以认为是一个平面且与惯性参考系的  $x_{n}y_{n}$  面平行，因此变换矩阵  $^c M_{m_i}$  的逆矩阵  $^m_{M_c}$  ，其所包含的数据更能直接地解算惯性参考系下无人机的相对位置和姿态，表达式如下

$$
{}^m_{M_c} = \left({}^c M_{m_i}\right)^{-1} = \left[ \begin{array}{cc}\left({}^c R_{m_i}\right)^{-1} & -{}^c t_{m_i}\left({}^c R_{m_i}\right)^{-1}\\ 0^T & 1 \end{array} \right] \tag{3.3}
$$

其中， ${}^{m_i}M_{c}$  为  $C$  系到  $M_{i}$  系转换矩阵。而  $M_{i}$  系原点位于第i个ArUco码的中心并与之随动。若要得到摄像机相对于降落板的位姿，则需要利用已知的各ArUco码尺寸和平面分布信息来得到  $M_{i}$  系到降落板世界坐标系  $W$  的变换矩阵  ${}^{w}M_{m_i}$ ，表示为

$$
{}^{w}M_{m_{i}} = \left[ \begin{array}{cccc}1 & 0 & 0 & \Delta x_{m_{i}}\\ 0 & 1 & 0 & \Delta y_{m_{i}}\\ 0 & 0 & 1 & 0\\ 0 & 0 & 0 & 1 \end{array} \right] \tag{3.4}
$$

其中， $\Delta x_{m_i}$ 、 $\Delta y_{m_i}$  为第i个ArUco码的几何中心与降落板中心点的位置偏差值，如图3.14所示。

![](images/e69d544e94ed520ea6b5671bc550bc111ddbfff9d7addf00c7a76ef4f94d3d78.jpg)  
图3.14降落板中心点与各ArUco码的位置关系

则参考点  $a_{i}$  在降落板世界坐标系  $W$  下的坐标  $\left[x_{w}^{i}, y_{w}^{i}, z_{w}^{i}\right]^T$  可以表示为

$$
\left[ \begin{array}{c}x_{w}^{i} \\ y_{w}^{i} \\ z_{w}^{i} \\ 1 \end{array} \right] = {}^{w}M_{M_{i}}\left[ \begin{array}{c}x_{m}^{i} \\ y_{m}^{i} \\ z_{m}^{i} \\ 1 \end{array} \right] \tag{3.5}
$$

由于每个ArUco码的尺寸和位置是已知的，则其角点可以认为是4个已知的3D点，且在同一个平面  $x_{m_i}y_{m_i}$  和  $x_{w}y_{w}$  上。根据摄像机模型，这些角点在图像中的2D点坐标是能够获得的，因此可以通过求解PnP方程得到摄像机系原点在第i个ArUco码世界坐标系  $M_{i}$  中的位置估计，再通过图3.14所示的位置变换，即可得出摄像机系原点  $O_{c}$  在降落板世界坐标系  $W$  下的位置估计值。

# 3.3.2 二维码标志位姿估计方法

对于尺寸已知的ArUco码, 其4个角点  $q_{i}(i = 1,2,3,4)$  都在  $z_{mi} = 0$  平面上, 且坐标系  $M_{i}$  下和图像坐标系下的坐标是已知的, 如图3.15所示。

![](images/5c14961ffa15aa2149113af8872b6bec99d0766bee8ebbbc9af4c44513b1a5b5.jpg)  
图3.15已知4个共面点对（3D-2D）

空间点  $q_{i}(i = 1,2,3,4)$  在图像中的成像点为  $q_{i}^{\prime}(i = 1,2,3,4)$ , 可以进行如下计算

$$
\left[ \begin{array}{c}x_{1c}^{i} \\ y_{1c}^{i} \\ 1 \end{array} \right] = M_{in}^{-1} \left[ \begin{array}{c}u_{i} \\ \nu_{i} \\ 1 \end{array} \right] \tag{3.6}
$$

其中,  $(x_{1c}^{i}, y_{1c}^{i})$  为焦距归一化平面上成像点的坐标, 将  $q_{i}$  在  $M_{i}$  系下的坐标  $(x_{ri}, y_{ri}, 0)$  代入摄像机外参数模型, 则有

$$
\left\{ \begin{array}{l}x_{ci} = {}^c n_{rx}x_{ri} + {}^c o_{rx}y_{ri} + {}^c p_{rx} \\ y_{ci} = {}^c n_{ry}x_{ri} + {}^c o_{ry}y_{ri} + {}^c p_{ry} \\ z_{ci} = {}^c n_{rz}x_{ri} + {}^c o_{rz}y_{ri} + {}^c p_{rz} \end{array} \right. \tag{3.7}
$$

式中,  ${}^{c}n_{r} = \left[{}^{c}n_{rx}{}^{c}n_{ry}{}^{c}n_{rz}\right]^{T}$  是坐标系  $M_{i}$  的坐标轴  $O_{m_{i}}x_{mi}$  在摄像机坐标系中的方向向量;  ${}^{c}o_{r} = \left[{}^{c}o_{rx}{}^{c}o_{ry}{}^{c}o_{rz}\right]^{T}$  是坐标系  $M_{i}$  的坐标轴  $O_{m_{i}}y_{mi}$  在摄像机坐标系中的方向向量;  ${}^{c}p_{r} = \left[{}^{c}p_{rx}{}^{c}p_{ry}{}^{c}p_{rz}\right]^{T}$  是坐标系  $M_{i}$  的原点  $O_{m_{i}}$  在摄像机坐标系中的位置向量;  $(x_{ci}, y_{ci}, z_{ci})$  为点  $q_{i}$  在摄像机坐标系下的坐标。

由式(3.6)可得

$$
\left\{ \begin{array}{l}x_{1c}^{i} = x_{ci} / z_{ci} = (u_{i} - u_{0}) / k_{x} \\ y_{1c}^{i} = y_{ci} / z_{ci} = (\nu_{i} - \nu_{0}) / k_{y} \end{array} \right. \tag{3.8}
$$

式中,  $k_{x}$  、  $k_{y}$  为摄像机内参数模型中的系数, 表示如下

$$
\left\{ \begin{array}{l}k_{x} = f_{c} / d_{x} \\ k_{y} = f_{c} / d_{y} \end{array} \right. \tag{3.9}
$$

将式(3.7)代入式(3.8)中,得

$$
\left\{ \begin{array}{l}x_{ri}{}^{c}n_{rx} + y_{ri}{}^{c}o_{rx} - x_{1c}^{i}x_{ri}{}^{c}n_{rz} - x_{1c}^{i}y_{ri}{}^{c}o_{rz} + {}^{c}p_{rx} - x_{1c}^{i}{}^{c}p_{rz} = 0 \\ x_{ri}{}^{c}n_{ry} + y_{ri}{}^{c}o_{ry} - y_{1c}^{i}x_{ri}{}^{c}n_{rz} - y_{1c}^{i}y_{ri}{}^{c}o_{rz} + {}^{c}p_{ry} - y_{1c}^{i}{}^{c}p_{rz} = 0 \end{array} \right. \tag{3.10}
$$

ArUco码的角点共面且已知,每个角点可建立式(3.10)的方程组,整理得到

$$
A_{1}H_{1} + A_{2}H_{2} = 0 \tag{3.11}
$$

其中,  $A_{1}$  是  $8\times 3$  矩阵,  $A_{2}$  是  $8\times 6$  矩阵,  $H_{1}$  和  $H_{2}$  为向量,如下所示

$$
A_{1} = \left[ \begin{array}{cccc}x_{r1} & 0 & -x_{1c}^{1}x_{r1} \\ 0 & x_{r1} & -y_{1c}^{1}x_{r1} \\ \vdots & \vdots & \vdots \\ x_{r4} & 0 & -x_{1c}^{4}x_{r4} \\ 0 & x_{r4} & -y_{1c}^{4}x_{r4} \end{array} \right] \tag{3.12}
$$

$$
A_{2} = \left[ \begin{array}{cccccc}y_{r1} & 0 & -x_{1c}^{1}y_{r1} & 1 & 0 & -x_{1c}^{1} \\ 0 & y_{r1} & -y_{1c}^{1}y_{r1} & 0 & 1 & -y_{1c}^{1} \\ \vdots & \vdots & \vdots & \vdots & \vdots & \vdots \\ y_{r4} & 0 & -x_{1c}^{4}y_{r4} & 1 & 0 & -x_{1c}^{4} \\ 0 & y_{r4} & -y_{1c}^{4}y_{r4} & 0 & 1 & -y_{1c}^{4} \end{array} \right] \tag{3.13}
$$

$$
H_{1} = \left[ \begin{array}{ccc}{}^{c}n_{rx} & {}^{c}n_{ry} & {}^{c}n_{rz} \end{array} \right]^{T} \tag{3.14}
$$

$$
H_{2} = \left[ \begin{array}{cccccc}{}^{c}o_{rx} & {}^{c}o_{ry} & {}^{c}o_{rz} & {}^{c}p_{rx} & {}^{c}p_{ry} & {}^{c}p_{rz} \end{array} \right]^{T} \tag{3.15}
$$

而  $H_{1}$  为  ${}^{c}n_{r} = \left[ \begin{array}{ccc}{}^{c}n_{r} & {}^{c}n_{ry} & {}^{c}n_{rz} \end{array} \right]^{T}$ ,满足

$$
\left\| H_{1}\right\| = 1 \tag{3.16}
$$

通过构造目标函数  $F$ ,如下所示

$$
F = \left\| A_{1}H_{1} + A_{2}H_{2}\right\|^{2} + \lambda (1 - \left\| H_{1}\right\|^{2}) \tag{3.17}
$$

则以上求解可视作优化问题,在任意  $\lambda$  条件下使得  $F$  最小。则  $H_{1}$  和  $H_{2}$  的解由式(3.18)和式(3.19)给出,其中  $H_{1}$  为矩阵  $B$  的最小特征值所对应的特征向量。

$$
\left\{ \begin{array}{l}BH_{1} = \lambda H_{1} \\ H_{2} = -(A_{2}^{T}A_{2})^{-1}A_{2}^{T}A_{1}H_{1} \end{array} \right. \tag{3.18}
$$

其中

$$
B = A_{1}^{T}A_{1} - A_{1}^{T}A_{2}(A_{2}^{T}A_{2})^{-1}A_{2}^{T}A_{1} \tag{3.19}
$$

利用  $H_{1}$  和  $H_{2}$ ,可得到摄像机相对于  $M_{i}$  系的外参数矩阵  $^{c}M_{m_{i}}$ ,而  $^{c}M_{m_{i}}$  的第三

列(向量  $\boldsymbol{a}_{r} = \left[ \begin{array}{c c c}{c_{a_{r x}}} & {c_{a_{r y}}} & {c_{a_{r z}}} \end{array} \right]^{T}$ ) 可以由第一列和第二列做叉乘来求解。对于已知4个共面空间点的问题,式(3.11)有8个未知数,具有8个方程,因此能够利用上述方法对共面P4P问题进行线性求解。

# 3.3.3 多二维码数据融合方法

假设同一时刻有  $n$  个ArUco码被识别并测量得到其位姿,同一时刻下得到的对某状态量  $p$  的观测值  $p_{1},\ldots ,p_{n}(n\geq 2)$  是不相关随机变量,将多个观测值进行加权平均处理得到的最优估计  $\hat{p}$  是  $p$  的无偏估计,其表示如下

$$
\hat{p} = \sum_{i = 1}^{n}w_{i}p_{i} \tag{3.20}
$$

式中  $w_{i}$  为加权系数,需满足如下条件

$$
\left\{ \begin{array}{l}\sum_{i = 1}^{n}w_{i} = 1 \\ w_{i}\geq 0 \end{array} \right. \tag{3.21}
$$

估计误差  $\tilde{p}$  和其已知的条件如下所示

$$
\left\{ \begin{array}{l}\tilde{p} = p - \hat{p} \\ E(\tilde{p}) = 0 \end{array} \right. \tag{3.22}
$$

式中  $E(\tilde{p})$  为  $\tilde{p}$  的期望值。假设  $p_{1},\ldots ,p_{n}(n\geq 2)$  的方差  $\operatorname {Var}(p_{i})$  非零且有限,则有

$$
\operatorname {Var}\left(\sum_{i = 1}^{n}w_{i}p_{i}\right) = \sum_{i = 1}^{n}\operatorname {Var}(w_{i}p_{i}) = \sum_{i = 1}^{n}w_{i}^{2}\operatorname {Var}(p_{i}) \tag{3.23}
$$

通过拉格朗日乘子法可以证明[213],当加权系数  $w_{i}$  按照如下所示选择时,估计  $\hat{p}$  的方差可以最小化。

$$
w_{i} = \frac{\frac{1}{\operatorname{Var}(p_{i})}}{\sum_{j = 1}^{n}\frac{1}{\operatorname{Var}(p_{j})}} \tag{3.24}
$$

此时,估计  $\hat{p}$  的方差最小值为

$$
\operatorname {Var}(\hat{p})_{\min} = \frac{1}{\sum_{i = 1}^{n}\frac{1}{\operatorname{Var}(p_{i})}} \tag{3.25}
$$

因此,状态量  $p$  的最优估计量  $\hat{p}$  为

$$
\hat{p} = \frac{\frac{1}{\operatorname{Var}(p_{i})}}{\sum_{j = 1}^{n}\frac{1}{\operatorname{Var}(p_{j})}}\cdot p_{i} \tag{3.26}
$$

# 3.4 四旋翼无人机动力学模型

# 3.4.1 坐标系与坐标转换

本章采用了地理坐标系、导航坐标系、机体坐标系和地心坐标系。其中导航坐标系为北东地（North- East- Down, NED）坐标系，地心坐标系为WGS- 84（World Geodetic System 1984）坐标系，其采用的椭球参数如下[214]。

$$
a = 6378137.0 \tag{3.27}
$$

$$
f = \frac{1}{298.257223563} \tag{3.28}
$$

$$
b = a(1 - f) = 6356752.3 \tag{3.29}
$$

$$
e = \sqrt{\frac{a^2 - b^2}{a^2}} = 0.08181919 \tag{3.30}
$$

取WSG- 84坐标系的某点  $P^{e}(X_{p}^{e}, Y_{p}^{e}, Z_{p}^{e})$ ，地理坐标系与地心坐标系的转换为

$$
\begin{array}{r}X_{p}^{e} = (N_{e} + h_{p})\cos \phi_{p}\cos \lambda_{p}\\ Y_{p}^{e} = (N_{e} + h_{p})\cos \phi_{p}\sin \lambda_{p} \end{array} \tag{3.32}
$$

$$
Z_{p}^{e} = \left(\frac{b^{2}}{a^{2}} N_{e} + h_{p}\right)\sin \phi_{p} \tag{3.33}
$$

式中，  $(\lambda_{p},\phi_{p},h_{p})$  为该点的经纬度和海拔高度，  $N_{e}$  与纬度相关，如下所示

$$
N_{e} = \frac{a}{\sqrt{1 - e^{2}\sin^{2}\phi_{p}}} \tag{3.34}
$$

定义导航坐标系的原点在地心坐标系中表示为  $P_{0}^{e}(X_{0}^{e}, Y_{0}^{e}, Z_{0}^{e})$ ，该点的地理系坐标为  $(\lambda_{0},\phi_{0},h_{0})$ ，则地心坐标系下点  $P_{i}^{e}$  在导航坐标系下的坐标  $P_{i}^{n}$  表达为

$$
P_{i}^{n} = R_{e}^{n}(P_{i}^{e} - P_{0}^{e}) \tag{3.35}
$$

其中，  $R_{e}^{n}$  为

$$
R_{e}^{n} = \left[ \begin{array}{ccc} - \sin \phi_{0}\cos \lambda_{0} & -\sin \phi_{0}\sin \lambda_{0} & \cos \phi_{0}\\ -\sin \lambda_{0} & \cos \lambda_{0} & 0\\ -\cos \phi_{0}\cos \lambda_{0} & -\cos \phi_{0}\sin \lambda_{0} & -\sin \phi_{0} \end{array} \right] \tag{3.36}
$$

导航坐标系与机体坐标系的变换矩阵  $R_{n}^{b}$  由三个欧拉角  $(\psi ,\theta ,\phi)$  计算得到

$$
R_{n}^{b} = \left[ \begin{array}{ccc}\cos \theta \cos \psi & \cos \theta \sin \psi & -\sin \theta \\ -\cos \phi \sin \psi +\sin \phi \sin \theta \cos \psi & \cos \phi \cos \psi +\sin \phi \sin \theta \sin \psi & \sin \phi \cos \theta \\ \sin \phi \sin \psi +\cos \phi \sin \theta \cos \psi & -\sin \phi \cos \psi +\cos \phi \sin \theta \sin \psi & \cos \phi \cos \theta \end{array} \right]
$$

# 3.4.2 四旋翼无人机刚体动力学模型

四旋翼无人机刚体动力学模型如下：

$$
\left\{ \begin{array}{l}^{n}\dot{p} = ^{n}\nu \\ ^{b}\dot{\nu} = -[^{b}\omega ]_{x}^{b}\nu +gR_{n}^{b}e_{3} - \frac{f_{b}}{m} e_{3} + D \\ \dot{\Theta} = W\cdot^{b}\omega \\ J\cdot^{b}\dot{\omega} = -^{b}\omega \times (J\cdot^{b}\omega) + G_{a} + \tau \end{array} \right. \tag{3.38}
$$

式中  $^n p$  和  $^n\nu$  分别为无人机质心相对于NED系的位置和速度，  $^b\nu$  为无人机质心在机体坐标系下的速度，  $J$  为无人机的转动惯量，  $g$  为重力加速度，  $^b\omega$  为无人机在机体系下的角速度，  $[^{b}\omega ]_{x}$  为  $^b\omega$  的斜对称形式，  $e_{3}$  为单位向量  $\left[0\quad 0\quad 1\right]^T$  ，  $f_{b}$  为螺旋桨在机体系下的拉力，  $D$  为气动力阻力，  $G_{a}$  为陀螺力矩，  $\tau$  为螺旋桨在机体轴上产生的力矩，  $\Theta$  为欧拉角  $\left[\phi \quad \theta \quad \psi \right]^T$  ，  $W$  为转换矩阵表示为

$$
W = \left[ \begin{array}{ccc}1 & \tan \theta \sin \phi & \tan \theta \cos \phi \\ 0 & \cos \phi & -\sin \phi \\ 0 & \sin \phi /\cos \theta & \cos \phi /\cos \theta \end{array} \right] \tag{3.39}
$$

# 3.4.3 螺旋桨理论模型

四旋翼无人机一般采用定桨距螺旋桨，可采用如下公式计算

$$
T = C_{T}\rho \cdot (\frac{N}{60})^{2}D_{p}^{4} \tag{3.40}
$$

$$
M = C_{M}\rho \cdot (\frac{N}{60})^{2}D_{p}^{5} \tag{3.41}
$$

其中，  $N$  为螺旋桨转速，  $D_{p}$  为螺旋桨直径，  $C_{T}$  和  $C_{M}$  为拉力系数和转矩系数，  $\rho$  为空气密度，表示如下

$$
\rho = \frac{273P_{a}}{101325(273 + T_{t})}\rho_{0} \tag{3.42}
$$

$$
\rho_{0} = 1.283kg / m^{3} \tag{3.43}
$$

$$
P_{a} = 101325(1 - 0.0065\frac{h}{273 + T_{t}})^{5.2561} \tag{3.44}
$$

式中，为  $P_{a}$  该点大气压强，  $h$  和  $T_{t}$  为当前海拔高度和温度，  $\rho_{0}$  为标准大气密度。

对螺旋桨拉力的估算，最主要的就是求解拉力系数和扭矩系数，本文采用如下公式进行求解

$$
C_{T} = 0.25\pi^{3}\lambda \zeta^{2}B_{p}K_{0}\frac{\epsilon\arctan\frac{H_{p}}{\pi D_{p}} - \alpha_{0}}{\pi A + K_{0}} \tag{3.45}
$$

$$
C_{M} = \frac{1}{8A}\pi^{2}C_{d}\zeta^{2}\lambda B_{p}^{2} \tag{3.46}
$$

其中

$$
C_{d} = C_{fd} + \frac{\pi AK_{0}^{2}}{e}\cdot \frac{(\epsilon\arctan\frac{H_{p}}{\pi D_{p}} - \alpha_{0})^{2}}{(\pi A + K_{0})^{2}} \tag{3.47}
$$

本文所采用的是APC1046螺旋桨，  $K_{0}$  为螺旋桨叶片截面翼型的升力系数近似值，螺旋桨直径为  $D_{p} = 0.254m$  ，螺距为  $H_{p} = 0.11684m$  ，  $A$  为螺旋桨展弦比，  $\alpha_{0}$  为零升迎角，  $\epsilon$  是由下洗效应带来的校正因子，  $e$  为Oswald因子，  $\lambda$  和  $\zeta$  为修正系数，  $C_{d}$  为螺旋桨叶片的阻力系数，  $C_{fd}$  为叶片零升阻力系数。为了验证螺旋桨模型，这里选取一组适应性较广的平均参数如下所示

$$
\left\{ \begin{array}{l}A = 5 \\ \alpha_{0} = 0 \\ \epsilon = 0.85 \\ K_{0} = 6.11 \\ C_{fd} = 0.015 \\ e = 0.83 \\ \lambda = 0.75 \\ \zeta = 0.5 \end{array} \right. \tag{3.48}
$$

将上述螺旋桨参数代入理论模型中可以计算得出螺旋桨拉力和力矩的理论值，进一步地将理论值与螺旋桨厂家官方网站提供的APC1046二叶桨拉力和力矩实验数据进行对比，其拉力曲线和扭矩曲线如图3.16和图3.17所示。

![](images/2e69d95a271ab2494ee2307055a2d61be2dd00cefe84bb5044ab33c807a41272.jpg)  
图3.16 APC1046 桨拉力曲线与实验数据对比

![](images/d448d566f3234f5a78a467ea9b497cb1c39cf4e32578371ac385cea54fe7c33c.jpg)  
图3.17 APC1046 桨转矩曲线与实验数据对比

比较结果如图 3.16 和图 3.17 所显示，在螺旋桨常用的  $3000 \sim 6000$  转速区间，螺旋桨模型的计算结果和实验数据比较吻合，当螺旋桨转速超过 10000 转后，螺旋桨模型计算得到的拉力值和扭矩值与实验数据存在一定的偏差。主要原因在于近似模型与真实模型之间存在偏差，包括  $\alpha_0$  等螺旋桨参数取值与实际桨叶不同等，随着转速的增加，由不准确的螺旋桨参数和模型计算得到的力和力矩与真实值的偏差会越来越大。

# 3.4.4 四旋翼的布局和控制效率模型

四旋翼无人机的布局有以下两种形式，如图 3.18 所示。

![](images/d948a680d39447698612e06593fd2a3490ee48dd6fc9769c7aecc7e0a9c17d4e.jpg)  
图3.18 四旋翼的布局形式

针对这两种布局，螺旋桨作用在四旋翼上的拉力  $f_{T}$  均可如下表示

$$
f_{T} = \sum_{i = 1}^{4} c_{T} \pi_{i}^{2} \tag{3.49}
$$

其中， $c_{T}$  为与螺旋桨拉力系数相关的常数， $\pi_{i}$  为螺旋桨转速。

对于“十”字型布局的四旋翼，其控制效率矩阵如下所示

$$
\left[ \begin{array}{c}f_{T} \\ \tau_{x} \\ \tau_{y} \\ \tau_{z} \end{array} \right] = \left[ \begin{array}{cccc}c_{T} & c_{T} & c_{T} & c_{T} \\ 0 & -dc_{T} & 0 & dc_{T} \\ dc_{T} & 0 & dc_{T} & 0 \\ -c_{M} & c_{M} & -c_{M} & c_{M} \end{array} \right] \left[ \begin{array}{c} \pi_{1}^{2} \\ \pi_{2}^{2} \\ \pi_{3}^{2} \\ \pi_{4}^{2} \end{array} \right] \tag{3.50}
$$

其中，四旋翼无人机不相邻的两个螺旋桨之间的距离一般称为轴距，这里的轴距可表示为  $2d$ 。

本文采用的是“X”字型布局的四旋翼，其控制效率矩阵如下所示

$$
\left[ \begin{array}{c}f_{T}\\ \tau_{x}\\ \tau_{y}\\ \tau_{z} \end{array} \right] = \left[ \begin{array}{cccc}c_{T} & c_{T} & c_{T} & c_{T}\\ \frac{\sqrt{2}}{2} dc_{T} & -\frac{\sqrt{2}}{2} dc_{T} & -\frac{\sqrt{2}}{2} dc_{T} & \frac{\sqrt{2}}{2} dc_{T}\\ \frac{\sqrt{2}}{2} dc_{T} & \frac{\sqrt{2}}{2} dc_{T} & -\frac{\sqrt{2}}{2} dc_{T} & -\frac{\sqrt{2}}{2} dc_{T}\\ -c_{M} & c_{M} & -c_{M} & c_{M} \end{array} \right]\left[ \begin{array}{c}\pi_{1}^{2}\\ \pi_{2}^{2}\\ \pi_{3}^{2}\\ \pi_{4}^{2} \end{array} \right] \tag{3.51}
$$

# 3.4.5 动力系统模型

四旋翼无人机的动力系统由电池、无刷直流电机、电调和螺旋桨组成的。本文的动力系统配置方案为11.1V格式航模动力锂电池、新西达2212无刷直流电机、好盈电调（30A）和APC1046螺旋桨。该方案应用比较成熟，并且其所采用的配件性价比较高，成本可以控制在相对较低水平。

电调的功能则是把遥控器或飞行控制器输出的脉冲宽度调制（PulseWidthModulation，PWM）波指令信号进行放大，把直流电源转换为供给无刷电机使用的三相交流电源，从而驱动电机并调整转速。下图所示为四旋翼动力系统组成以及从遥控器指令到力和力矩的信号传递过程。

![](images/f6e6cce54902bd5ed3cd5f642431df650892734300f2e119ac9832ec530d0a68.jpg)  
图3.19四旋翼动力系统从遥控器指令到拉力和力矩的信号传递

电调接收遥控器或飞行控制系统发出的油门指令  $\sigma$  ，其为一个取值在  $0\sim 1$  之间的指令信号。  $U_{b}$  为电池输出电压，其在经过电调处理后则产生等效平均电压  $U_{m}$  从而驱使电机转动并调节到稳态转速  $\pi_{ss}$  ，这里给出其线性化的转换关系表达式如下

$$
U_{m} = \sigma U_{b} \tag{3.52}
$$

$$
\pi_{ss} = C_bU_m + \pi_b = C_R\sigma +\pi_b \tag{3.53}
$$

其中，  $C_{R} = C_{b}U_{b}$  ，  $C_b$  和  $\pi_{b}$  为常数，可由实验测量得出。而电机在收到油门指令后，达到稳态转速需要一段时间，其动态过程可简化为如下传递函数

$$
\pi = \frac{1}{T_{m}s + 1}\pi_{ss} \tag{3.54}
$$

其中，  $T_{m}$  为时间常数，根据上文所述的动力系统各单元的配置，经过实验测试，并对油门曲线进行线性拟合，各参数取值如下[215]

$$
\left\{ \begin{array}{l}C_{R} = 6432RPM \\ \omega_{b} = 1778RPM \\ T_{m} = 0.098s \end{array} \right. \tag{3.55}
$$

# 3.4.6 气动阻力模型

取  $V_{\mathrm{ab}}$  ，  $V_{\mathrm{ab}}$  ，  $V_{\mathrm{zb}}$  分别为四旋翼无人机沿机体轴  $O_{b}X_{b}$  ，  $O_{b}Y_{b}$  ，  $O_{b}Z_{b}$  的速度。考虑到“X”字型四旋翼的对称性，其阻力可以表示为如下形式

$$
\left\{ \begin{array}{l}f_{xb} = -k_{drag}^{x}V_{xb}^{2} \\ f_{yb} = -k_{drag}^{y}V_{yb}^{2} \\ f_{zb} = -k_{drag}^{z}V_{zb}^{2} \end{array} \right. \tag{3.56}
$$

其中，  $f_{xb}$  ，  $f_{yb}$  ，  $f_{zb}$  分别为沿机体轴  $O_{b}X_{b}$  ，  $O_{b}Y_{b}$  ，  $O_{b}Z_{b}$  的阻力，  $k_{drag}^{x}$  ，  $k_{drag}^{y}$  ，  $k_{drag}^{z}$  为常值阻力系数，根据其对称性，应取  $k_{drag}^{x} = k_{drag}^{y}$  。

# 3.5 四旋翼无人机控制系统

# 3.5.1 底层飞行控制框架

四旋翼无人机是一个欠驱动系统，有六个自由度（位置  $p\in \mathbb{R}^3$  和姿态  $\Theta \in \mathbb{R}^3$  ），只有四个独立输入（总拉力  $f_{T}\in \mathbb{R}$  和力矩  $\tau \in \mathbb{R}^3$  ）。因此，四旋翼只能跟踪四个期望指令（期望位置  $p_d\in \mathbb{R}^3$  和期望偏航角  $\psi_d\in \mathbb{R}$  ），剩余变量（期望滚转角  $\phi_d\in \mathbb{R}$  和期望俯仰角  $\theta_d\in \mathbb{R}$  ）由期望指令  $p_d$  和  $\psi_d$  确定。

对四旋翼无人机进行控制之前，比较重要的是导航系统要得到当前无人机状态，主要包括位置和姿态，如果对无人机状态估计不准确或者与实际值偏差较大，会直接导致无人机的控制出现漂移甚至失控。在导航信息比较准确的情况下，对四旋翼进行位置和姿态控制才是有效的。

位置环根据四旋翼当前位置  $p$  和期望位置  $p_d$  解算期望总拉力  $f_{d}$  、期望滚转角  $\phi_d$  和期望俯仰角  $\theta_d$  。姿态环则根据当前姿态角  $\phi$  、  $\theta$  、  $\psi$  和期望的姿态角  $\phi_d$  、  $\theta_d$  、  $\psi_d$  解算出期望的力矩  $\tau_d$  。在计算得到了四旋翼期望的力和力矩后，最终还是需要通过调整螺旋桨的转速来操纵无人机，而螺旋桨转速主要是根据电调的油门信号来调整的。因此，可以通过  $f_{d}$  和  $\tau_d$  计算期望的螺旋桨转速  $\sigma_{d,k}(k = 1,2,3,4)$ ，再进一步计算期望油门指令  $\sigma_{d,k}(k = 1,2,3,4)$ ，最终传递给电调以驱动电机调整转速。

四旋翼飞行控制系统的闭环结构如下图所示

![](images/93bbba8092d1a144e69594ba239b2d3321e07de199772f74dd2ff27108450f52.jpg)  
图3.20 四旋翼底层飞行控制的闭环结构

# 3.5.2 线性化模型

四旋翼无人机是一个非线性系统，这使得控制器设计十分复杂。考虑气动阻力  $D$  、陀螺力矩  $G_{a}$  和  ${}^{b}\omega \times (J\cdot{}^{b}\omega)$  项都比较小，可以忽略，从而得到导航坐标系中简化的位置方程如下式所示

$$
\left\{ \begin{array}{l}^{a}\dot{p}_{x} = -\frac{f_{T}}{m} (\sin \phi \sin \psi +\cos \phi \sin \theta \cos \psi) \\ {}^{a}\dot{p}_{y} = -\frac{f_{T}}{m} (-\sin \phi \cos \psi +\cos \phi \sin \theta \sin \psi) \\ {}^{a}\dot{p}_{z} = g - \frac{f_{T}}{m}\cos \phi \cos \theta \end{array} \right. \tag{3.57}
$$

考虑到四旋翼无人机能够处于准静止状态，其在飞行过程中，实际的俯仰角和滚转角都比较小，总拉力约等于四旋翼的重力，因此可使用如下所示的近似假设。

$$
\sin \phi \approx \phi ,\cos \phi \approx 1,\sin \theta \approx \theta ,\cos \theta \approx 1,f_{T}\approx mg \tag{3.58}
$$

简化后得到的水平方向的位置模型为

$$
\left\{ \begin{array}{l}\dot{p}_h = \nu_h \\ \dot{\nu}_h = -gA_\psi \Theta_h \end{array} \right. \tag{3.59}
$$

其中，  $p_h$  ，  $A_{\psi}$  ，  $\Theta_h$  的表达式如下

$$
p_h = \left[ \begin{array}{c}n \\ p_x \\ n \\ p_y \end{array} \right] \tag{3.60}
$$

$$
A_{\psi} = \left[ \begin{array}{cc}\sin \psi & \cos \psi \\ -\cos \psi & \sin \psi \end{array} \right] \tag{3.61}
$$

$$
\Theta_{h} = \left[ \begin{array}{c}\phi \\ \theta \end{array} \right] \tag{3.62}
$$

简化后得到的高度通道位置模型为

$$
\left\{ \begin{array}{l l}{\dot{p}_{z} = \nu_{z}}\\ {\dot{\nu}_{z} = g - \frac{f_{T}}{m}} \end{array} \right. \tag{3.63}
$$

线性化的姿态模型为

$$
\left\{ \begin{array}{l l}{\dot{\Theta} = ^{b}\omega}\\ {J\cdot^{b}\omega = \tau} \end{array} \right. \tag{3.64}
$$

# 3.5.3 位置控制器

设计位置控制器的目标是,当  $t \to \infty$  时,  $\left\| p(t) - p_{d}(t)\right\| \to 0$ ,即已知期望位置,再根据当下无人机的位置来控制无人机,使得无人机最终能达到与期望位置一致。针对线化后的水平位置模型,希望设计期望姿态角  $\Theta_{hd} = \left[\phi_{d} \quad \theta_{d}\right]^{T}$ ,使得

$$
\begin{array}{r}{\lim_{t\rightarrow \infty}\left\| e_{p h}(t)\right\| = 0} \end{array} \tag{3.65}
$$

其中,  $e_{ph} \triangleq p_{h} - p_{hd}$  。设计  $\nu_{h}$  的期望值  $\nu_{hd}$  为

$$
\nu_{hd} = K_{ph}(p_{hd} - p_{h}) \tag{3.66}
$$

其中,  $K_{ph}$  是水平位置控制环节的比例系数。定义  $e_{vh} \triangleq \nu_{h} - \nu_{hd}$ ,在假设  $\dot{p}_{hd} = 0_{2\times 1}$  的情况下,如果满足  $\lim_{t\to \infty}\left\| e_{vh}(t)\right\| = 0$ ,则  $\lim_{t\to \infty}\left\| e_{ph}(t)\right\| = 0$ 。

设计  $\Theta_{h}$  的期望值为  $\Theta_{hd}$ ,由于实际中的  $e_{ph}$  可能很大,导致生成的姿态角期望值是系统无法接受的,破坏了小角度假设,通过采用加饱和的比例积分微分(Proportional- Integral- Derivative,PID)控制器可以解决这类问题,其在水平位置通道上的表达式为

$$
\left\{ \begin{array}{l l}{e_{v h} = s a t_{g d}\left(\nu_{h} - \nu_{h d},a_{1}\right)}\\ {\Theta_{h d} = s a t_{g d}\left(g^{-1}A_{\psi}^{-1}(K_{v h}^{p}e_{v h} + K_{v h}^{i}\int e_{v h} + K_{v h}^{d}\dot{e}_{v h}),a_{2}\right)} \end{array} \right. \tag{3.67}
$$

其中,  $K_{vh}^{p}$ ,  $K_{vh}^{i}$ ,  $K_{vh}^{d}$  分别为水平速度控制环节的比例、积分和微分系数,  $a_{1}$  和  $a_{2}$  为与饱和相关的参数,  $sat_{gd}(x,a)$  为保方向饱和函数,定义如下

$$
sat_{gd} \triangleq \left\{ \begin{array}{ll} x, & \left\| x\right\|_{\infty} \leq a \\ a \frac{x}{\left\|x\right\|_{\infty}}, & \left\| x\right\|_{\infty} > a \end{array} \right. \tag{3.68}
$$

其中,  $a$  为与饱和相关的参数,定义  $x \triangleq \left[x_{1}\dots x_{n}\right]^{T}$ ,  $\left\| x\right\|_{\infty} \triangleq \max \left(\left|x_{1}\right|,\ldots ,\left|x_{n}\right|\right)$ 。在假设  $\dot{\nu}_{hd} = 0_{2\times 1}$  的情况下,如果满足  $\lim_{t\to \infty}\left\| \Theta_{h}(t) - \Theta_{hd}(t)\right\| = 0$ ,则  $\lim_{t\to \infty}\left\| e_{vh}(t)\right\| = 0$ 。

在高度通道上,同理可设计  $\nu_{z}$  的期望值  $\nu_{zd}$  如下所示

$$
\nu_{zd} = K_{pz}(p_{zd} - p_{z}) \tag{3.69}
$$

其中,  $K_{pz}$  是高度通道位置控制环节的比例系数。定义  $e_{\nu z} \triangleq \nu_{z} - \nu_{zd}$ , 在假设  $\dot{p}_{zd} = 0$  的情况下, 如果满足  $\lim_{t \to \infty} \left\| e_{\nu z}(t) \right\| = 0$ , 则  $\lim_{t \to \infty} \left\| e_{pz}(t) \right\| = 0$  。设计拉力  $f_{T}$  的期望值为  $f_{d}$ , 采用如下加饱和的PID控制器

$$
\left\{ \begin{array}{l l}{e_{\nu z} = s a t_{g d}(\nu_{z} - \nu_{z d},a_{3})}\\ {f_{d} = s a t_{g d}(m(g + K_{\nu z}^{p}e_{\nu z} + K_{\nu z}^{i}\int e_{\nu z} + K_{\nu z}^{d}\dot{e}_{\nu z}),a_{4})} \end{array} \right. \tag{3.70}
$$

其中,  $K_{\nu z}^{p}$ ,  $K_{\nu z}^{i}$ ,  $K_{\nu z}^{d}$  分别为高度通道速度控制环节的比例、积分和微分系数,  $a_{3}$  和  $a_{4}$  为与饱和相关的参数。在  $\dot{\nu}_{zd} = 0$  的假设下, 如果满足  $\lim_{t \to \infty} \left\| f_{T}(t) - f_{d}(t) \right\| = 0$ , 则  $\lim_{t \to \infty} \left\| e_{\nu z}(t) \right\| = 0$  。

# 3.5.4 姿态控制器

当位置控制器解释得到了期望的角度  $\Theta_{hd}$ , 而  $\psi_{d}$  根据任务单独给定, 姿态控制器的目的是设计力矩  $\tau$  的期望值  $\tau_{d}$ , 使得四旋翼无人机从当前姿态  $\Theta$  旋转达到期望的姿态角  $\Theta_{d} = \left[ \Theta_{hd}^{T} \psi_{d} \right]^{T}$  。定义  $e_{\Theta} \triangleq \Theta - \Theta_{d}$ , 设计角速度  $\omega$  的期望值为  $\omega_{d}$ , 如下所示

$$
\omega_{d} = -K_{\Theta}e_{\Theta} \tag{3.71}
$$

其中,  $K_{\Theta}$  为角度控制环节的比例系数。定义  $e_{\omega} \triangleq \omega - \omega_{d}$ , 在假设  $\dot{\Theta}_{d} = 0_{3 \times 1}$  的情况下, 如果满足  $\lim_{t \to \infty} \left\| e_{\omega}(t) \right\| = 0$ , 则  $\lim_{t \to \infty} \left\| e_{\Theta}(t) \right\| = 0$  。设计期望力矩  $\tau_{d}$ , 同样需要采用加饱和的PID控制器, 如下所示

$$
\left\{ \begin{array}{l l}{e_{\omega} = s a t_{g d}(\omega -\omega_{d},a_{5})}\\ {\tau_{d} = s a t_{g d}(-K_{\omega}^{p}e_{\omega} - K_{\omega}^{i}\int e_{\omega} - K_{\omega}^{d}\dot{e}_{\omega},a_{6})} \end{array} \right. \tag{3.72}
$$

其中,  $K_{\omega}^{p}$ ,  $K_{\omega}^{i}$ ,  $K_{\omega}^{d}$  分别为角速度控制环节的比例、积分和微分系数,  $a_{5}$  和  $a_{6}$  为与饱和相关的参数。在  $\dot{\omega}_{d} = 0_{3 \times 1}$  的假设下, 如果满足  $\lim_{t \to \infty} \left\| \tau (t) - \tau_{d}(t) \right\| = 0$ , 则  $\lim_{t \to \infty} \left\| e_{\omega}(t) \right\| = 0$  。

# 3.6 四旋翼无人机自主降落飞行试验及结果分析

# 3.6.1 飞行平台和试验环境搭建

为了进一步地测试本文所建立的无人机自主降落系统, 本文搭建了室内飞行试验环境和小型四旋翼无人机飞行平台, 整个试验场地采用尼龙网进行保护, 地

面铺设质地较软的泡沫板，防止无人机在试验过程中因摔落而损坏机载设备。由于在室内环境下，无人机无法通过卫星导航数据来实现对自身的定位，只能通过机载视觉系统对合作降落板目标进行识别和位姿估计，估计结果用于提供给四旋翼无人机的飞行控制器解算制导控制指令，从而能够实现无人机在降落过程中以全自主模式飞行。

图3.21所示为基于DJIF450无人机改装的飞行试验平台，其轴距为  $450\mathrm{mm}$ ，起飞重量为  $1.62\mathrm{kg}$ 。机载电脑和飞行控制器均安装在机架内部，摄像头固定安装在机身底部，朝无人机正下方进行拍摄。机载电脑通过实验室搭建的无线网络与地面站进行通信，由地面站远程控制机载电脑的用户界面来输入指令，同时可以实时观察机载摄像头画面并监测无人机的飞行状态。

![](images/09e5aebba48027040114e4964e43c26bf756879503f123664ba36701936a307a.jpg)  
图3.21室内环境下的无人机飞行试验平台搭建

该四旋翼无人机搭载的电池是  $3300\mathrm{mAh}$  锂电池，为动力系统和所有机载设备供电。为了精确地记录无人机在室内的飞行轨迹，本文在室内实验室搭建了动作捕捉系统，通过对无人机顶部三个感光球的检测来计算无人机相对于室内参考坐标系的位置和姿态，输出频率为  $100\mathrm{Hz}$ 。因为动作捕捉系统的估计精度非常高，可以作为真实值与本文机载视觉系统得到的位姿估计数据进行对比。但动作捕捉系

统的检测范围有限，只能在系统划定的区域内进行检测，因此无人机只能在较小的范围内进行飞行试验。

![](images/30ba3fb834fc225615608c326b757680df30e85301851aa380394f4106026030.jpg)  
图3.22所示为四旋翼无人机室内飞行试验系统的硬件架构。

在全自主飞行试验开始前，手动切换遥控器开关，使飞行控制器进入外部控制模式。此时，地面站与机载电脑通过无线网络连接，可以在地面端实时监控机载电脑的画面和程序的运行状态。本文首先进行定点悬停试验，无人机通过识别降落板标志，控制自身的位置和姿态，于降落板中心点上方固定高度定点悬停。随后，进行全自主地降落试验，无人机识别和跟踪降落板，接近目标时判断会合条件，最后自动关闭电源，完成降落动作。

飞行试验数据和机载摄像机的第一视角画面由机载计算机记录下来，无人机降落过程的视频由地面摄像机拍摄，图3.23所示为无人机在室内环境下的自主降落试验过程，相关试验视频已作为附件上传至公开网络[216]。

![](images/c1dbf023c0d6ca81c7b4f2a9b6512d540975d169afec7d9a5a946fade7e111a7.jpg)

![](images/d20db3c4f1b8448cfc32d8c535e0076e16e06ed6a40daa8d11ac12e62e45f59a.jpg)  
图3.23 室内环境下的无人机自主降落试验

# 3.6.2 定点悬停试验与结果分析

在室内环境下，GPS和气压高度传感器数据是不可用的，无人机只依靠陀螺仪、加速度计、磁力计和视觉系统提供导航信息，视觉系统的估计频率为  $25\mathrm{Hz}$ 。在这样的条件下，飞行控制器执行指令，将无人机控制在降落板中心点上方  $1.0\mathrm{m}$  处，在悬停过程中，无人机的偏航角锁定为  $0^\circ$ ，最后将试验过程中记录下的飞行数据与动作捕捉系统的真实值比较，如图3.24所示。

![](images/53f45f77b6c38f0ae5be831aa3a74a81a81c79ed76572d1b0b30de8916d79a11.jpg)  
图3.24 无人机定点悬停于降落板正上方  $1\mathrm{m}$  高度时的飞行试验数据

由上图可以看到，在82s的悬停测试过程中，系统分别记录下采用本章降落板标志和位姿估计算法得到的位姿估计值（蓝色实线）、单个二维码得到的位姿估计值（绿色实线）以及真实值（红色实线）。根据数据分析得出，采用本章提出的多层级嵌套二维码降落板标志和位姿估计算法，在X方向和Y方向的位置估计偏差分别为  $10.1\mathrm{mm}$  和  $19.1\mathrm{mm}$ ，XY平面位置估计偏差为  $21.6\mathrm{mm}$ ，3D位置估计偏差为  $22.2\mathrm{mm}$ ，如表3.1所示。可以看出，与采用单个二维码标志进行位姿估计得到的数据相比，采用本章所提出的方法在估计精度上有明显的提高。

表3.1降落板上方1m高度定点悬停的位姿估计均方根误差  

<table><tr><td>均方根误差（mm）</td><td>X</td><td>y</td><td>z</td><td>xy 平面</td><td>3D</td></tr><tr><td>单个二维码</td><td>30.8</td><td>44.0</td><td>8.9</td><td>53.7</td><td>54.4</td></tr><tr><td>嵌套二维码</td><td>10.1</td><td>19.1</td><td>5.5</td><td>21.6</td><td>22.2</td></tr></table>

在悬停试验过程中，因为滚转角和俯仰角都控制在很小的范围内，因此无人机姿态的抖动并没有引起明显的估计误差。主要的位姿估计误差是图像背景噪声、PnP方程求解的误差以及摄像机畸变模型不准确等因素造成的。但对于如此低成本的摄像机来说，目前所发现的位置估计和位置控制精度已达到比较可观的水平，能够满足实际使用需要。

# 3.6.3 自主降落试验与结果分析

本节对两次无人机自主降落试验的结果进行分析，试验过程中降落板固定在静止的无人小车上。第一次试验开始前，无人机在相对于降落板中心位置(0,0,1900mm)处悬停，当操作遥控器将飞行控制器切换至外部控制模式后，无人机进入全自主状态飞行。第一次试验，无人机在设定点悬停7s后收到降落指令，整个下降高度的过程用了8s，随后系统开始判断是否满足会合条件，最后在44s左右关闭电源。如图3.25所示，无人机最终的落点位于降落板（- 35.5mm,106.1mm）处，非常接近降落板的中心点。

![](images/ef7e4bb9358c6a9a35d96665d79ed007dc1877d2097e5ba80561962dd31c8651.jpg)  
第83页

![](images/3350b361324191699da2068791d74d9e30b3c5fbe75447a9ed6b82328df2af83.jpg)  
图3.25无人机从  $1.9\mathrm{m}$  高度进行自主降落的飞行试验数据

第二次试验，无人机成功地实现了自主起飞、悬停、移动和降落全过程。起飞阶段，无人机执行一键起飞命令，悬停于降落板上方(0,0,1900mm)处，随后由地面站发送指令向  $(550\mathrm{mm}, - 550\mathrm{mm},2100\mathrm{mm})$  点移动并定点悬停，试验开始后60s左右，执行降落指令，并最终于72.5s左右关闭电源，完成降落动作。无人机最终的落点位于降落板  $(21.9\mathrm{mm}, - 131.4\mathrm{mm})$  处，如图3.26所示。

![](images/7172e90dc00e60257d4608d3d02229e64172eb9077238876eb440683dbbcb4e5.jpg)  
图3.26无人机进行自主起飞、悬停、移动、降落全过程的飞行试验数据

在控制系统的作用下，四旋翼姿态的快速变化会使摄像机的画面产生抖动，因此在当前试验中的位姿估计值仍然存在一些振荡。配置图像处理能力能强的硬件设备可以提高视觉系统的估计频率，有利于进一步提高控制精度。事实上，考虑到所有传感器和设备的成本都比较低，该视觉系统的性能和所实现的自主降落精度，在实际应用中是比较有吸引力的。

# 3.7 考虑视角和落角约束的偏置比例导引律

# 3.7.1 捷联式摄像头视角和落角约束问题

由于摄像机位于无人机机身底部，固定安装且镜头朝下，其可视范围受到其最大视场角的限制。在末制导阶段，无人机与目标在相对运动过程中，若目标超出摄像机视野，即超过摄像机视角限制时，制导系统就无法继续工作。因此，对于低成本捷联式摄像头，为了保证制导系统能够持续稳定工作从而始终保持无人机对目标的跟踪，则必须确保目标处于摄像机的视场内。

下面通过图3.27来描述目标跟踪的过程，并进一步地说明捷联式摄像头的视角约束问题。

![](images/969c2c358b4392d4033e4d83da1a4f37294d9ea0ba83ebe16d91a76ea4c54abe.jpg)  
图3.27采用捷联式摄像头的无人机与目标相对运动关系

图3.27中忽略了无人机机体坐标系与摄像机坐标系的原点位置偏差，且只考虑了纵平面内的运动，其中  $A$  表示无人机，  $T$  表示降落平台，  $AT$  表示视线方向（Line- of- sight, LOS），  $\lambda$  为视线角，  $r$  为相对距离，取连线  $AT$  与摄像机光轴  $O_{c}Z_{c}$  之间的夹角设为目标在摄像机中的视角  $\kappa$  ，假设摄像机的最大视场角  $\eta_{\mathrm{max}}$  为  $120^{\circ}$  ，则由无人机与降落平台相对运动引起的视角  $\kappa$  变化应始终控制在不超过  $60^{\circ}$  的范围内。  $\theta$  和  $\theta_{a}$  分别为无人机的俯仰角和速度倾角，设降落平台始终沿水平方向运动。

纯追踪法（Pure Pursuit Law, PPL）是早期出现的一种导引方法，这种导引规则也称为“猎犬追兔”（Hound- Hare Pursuit），其始终控制追踪者的速度矢量始

终指向目标，即与视线方向一致。在拦截机动目标时，纯追踪法的精度较低，一般只用于追踪低速运动或静止目标。

平行导引法（Parallel Navigation），视线相对于惯性坐标系保持恒定，在导引期间，该规则可表示为

$$
\dot{\lambda} (t) = 0 \tag{3.73}
$$

比例导引（Proportional Navigation, PN）是最经典的末制导方法。根据平行导引法，视线角速度必须为零，而实际中，视线角速度通常不为零。而比例导引律的加速度指令  $a_{c}(t)$  如下所示。

$$
a_{c}(t) = N\nu_{cl}\dot{\lambda} (t) \tag{3.74}
$$

其中，  $N$  为有效导引比，  $\nu_{cl}$  为接近速度。

为了进一步研究末制导阶段无人机与目标的相对运动问题，根据3.4节和3.5节的内容，在Matlab- Simulink环境下搭建了四旋翼无人机六自由度动力学模型，对采用不同制导方法的飞行控制系统进行仿真试验。在本地为北东地坐标系下，设无人机的初始位置为  $(0,0, - 154m)$ ，无人机的初始速度为  $10\mathrm{m / s}$  朝向目标，目标初始位置为  $(150\mathrm{m},0, - 4m)$ ，目标初始速度为  $8\mathrm{m / s}$ ，朝北偏东  $51.3^{\circ}$  方向匀速直线运动。在仿真过程中，假设视线角和视线角速度是已知的，分别根据追踪法和比例导引两种方法生成四旋翼无人机加速度指令，其中有效导引比为  $N = 3$  。设置仿真结束条件为，无人机与目标的水平距离小于  $\sqrt{2} m$  且相对高度小于  $3\mathrm{m}$ ，当满足结束条件时即认为末制导阶段结束，无人机进入目标跟踪阶段，通过近距离识别降落板标志来完成其与降落平台的会合任务。

图3.28所示为采用纯追踪法和比例导引的末制导段仿真结果，两种方法都未考虑视角和落角约束问题，其中绿色实线为采用纯追踪法的仿真结果，红色实线为采用真比例导引律（True Proportional Navigation Guidance, TPNG）[217]的仿真结果。

![](images/4ea4a6e6a624548309741483efaa56575b4d7d906a0499c9dcf2d1cbf32d0162.jpg)

![](images/eec71fed0d269f0edede8ea656884f85c790a63d719e4604d21312a2742759b2.jpg)  
图3.28采用纯追踪法和真比例导引律的仿真结果

由上图可得，采用纯追踪法和真比例导引律分别于109.2s和70.33s到达目标区域，仿真结束时的视线角分别为  $- 0.53^{\circ}$  和  $- 35.67^{\circ}$ ，飞行过程中，无人机纵向最大视角分别为  $94.01^{\circ}$  和  $59.43^{\circ}$ 。对于自主降落问题，无人机相对于降落平台垂直地接近为最佳状态，即要求终端时刻期望的视线角为  $- 90^{\circ}$ 。采用真比例导引律更快接近目标，但由于没有对视角进行约束，飞行过程中容易丢失目标。而由于纯追踪法的特点，在接近目标时，视线角趋近于  $0^{\circ}$ ，在摄像头固定朝下安装的情况

下，目标已超出摄像机视野。由该仿真结果可知，在设计制导律时应保证飞行全程满足摄像机视角约束，并以更佳的落角接近降落平台。

# 3.7.2 偏置比例导引律

比例导引律对于运动目标的拦截非常有效，但由于经典的比例导引在解算制导指令时未考虑视角以及落角约束问题，因此，在实际应用中可以对比例导引律进行适当的改进。偏置比例导引律（Biased Proportional Navigation Guidance, BPNG）的思想就是在比例导引的基础上，增加一个随时间变化或根据某种逻辑变换的偏置项，来改进比例导引在实际应用中的性能，从而满足视角和落角的约束，其表达式如下所示[218]。

$$
a_{c}(t) = N\nu_{cl}(\dot{\lambda} +b) \tag{3.75}
$$

其中，  $b$  是待设计的时变偏置项，以下分两种情况对其进行设计。

情况一：飞行过程中无人机视角  $\kappa$  未达到边界值  $\kappa_{\mathrm{max}}$  。

在该情况下，目标保持在摄像头视野中，满足无人机的视角约束条件，偏置项的设计应主要考虑无人机以期望的落角到达终端，则设计时变项  $b(t)$  为如下表达式

$$
b(t) = b_{1} = \frac{\eta\nu_{cl}(\lambda_{d} - \lambda)}{Nr\cos(\theta_{a} - \lambda)} \tag{3.76}
$$

其中，  $\lambda$  为飞行过程中无人机与目标的视线角，  $\lambda_{d}$  为终端期望视线角，  $\eta$  为常值参数，  $r$  为无人机与降落平台的相对距离，这里采用已知的无人机对地高度  $H_{a}$  、降落平台高度  $H_{t}$  和视线角  $\lambda$  来估算，表达如下

$$
r(t)\approx \left|\frac{H_a - H_t}{\sin\lambda}\right| \tag{3.77}
$$

由式（3.76）可知，当视线角与期望视线角的偏差越大，则偏置项的作用越大，并且随着无人机越来越接近目标，  $r$  逐渐减小，也会进一步增加偏置项的作用，从而使无人机满足终端落角约束。

情况二：飞行过程中某时刻无人机视角  $\kappa$  达到边界值  $\kappa_{\mathrm{max}}$  。

边界值的取值小于实际的最大视角约束，在此情况下，无人机视角已经达到边界值，为防止无人机视角进一步超过其边界使得目标在摄像头视野中丢失，时变项  $b(t)$  应切换为如下表达式

$$
b(t) = b_{2} = \xi (\kappa -\kappa_{\mathrm{max}}) \tag{3.78}
$$

当无人机视角  $\kappa$  进一步超过边界值  $\kappa_{\mathrm{max}}$  时，时变项  $b(t)$  的作用增大，使无人机朝着减小视角的方向运动。在无人机制导控制系统的作用下，当无人机的视角  $\kappa$  小

于边界值  $\kappa_{\mathrm{max}}$  时，时变项  $b(t)$  则由  $b_{2}$  切换为  $b_{1}$ ，以保证无人机的终端落角约束得到满足。

综合以上两种情况，本节所提出的考虑视角和落角约束的偏置比例导引律表示如下

$$
a_{c}(t) = \left\{ \begin{array}{ll}N\nu_{cl}(\dot{\lambda} +b_{1}) & \text{初始阶段} \\ N\nu_{cl}(\dot{\lambda} +b_{2}) & \text{当} |\kappa (t)|\geq \kappa_{\max} \\ N\nu_{cl}(\dot{\lambda} +b_{1}) & \text{当} |\kappa (t)|< \kappa_{\max} \end{array} \right. \tag{3.79}
$$

由于无人机的机动能力是有限的，加速度指令  $a_{c}(t)$  不应大于无人机最大机动能力，因此需对其进行如下限幅

$$
a_{c}\leq a_{\mathrm{max}} \tag{3.80}
$$

为了进一步地说明该偏置比例导引律的应用效果，将该方法与同类方法进行对比。杨俊鹏等人[219]，提出了积分比例导引律（Integral Proportional Navigation Guidance, IPNG），其表达式如下

$$
a_{c}(t) = \nu_{cl}[k_{1}\dot{\lambda} +k_{2}\kappa +k_{3}(\lambda -\lambda_{d})] \tag{3.81}
$$

式中，  $k_{1}$  、  $k_{2}$  和  $k_{3}$  均为该制导律的参数，  $\kappa$  和  $(\lambda - \lambda_{d})$  相当于增加了两个分别考虑视角和落角约束的积分项，从而对比例导引律进行修正。

覃天等人[220]，提出了一种落角控制导引律（Impact Angle Control Navigation Guidance, IACNG），其表达式如下

$$
a_{c}(t) = \frac{\theta_{d} - \theta_{a}}{\lambda_{d} - \lambda_{c}} \nu_{cl}\dot{\lambda} + K(\lambda -\lambda_{d}) \tag{3.82}
$$

其中，  $\theta_{d}$  为终端的期望速度倾角，  $K$  为常系数，该方法主要考虑在有限时间内，使得视线角和速度倾角都能在终端达到期望值。

# 3.7.3 追踪地面移动平台模拟仿真

本文对所提出的偏置比例导引律（BPNG），以及纯追踪法（PPL）、真比例导引律（TPNG）、积分比例导引律（IPNG）和落角控制导引律（IACNG）在Matlab环境中开展飞行仿真试验。无人机初始位于  $(0,0, - 154m)$  点，目标位于  $(150,0, - 4m)$  点，降落平台以匀速直线运动和匀加速直线运动两种状态向前移动，无人机以  $- 90^{\circ}$  的期望落角接近两种运动状态的降落平台。

状态一（非机动运动目标）：降落平台以  $8\mathrm{m / s}$  朝正北方向匀速直线运动，相应的制导律参数选取如表3.2所示。仿真结束条件为：无人机与目标的水平距离小于  $\sqrt{2} m$  且相对高度小于  $3\mathrm{m}$  。无人机的初始状态、目标的运动状态、视角和落角约束等参数，如表3.3所示。

<table><tr><td colspan="4">表3.2 制导律参数</td></tr><tr><td colspan="4">真比例导引律参数（TPNG）</td></tr><tr><td>N=3</td><td colspan="3"></td></tr><tr><td colspan="4">积分比例导引律参数（IPNG）</td></tr><tr><td>k1=3</td><td>k2=-0.1</td><td>k3=0.025</td><td></td></tr><tr><td colspan="4">落角控制导引律参数（LACNG）</td></tr><tr><td>K=1.3</td><td colspan="3"></td></tr><tr><td colspan="4">偏置比例导引律参数（BPNG）</td></tr><tr><td>N=3</td><td>η=-0.0035</td><td>ξ=-0.14</td><td></td></tr><tr><td colspan="4">表3.3 仿真模型相关参数</td></tr><tr><td>模型参数</td><td>参数值</td><td>模型参数</td><td>参数值</td></tr><tr><td>无人机初始速度</td><td>10m/s</td><td>目标运动速度</td><td>8m/s</td></tr><tr><td>无人机初始速度倾角</td><td>-45°</td><td>目标初始位置</td><td>(150m,0,-4m)</td></tr><tr><td>视角限制边界值Kmax</td><td>45°</td><td>无人机初始位置</td><td>(0,0,-154m)</td></tr><tr><td>摄像机视场角</td><td>120°</td><td>终端期望视线角λd</td><td>-90°</td></tr></table>

各制导方法的飞行仿真试验结果如图3.29所示。

![](images/cf0ea68b1ccfe09ce7a4b7357d289b3df5d7e3f48375692ca8d9bb5273c97408.jpg)

![](images/fb018f3b86ecb1ec68b1f5f162b1bfcdd1e11dcae5e8805426639999a69425fb.jpg)  
图3.29 无人机追踪非机动移动平台的仿真结果

状态二（机动运动目标）：降落平台的初速度为  $5\mathrm{m / s}$  并以  $0.4\mathrm{m / s}^2$  的加速度朝正北方做匀加速直线运动，20s后再以  $- 0.4\mathrm{m / s}^2$  的加速度减速至  $0\mathrm{m / s}$ ，等待无人机降落。相应的制导律参数选取以及相关参数与表3.2和表3.3一致。仿真结束条件为：无人机与目标的水平距离小于  $\sqrt{2} m$  且相对高度小于  $3\mathrm{m}$ 。

采用上述各制导方法分别对无人机降落过程进行仿真测试，其试验结果如图3.30所示。

![](images/a06e049ead972ab940d5c99e30b03b0df4ee77b458629b1abd325c855a722634.jpg)  
第91页

![](images/fcfda5e6b741d7fc293cd0e455d7b3c294101f5feb396525e6ed51314e88d161.jpg)  
图3.30无人机追踪机动移动平台的仿真结果

由状态一和状态二的仿真结果可知，纯追踪法由于其局限性，对于运动目标的追踪性能有限，拦截目标的时间最长，分别为115.00s和42.43s，飞行过程中目标已超出摄像头视野，在接近降落平台时的终端落角分别为  $- 0.55^{\circ}$  和  $- 20.03^{\circ}$ ，与期望落角差距最大。真比例导引由于没有考虑视角和落角约束，在飞行过程中，存在丢失目标的风险，实际的落角与期望落角也有一定差距。IPNG、IACNG和BPNG方法都能满足飞行全程的视角约束条件，收敛速度也比较快，但采用BPNG

方法的终端落角更接近于期望值，状态一和状态二下的终端落角达到了- 89.83°和- 89.76°，都优于其他制导方法。以上各种制导方法在状态一和状态二下的仿真时间、视角和落角统计如表3.4和表3.5所示。

表3.4跟踪非机动移动平台（状态一）的各制导方法对比  

<table><tr><td>制导律</td><td>PPL</td><td>TPNG</td><td>IPNG</td><td>IACNG</td><td>BPNG</td></tr><tr><td>仿真时间</td><td>115.00s</td><td>60.37s</td><td>33.63s</td><td>34.40s</td><td>41.17s</td></tr><tr><td>最大视角（纵向）</td><td>93.97°</td><td>61.34°</td><td>56.59°</td><td>56.98°</td><td>57.45°</td></tr><tr><td>终端落角（视线角）</td><td>-0.55°</td><td>-35.76°</td><td>-80.64°</td><td>-83.71°</td><td>-89.83°</td></tr></table>

表3.5跟踪机动移动平台（状态二）的各制导方法对比  

<table><tr><td>制导律</td><td>PPL</td><td>TPNG</td><td>IPNG</td><td>IACNG</td><td>BPNG</td></tr><tr><td>仿真时间</td><td>42.43s</td><td>38.11s</td><td>35.70s</td><td>38.55s</td><td>32.17s</td></tr><tr><td>最大视角（纵向）</td><td>79.05°</td><td>62.77°</td><td>57.99°</td><td>57.11°</td><td>59.07°</td></tr><tr><td>终端落角（视线角）</td><td>-20.03°</td><td>-43.43°</td><td>-61.10°</td><td>-77.38°</td><td>-89.76°</td></tr></table>

多旋翼无人机与固定翼无人机的不同点在于其能够保持在准静止状态，采用直接力进行控制，机动性比较强。基于本节提出的带视角和落角约束的偏置比例方法，充分利用了多旋翼无人机的特点，将对视角的控制和满足终端落角约束分为不同的阶段来执行，当无人机视角达到甚至超过边界值  $\kappa_{\mathrm{max}}$  时，则调动多旋翼的最大机动能力优先满足对视角的控制，而当视角小于边界值  $\kappa_{\mathrm{max}}$  时，则以满足终端的落角约束为主要目的来生成制导指令。该方法在跟踪两种状态下移动平台的仿真结果均能满足对视角和落角的约束，性能优于其它同类对比方法，在实际应用中能够更好地满足无人机自主降落任务的要求。

# 3.8 小结

本章基于合作嵌套二维码标志，设计了一种多层级图案的降落板，建立了车载式微小型无人机自主降落系统，并进行了仿真试验与飞行试验。试验结果表明，本章所提出的算法能准确地估计降落平台的位姿，并能控制无人机自主地降落在车载平台上，证明了该降落引导系统的可行性。主要研究工作及结论如下：

（1）分析了ArUco二维码的检测和识别原理，基于嵌套二维码设计了一种多层级图案的降落板标志，在地面测试了该标志在不同距离处的检测效果，测试结果表明，该降落板标志相比单一的二维码在近距离下的检测成功率更好，并且增加了其可检测范围。分析了共面P4P问题的求解过程，提出了一种基于多数据融合的降落板位姿估计方法，并在机载计算机中部署算法，基于pixhawk控制器搭建了车载式微小型多旋翼无人机自主降落系统，并通过室内环境下的飞行试验证明了该降落引导系统的可行性，同时也验证了该位姿估计方法较单一数据来源的

估计方法更准确；

（2）分析了四旋翼无人机的布局，对动力系统进行建模，将螺旋桨的拉力与转矩仿真曲线与实验数据进行对比，结果表明该模型的计算结果与实验值比较接近。在Matlab/Simulink中建立了四旋翼无人机飞行控制系统框架，设计了位置控制器和姿态控制器，搭建了六自由度动力学仿真模型；

（3）针对末制导阶段的视角和落角约束问题，提出了一种带视角和落角约束的偏置比例导引律，并与同类型末制导方法进行了对比分析，利用四旋翼动力学模型进行仿真试验。仿真结果表明，该方法对视角的控制满足要求，终端落角相比其它方法更接近于期望落角，验证了该方法的实用性和高效性。

# 第四章 基于深度学习的非合作目标并行检测与跟踪方法

# 4.1 引言

合作目标可根据事先已知的尺寸、图案、颜色等信息实现精准识别，在识别基础上利用所设计的特征信息对其位姿进行估计。而非合作目标由于没有任何已知信息，只能在实时图像中提取目标的轮廓作为目标样本，并以此对跟踪器进行训练。传统目标跟踪方法的优点在于能够以较高频率对运动目标进行连续估计，但当画面中的目标模型存在尺度、光照、及旋转等变化，特别是目标从摄像机视野中消失时，传统跟踪方法会产生较大误差，并且无法重新搜索捕获目标。因此，为了克服传统跟踪方法的不足，可以引入经过大量样本数据训练的检测器，建立检测器和跟踪器的并行框架，利用检测算法低漂移的特点来辅助跟踪器提高准确度，并在跟踪出现异常时，能够对丢失的目标进行重新搜索。

本章针对非合作目标的长时间跟踪问题，对目前主流算法存在的不足进行深入分析，采用尺度自适应方法来提高预测目标框与真实目标的吻合度。由于非合作目标缺乏样本数据，需要用户手动地框选目标才能完成跟踪器的初始化，初始框的选取对后续跟踪的影响比较大。针对这类问题，本章对深度学习网络进行研究，经过大量样本训练可以得到对特定物体的准确检测，在跟踪过程中引入检测器，提出基于深度学习的非合作目标并行检测与跟踪框架，并测试算法的实际应用效果。

# 4.2 基于尺度自适应核相关滤波器的目标跟踪

# 4.2.1 相关滤波器基础理论

近年来，相关滤波器（Correlation Filter, CF）在目标跟踪领域的应用取得了不错的效果，在多次国际挑战赛中，该类方法在速度和精度方面都名列前茅，与众多主流方法相比有不小的优势。其思路是通过训练一个滤波器作用于待测样本，其表达如下

$$
g = f\otimes h \tag{4.1}
$$

其中，当滤波器  $h$  作用于图像  $f$  上时，可以得到响应  $g$ ，认为响应值最大处为目标的位置估计值。

由上式可知，滤波器是实现估计的决定因素，在初始阶段，需要根据事先给定的目标图片对滤波器进行初始化，并在后续的跟踪过程中不断更新。对上式进行快速傅立叶变换（Fast Fourier Transform, FFT），可以极大地提高运算速度，其

表达式为

$$
G = F\odot H^{*} \tag{4.2}
$$

式中，  $G$  、  $F$  和  $H$  分别为  $g$  、  $f$  和  $h$  傅立叶变换，  $H^{*}$  为  $H$  的复共轭。

则接下来的主要任务就是对滤波器模板的训练，为了实现对目标位置的连续预测，可以采用一系列的训练图像  $f_{i}$  和由真实值得到的响应  $g_{i}$ ，来对滤波器进行训练，其傅立叶变换形式如下

$$
H_{i}^{*} = \frac{G_{i}}{F_{i}} \tag{4.3}
$$

为了找到一个滤波器能够把训练的输入值映射到期望的输出值，可以通过对实际输出值和期望值之间的误差平方最小化，从而确定最佳的滤波器模板  $H$ ，如下所示

$$
\min_{H^{*}}\sum_{i}\left|F_{i}\odot H^{*} - G_{i}\right|^{2} \tag{4.4}
$$

进一步地把该问题变换为优化  $H$  中每个元素的问题

$$
H_{w\nu} = \min_{H_{w\nu}}\sum_{i}\left|F_{i\nu \nu}H_{w\nu}^{*} - G_{i\nu \nu}\right|^{2} \tag{4.5}
$$

式中，  $w$  和  $\nu$  是元素  $H_{w\nu}$  的索引。

为了实现最小化，则对式（4.5）求偏导，并令偏导数为0，得到

$$
0 = \frac{\partial}{\partial H_{w\nu}^{*}}\sum_{i}\left|F_{i\nu \nu}H_{w\nu}^{*} - G_{i\nu \nu}\right|^{2} \tag{4.6}
$$

上式可以进行如下变换

$$
0 = \frac{\partial}{\partial H_{w\nu}^{*}}\sum_{i}(F_{i\nu \nu}H_{w\nu}^{*} - G_{i\nu \nu})(F_{i\nu \nu}H_{w\nu}^{*} - G_{i\nu \nu})^{*} \tag{4.7}
$$

$$
0 = \frac{\partial}{\partial H_{w\nu}^{*}}\sum_{i}[(F_{i\nu \nu}H_{w\nu}^{*})(F_{i\nu \nu}H_{w\nu}^{*})^{*} - (F_{i\nu \nu}H_{w\nu}^{*})G_{i\nu \nu}^{*} - G_{i\nu \nu}(F_{i\nu \nu}H_{w\nu}^{*})^{*} + G_{i\nu \nu}G_{i\nu \nu}^{*}] \tag{4.8}
$$

$$
0 = \frac{\partial}{\partial H_{w\nu}^{*}}\sum_{i}F_{i\nu \nu}F_{i\nu \nu}^{*}H_{w\nu}H_{w\nu}^{*} - F_{i\nu \nu}G_{i\nu \nu}^{*}H_{w\nu}^{*} - F_{i\nu \nu}^{*}G_{i\nu \nu}H_{w\nu} + G_{i\nu \nu}G_{i\nu \nu}^{*} \tag{4.9}
$$

计算偏导数，得到

$$
0 = \sum_{i}[F_{i\nu \nu}F_{i\nu \nu}^{*}H_{w\nu} - F_{i\nu \nu}G_{i\nu \nu}^{*}] \tag{4.10}
$$

可以对元素  $H_{w\nu}$  进行求解，得到

$$
H_{w\nu} = \frac{\sum_{i}F_{i\nu\nu}G_{i\nu\nu}^{*}}{\sum_{i}F_{i\nu\nu}F_{i\nu\nu}^{*}} \tag{4.11}
$$

最后，可以得到  $H$  和  $H^{*}$  的表达式如下

$$
H = \frac{\sum_{i}F_{i}\odot G_{i}^{*}}{\sum_{i}F_{i}\odot F_{i}^{*}} \tag{4.12}
$$

$$
H^{*} = \frac{\sum_{i}G_{i}\odot F_{i}^{*}}{\sum_{i}F_{i}\odot F_{i}^{*}} \tag{4.13}
$$

当图像中出现目标后，可以选取目标框并对图片进行变换，从而获得多个训练输入图像  $f_{i}$  。训练输出  $g_{i}$  则是由高斯函数产生，其峰值与目标中心真实位置相对应。在目标跟踪过程中，目标的外形往往会发生改变，比如旋转、位移、尺度以及光照变化等。滤波器为了快速适应目标的这种改变，可以采用如下表达式进行更新

$$
H_{i}^{*} = \frac{A_{i}}{B_{i}} \tag{4.14}
$$

其中，分子  $A_{i}$  和分母  $B_{i}$  的表达式为

$$
\begin{array}{r}A_{i} = \eta_{LR}G_{i}\odot F_{i}^{*} + (1 - \eta_{LR})A_{i - 1}\\ B_{i} = \eta_{LR}F_{i}\odot F_{i}^{*} + (1 - \eta_{LR})B_{i - 1} \end{array} \tag{4.16}
$$

式中，  $\eta_{LR}$  为学习率（Learning Rate, LR），  $A_{i - 1}$  和  $B_{i - 1}$  分别为上一帧的分子和分母，  $A_{i}$  和  $B_{i}$  分别为当前帧的分子和分母。

# 4.2.2 基于核相关滤波器的目标跟踪方法

基于核相关滤波器（KCF）的目标跟踪算法于2014年提出[121]，由于其在跟踪速度和精度上都有着十分亮眼的表现，被广泛应用于工程领域。KCF跟踪器把样本训练的过程考虑为岭回归问题，并且所有的训练样本都是基础样本经过循环变换得到的。假设代表目标块的基础样本为一个  $\mathbf{n}\times 1$  向量  $\boldsymbol {x} = \left[x_{1}x_{2}\dots x_{n}\right]^{T}$  通过矩阵  $P$  的作用以对基础样本进行变换，如下所示

$$
P x = \left[x_{n}x_{1}\dots x_{n - 1}\right]^{T} \tag{4.17}
$$

其中，  $P$  为置换矩阵，如下所示

$$
P = \left[ \begin{array}{lllll}0 & 0 & 0 & \dots & 1\\ 1 & 0 & 0 & \dots & 0\\ 0 & 1 & 0 & \dots & 0\\ \vdots & \vdots & \ddots & \ddots & \vdots \\ 0 & 0 & \dots & 1 & 0 \end{array} \right] \tag{4.18}
$$

置换矩阵  $P$  作用的次数越多，则样本产生的位移量也越大，将这些变换得到的向量组合在一起可以得到如下所示的循环矩阵  $X$

$$
X = C(x) = \left| \begin{array}{c}(P^{0}x)^{T}\\ (P^{1}x)^{T}\\ (P^{2}x)^{T}\\ \vdots \\ (P^{n - 1}x)^{T} \end{array} \right| = \left[ \begin{array}{ccccc}x_{1} & x_{2} & x_{3} & \dots & x_{n}\\ x_{n} & x_{1} & x_{2} & \dots & x_{n - 1}\\ x_{n - 1} & x_{n} & x_{1} & \dots & x_{n - 2}\\ \vdots & \vdots & \vdots & \ddots & \vdots \\ x_{2} & x_{3} & x_{4} & \dots & x_{1} \end{array} \right] \tag{4.19}
$$

对一个二维图像的循环变换可参考一维图像的方法进行处理，如4.1图所示为对一个二维图像进行移位的操作，其中绿色框中的图片是包含目标的基础样本，其它图片是对其进行循环移位操作产生的训练样本。通过不同的位移量，确定这些样本的回归值，从而可以利用这些样本来训练跟踪器。

![](images/e25ec92dd03e80e11ebb955fc2fab4dbeaea1a7af47c7f4e40f583ad2e2f421b.jpg)  
图4.1通过循环变化产生的样本

根据循环变换的特性，在傅立叶域中计算时，所有的循环矩阵都能使用离散傅立叶（Discrete Fourier Transform, DFT）矩阵进行对角化[2011]，从而能够极大地加

快模型训练和预测速度, 其表达式如下

$$
X = F d i a g(\hat{x})F^{H} \tag{4.20}
$$

其中,  $F$  是离散傅立叶常值矩阵,  $\hat{x}$  为  $x$  的离散傅立叶变换。

根据岭回归, 训练的目标是找到一个函数  $f(z)$ , 使得训练样本  $x_{i}$  与回归目标  $y_{i}$  的误差平方和最小, 其中线性回归函数  $f(z)$  的表达式

$$
f(z) = w^{T}z \tag{4.21}
$$

其中,  $w$  可以通过如下表达式求解

$$
\min_{w} \sum_{i} (f(x_{i}) - y_{i})^{2} + \lambda \left\| w \right\|^{2} \tag{4.22}
$$

其中,  $\lambda$  为回归参数, 则该最小化函数的闭式解为

$$
w = (X^{T}X + \lambda I)^{-1}X^{T}y \tag{4.23}
$$

式中,  $X$  的每一行为  $x_{i}$ , 而  $y$  由回归目标  $y_{i}$  组成。

由于后续的计算都在傅立叶域内进行, 因此把该式统一写为在傅立叶域中的表达式, 如下所示

$$
w = (X^{H}X + \lambda I)^{-1}X^{H}y \tag{4.24}
$$

通过对循环矩阵进行对角化并代入岭回归公式, 可以得到式 (4.24) 在傅立叶域中的表达式, 得到

$$
\hat{w} = \frac{\hat{x} \odot \hat{y}}{\hat{x}^{*} \odot \hat{x} + \lambda} \tag{4.25}
$$

式中,  $\hat{w}$  、  $\hat{x}$  和  $\hat{y}$  分别为  $w$  、  $x$  和  $y$  的离散傅立叶变换,  $\hat{x}^{*}$  是  $\hat{x}$  的复共轭, 通过离散傅立叶逆变换 (Inverse Discrete Fourier Transform, IDFT) 可以得到  $w$  。

当回归函数为非线性时, 通过映射函数  $\psi (z)$  把输入数据映射到高维特征空间中, 使得映射后的数据在新空间中线性可分, 其表达式如下

$$
f(z) = w^{T}\psi (z) \tag{4.26}
$$

式中,  $w$  为训练样本  $x_{i}$  的非线性变换的线性组合构成, 如下所示

$$
w = \sum_{i} \alpha_{i} \psi (x_{i}) \tag{4.27}
$$

则  $\alpha$  代替  $w$  成为待优化的变量, 将式 (4.27) 代入式 (4.26) 可得

$$
f(z) = \sum_{i} [\alpha_{i} \psi^{T}(x_{i}) \psi (z)] \tag{4.28}
$$

记  $\kappa (x_{i}, x_{j}) = \psi^{T}(x_{i}) \psi (x_{j})$  为核函数, 则

$$
f(z) = \sum_{i} \alpha_{i} \kappa (z, x_{i}) = \alpha^{T} \kappa (z) \tag{4.29}
$$

式中,  $\alpha$  和  $\kappa (z)$  为  $n \times 1$  向量,  $\alpha$  的第  $i$  个元素为  $\alpha_{i}$ ,  $\kappa (z)$  为测试样本与所有训练

样本的核函数,  $\kappa (z)$  的第  $i$  个元素为训练样本  $x_{i}$  与待测样本  $z$  的核函数值。

$f(z)$  是关于  $z$  的非线性函数, 但却是关于  $\kappa (z)$  的线性函数, 因此, 基于岭回归用线性函数的优化方法可以求解  $\alpha$ , 其表达式如下[222]

$$
\alpha = (K + \lambda I)^{-1}y \tag{4.30}
$$

其中,  $K$  为所有训练样本的核相关矩阵, 其元素  $K_{ij}$  如下所示

$$
K_{ij} = \kappa (x_{i},x_{j}) \tag{4.31}
$$

通过选择合适的核函数, 可以使得  $K$  也是一个可以对角化的循环矩阵, 根据线性回归的推导思路, 可解出

$$
\hat{\alpha} = \frac{\hat{y}}{\hat{k}^{xx} + \lambda} \tag{4.32}
$$

式中,  $k^{xx}$  为核相关矩阵  $K$  的第一行, 也就是  $x$  与  $x$  自身的核函数[223]。

设  $K^{z}$  为所有待测样本和所有训练样本的核相关矩阵, 每一列对应一个待测样本, 则可以计算所有待测样本的回归函数为

$$
\mathbf{f}(\mathbf{z}) = (K^{z})^{T}\alpha \tag{4.33}
$$

考虑到  $\mathbf{f}(\mathbf{z})$  是一个向量, 其包含了所有待测样本的检测响应, 为了提高计算效率, 利用循环矩阵的特性进行对角化, 推导过程如下所示

$$
\mathbf{f}(\mathbf{z}) = (C(k^{xz}))^{T}\alpha \tag{4.34}
$$

$$
\mathbf{f}(\mathbf{z}) = (Fdiag(\hat{k}^{xz})F^{H})^{T}\alpha \tag{4.35}
$$

$$
\mathbf{f}(\mathbf{z}) = F^{H}diag(\hat{k}^{xz})F\alpha \tag{4.36}
$$

最终得到

$$
\hat{\mathbf{f}} (\mathbf{z}) = \hat{k}^{xz}\odot \hat{\alpha} \tag{4.37}
$$

其中,  $k^{xz}$  为  $x$  与  $z$  的核函数。对于高斯核  $\kappa (x,x^{\prime}) = \exp (- \frac{1}{\sigma^{2}}\left\| x - x^{\prime}\right\|^{2})$ , 则有

$$
k^{xx} = \exp \left(-\frac{1}{\sigma^{2}}\left(\left\| x\right\|^{2} + \left\| x^{\prime}\right\|^{2} - 2F^{-1}\left(\hat{x}^{*}\odot \hat{x}^{\prime}\right)\right)\right) \tag{4.38}
$$

为了对比分析基于 KCF 的跟踪器与其它主流跟踪方法的性能, 本文选取了  $\mathbf{Struck}^{[116]}$ ,  $\mathbf{TLD}^{[118]}$ ,  $\mathbf{CSK}^{[120]}$ ,  $\mathbf{OAB}^{[115]}$ ,  $\mathbf{L1APG}^{[224]}$ ,  $\mathbf{MII}^{[117]}$ ,  $\mathbf{ORIA}^{[225]}$ ,  $\mathbf{CT}^{[226]}$  跟踪器与 KCF 跟踪器通过 OTB- 2013 数据集进行测试, 其中 50 个视频序列包含了光照变化 (Illumination Variation, IV)、尺度变化 (Scale Variation, SV)、低分辨率 (Low Resolution, LR)、遮挡 (Occlusion, OCC)、背景杂乱 (Background Clutters, BC)、变形 (Deformation, DEF)、快速运动 (Fast Motion, FM) 等各种情况。测试数据集已由人工标记得到了每一帧中的目标真实位置, 将跟踪算法的测试结果与已经标记好的目标真实数据进行对比, 可以画出距离精度 (Distance Precision,

DP）曲线图和成功率（Success Rate, SR）曲线图。

精度指标是根据跟踪器估计的目标位置与目标真实位置的距离并计算该距离小于给定阈值的百分比得到的，不同的给定阈值得到的结果也不同。而成功率是根据重合率得分（Overlap Score, OS）得到的，取跟踪器估计的目标框为  $r_t$ ，真实的目标框为  $r_a$ ，则两者的重合率如下所示

$$
OS = \frac{\left|r_t \cap r_a\right|}{\left|r_t \cup r_a\right|} \tag{4.39}
$$

成功率的定义为 OS 得分大于给定值的帧在所有帧中的占比，可以充分反映目标预测框和真实目标的吻合度。为了对比不同方法的跟踪鲁棒性，采取单次评估（One- Pass Evaluation, OPE）、空间鲁棒性评估（Spatial Robustness Evaluation, SRE）和时间鲁棒性评估（Temporal Robustness Evaluation, TRE）方法来评价各跟踪器的鲁棒性。

图 4.2 所示为包括 KCF 在内的上述 9 种算法采用 OTB- 2013 数据集进行测试得到的各指标计算结果，为了便于区分，不同的算法采用不同颜色的曲线进行表示。

![](images/f1670b6e6f0b620db73e8e4e8349bc399b358cf85fea4fbac2d896d9d79b5a30.jpg)

![](images/365ebbe48c3c619feae6785958c0faf2a9050abc5efe9f9990d731976368acdc.jpg)  
图4.2 KCF跟踪器与其它主流跟踪器的性能对比（OTB-2013）

对这9种跟踪器的跟踪速度和其分别在OPE、SRE、TRE下的距离精度及成功率进行统计，如表4.1所示。

表4.1各目标跟踪方法性能的统计数据  

<table><tr><td>跟踪器</td><td>FPS</td><td>OPE(DP)</td><td>SRE(DP)</td><td>TRE(DP)</td><td>OPE(SR)</td><td>SRE(SR)</td><td>TRE(SR)</td></tr><tr><td>KCF</td><td>138</td><td>0.740</td><td>0.683</td><td>0.774</td><td>0.623</td><td>0.571</td><td>0.676</td></tr><tr><td>Struck</td><td>10.0</td><td>0.656</td><td>0.635</td><td>0.707</td><td>0.559</td><td>0.536</td><td>0.615</td></tr><tr><td>TLD</td><td>21.7</td><td>0.608</td><td>0.573</td><td>0.624</td><td>0.521</td><td>0.471</td><td>0.531</td></tr><tr><td>CSK</td><td>269</td><td>0.545</td><td>0.525</td><td>0.618</td><td>0.443</td><td>0.421</td><td>0.521</td></tr><tr><td>OAB</td><td>5.13</td><td>0.504</td><td>0.506</td><td>0.563</td><td>0.427</td><td>0.415</td><td>0.483</td></tr><tr><td>L1APG</td><td>2.47</td><td>0.485</td><td>0.481</td><td>0.569</td><td>0.440</td><td>0.423</td><td>0.511</td></tr><tr><td>MIL</td><td>28.1</td><td>0.475</td><td>0.454</td><td>0.545</td><td>0.373</td><td>0.354</td><td>0.453</td></tr><tr><td>ORIA</td><td>9.0</td><td>0.457</td><td>0.407</td><td>0.479</td><td>0.381</td><td>0.316</td><td>0.414</td></tr><tr><td>CT</td><td>38.8</td><td>0.406</td><td>0.340</td><td>0.477</td><td>0.341</td><td>0.231</td><td>0.406</td></tr></table>

结合图4.2中的曲线和表4.1的数据分析可以看出，KCF跟踪器在速度和跟踪准确性上相比其它主流的跟踪方法都有一定的优势，但KCF跟踪方法没有考虑目标在画面中的尺度变化，所有的预测结果中目标框的大小都与最初保持一致。因此，为了更好地对运动中的目标进行跟踪，应该充分考虑其尺度变化，避免在长时间跟踪过程中丢失目标。

# 4.2.3 尺度自适应的核相关滤波器

针对目标尺度发生变化的情况，可以通过定义一个尺度池，在每一帧的预测时获取多个尺度的待测样本，利用双线性插值将其尺寸重新调整为与模板一致，再计算其最大的响应值，从而确定最合适的目标尺寸[123]。设模板尺寸固定为

$$
s_{T} = (s_{x},s_{y}) \tag{4.40}
$$

假设目标框在原图像中的尺寸为  $s_{i}$ ，取尺度池为

$$
S = \{t_{1},t_{2},\dots,t_{k}\} \tag{4.41}
$$

则在当前帧可以通过计算  $k$  个尺度的样本来寻找最合适的目标。可以按照如下所示的表达式进行计算

$$
\arg \max F^{-1}\hat{\mathbf{f}} (z^t) \tag{4.42}
$$

式中，  $z^t$  是大小为  $t_i s_t$  的样本，而被重新调整为与模板尺寸  $s_T$  一致。不同尺度的待测样本中，响应值最大的那一个即为最佳的目标尺度，如图4.3所示。

![](images/45dc162f780bb6bbc0a4f4a737501c9616b9fe222305af33049b779185009b2c.jpg)  
图4.3目标尺度预测

为了与其它基于相关滤波的跟踪算法进行对比分析，本节分别在Matlab环境建立了基于多尺度的核相关滤波跟踪算法（Multi- Scale KCF, MSKCF），定义尺度池为  $S = \{(1 + 0.04n)\}$ ，其中  $n$  为满足  $- 7 \leq n \leq 7$  的整数，因此  $S$  一共包含15个不同的尺度因子。算法在输出目标位置的同时，输出目标尺度估计值，该估计值是根据尺度池中的尺度因子和初始样本尺寸来决定的。利用OTB- 2013数据集中的CarScale视频序列，分别对带尺度更新的MSKCF跟踪器、不带尺度预测的KCF跟踪器和同样基于相关滤波的CSK跟踪器进行测试，运行环境如表4.2所示。

表4.2算法运行环境  

<table><tr><td>项目</td><td>配置</td></tr><tr><td>CPU</td><td>Intel Core i5-5400 2.70GHz</td></tr><tr><td>内存</td><td>8GB DDR4</td></tr><tr><td>显卡</td><td>NVIDIA GeForce GTX1050Ti 1GB</td></tr><tr><td>操作系统</td><td>Windows 7 旗舰版</td></tr></table>

对高速行驶车辆的跟踪过程如图4.4所示。

![](images/0417534e9da0177c545d3a6dc93f37f255ab05d2e9dfd5956496be12a8905bf9.jpg)

![](images/2b781aeeeebbf38d3e5b2d56c19b2197c02de37753ff81e26308401f7e7457ac.jpg)  
图4.4 CSK（蓝色框）、KCF（绿色框）与MSKCF（红色框）在CarScale视频上的测试结果

由图4.4可知，跟踪开始阶段，由于初始条件一致并且汽车在画面中较小，三种算法预测的目标框基本上重叠在一起。第136帧后，随着汽车在画面中逐渐变大，KCF跟踪器和CSK跟踪器所预测的目标框尺寸始终保持与初始框一致，只能输出固定尺寸的目标框（ $42 \times 26$ 像素），而MSKCF跟踪器随着目标大小的变化自适应地调整目标框的尺寸，车辆大部分都被包含在MSKCF跟踪器预测的目标框中。第176帧左右，由于受到树木的遮挡，CSK跟踪器已经丢失目标了，KCF跟踪器和MSKCF跟踪器能够持续稳定地输出目标框，但由于算法的局限性，KCF跟踪器只能对与目标局部进行跟踪，而MSKCF跟踪器输出的目标框与真实的目标轮廓更接近。从以上视频测试结果可以看出，在目标大小存在持续变化的情况下，MSKCF跟踪器的表现明显更优。

图4.5所示为CSK、KCF和MSKCF三种算法在OTB- 2013数据集上的各指标计算结果。

![](images/007a9ec173ade54560886c32b0e64d2729c49f444565de1d1fadb9da4fd1e1d5.jpg)

![](images/71856b4ecebe7e1929c3bf43df7f18a2a08527b33dadba6a4e966a72898198a2.jpg)  
图4.5 MSKCF、CSK、KCF跟踪器性能对比（OTB-2013）

由图4.5可以看出，CSK跟踪器各方面指标明显不如其它两个算法，而在距离精度方面KCF与MSKCF跟踪器比较接近，说明目标尺度变化对预测目标位置的影响不大。但从反映与真实目标重合度的成功率指标来看，MSKCF明显优于KCF，这是由于KCF采用与初始框一致的固定尺度预测，而MSKCF能够随目标变化自适应地更新目标尺度，因此与真实目标的重合度更高。选择OTB- 2013数据集中28个存在目标尺度变化的视频序列计算成功率指标，如图4.6所示。

![](images/78695c1d9f78af17fa032b3fee9a8ed44068cbaac3e395bd8d4cc352a4e69393.jpg)

![](images/76b9debed007923663088b7d68c1738701fb0d7df8ccb86f2166e6245cf688fc.jpg)  
图4.628个尺度变化视频序列的MSKCF、CSK、KCF跟踪器成功率指标对比

由图4.6可以看出，如果单独对包含尺度变化的视频序列进行测试，MSKCF跟踪器的成功率指标为0.613（OPE）、0.581（SRE）和0.652（TRE），相比KCF的成功率0.479（OPE）、0.447（SRE）和0.571（TRE），分别提高了  $27.97\%$  （OPE）、 $29.98\%$  （SRE）和  $14.19\%$  （TRE），改进效果十分明显。而在全部50个视频序列的测试中，MSKCF在成功率指标方面相比KCF也提高了  $9.95\%$  （OPE）。以上测试结果和数据说明，在加入对目标的尺度估计后，在各种测试条件下，MSKCF跟踪器预测的目标框总体上与真实目标更为接近，成功率指标明显优于KCF跟踪器，也证明了改进后的方法在实际应用中是有效的。

# 4.3 基于深度学习的非合作目标检测算法

# 4.3.1 基于改进YOLO的目标检测算法

传统的目标检测方法是建立在人工提取特征的基础之上的[227]，但特征提取流程比较复杂，且检测效果容易受到特征设计质量高低的影响。而近年来，基于卷积神经网络的深度学习方法快速发展，  $\mathbf{R} - \mathbf{CNN}^{[128]}$  是首个成功将基于CNN的深度学习应用到目标检测上的算法。从Fast  $\mathbf{R} - \mathbf{CNN}^{[129]}$  到Faster  $\mathbf{R} - \mathbf{CNN}^{[130]}$  在速度和精度方面有了明显提高，但仍然达不到实时目标检测的要求。

而YOLOv1方法被提出后，充分展示了其在计算速度上的优势，并且与R- CNN系列算法精度相当[228]。经过多次改进，YOLOv2、YOLOv3、YOLOv4等算法相继提出，通过MS- COCO等数据集测试，其在与ResNet- 101和ResNet- 152等主流网络的对比中展现了优异的性能[229- 232]。

为进一步提高对特定目标的检测精度，本节在YOLOv4[233]网络的基础上引入卷积注意力机制（ConvolutionalBlockAttentionModule，CBAM），从而使深度网络在提取特征时更加关注重要的目标信息，减少无用特征以及复杂背景环境的干

扰。YOLOv4- CBAM算法的网络框架如图4.7所示。

![](images/a113d979e5648462c13a71b88a241886a586aa372553021b8d3bc473244726fd.jpg)  
图4.7YOLOv4-CBAM算法网络框架

其中，CBAM模块在通道和空间两个维度依次计算，最后将注意力权重与原特征图相乘，如图4.8所示。

![](images/87b897d671df38595967f47adcf597e9723aa052503cf73e9ab3517d213159f6.jpg)  
图4.8CBAM模块

上图中，输入特征图  $F\in \mathbb{R}^{C\times H\times W}$  、通道注意力权值  $M_{c}\in \mathbb{R}^{C\times 1\times 1}$  和空间注意力权值  $M_{s}\in \mathbb{R}^{1\times H\times W}$  经过计算后得到输出特征图  $F^{\prime \prime}$  ，表示如下

$$
F^{\prime} = M_{c}(F)\otimes F \tag{4.43}
$$

$$
F^{\prime \prime} = M_{s}(F^{\prime})\otimes F^{\prime} \tag{4.44}
$$

$M_{c}(F)$  和  $M_{s}(F^{\prime})$  的表达式如下所示

$$
M_{c}(F) = \sigma \{MLP[AvgPool(F)] + MLP[MaxPool(F)]\} \tag{4.45}
$$

$$
M_{s}(F^{\prime}) = \sigma \left\{f^{7\times 7}[AvgPool(F^{\prime});MaxPool(F^{\prime})]\right\} \tag{4.46}
$$

式中，  $\sigma$  为Sigmoid激活函数，MaxPool和AvgPool分别为最大池化和平均池化，

MLP是主要由全连接层构成的共享感知器，  $f^{7\times 7}$  为  $7\times 7$  的卷积层。

YOLOv4- CBAM算法首先将CBAM卷积注意力模块加入到其主干网络CSPDarknet53[234]的残差结构中，将其改为CSPn- CBAM网络，由1个  $\mathrm{CSP1 + CBAM}$  层、1个  $\mathrm{CSP2 + CBAM}$  层、2个  $\mathrm{CSP8 + CBAM}$  层和1个  $\mathrm{CSP4 + CBAM}$  层构成。随后采用空间金字塔池化（Spatial Pyramid Pooling，SPP）和路径聚合网络（PathAggregationNetwork，Panet）组成特征融合网络对不同尺度特征信息进行融合，并在特征融合网络拼接的第5次卷积中加入CBAM模块，使其增强有效特征提取。最后利用检测网络对得到的  $13\times 13$  、  $26\times 26$  、  $52\times 52$  三种尺寸大小目标特征图进行分类和回归预测，输出目标框以及目标类别的预测结果。

通过引入CBAM模块，加强了YOLOv4网络对目标区域的特征表达以及目标的位置特性，在网络训练过程中对重要的特征加以更高的权重进行学习，更加关注需要识别的目标，忽略不重要的特征，从而提升算法的检测精度。下面将介绍部署YOLOv4- CBAM算法以及针对特定目标进行训练的相关内容。

# 4.3.2 针对特定目标的训练和检测结果

根据微小型无人机的任务场景，本文以船只、多旋翼无人机、固定翼飞机三种特定目标为训练对象，通过网络下载目标的样本图片。由于深度学习需要大量数据来支撑，在找不到大量数据的情况下，采用数据增强方法，对有限的训练数据进行变换而得到新的数据，从而有效地扩充样本数据集。本文主要采用水平翻转、垂直翻转、随机饱和度抖动、模糊图像、仿射变换、旋转等数据增强方式[235]，将样本集扩充到1000张图片，如图4.9所示。

![](images/efe066ea331fdd00c1705c8303d51b72292436612a6757daf3ea599ef8d3852c.jpg)  
图4.9通过网络下载和数据增强得到的训练样本

在网络训练时，需要已知样本图片中的目标位置与类别，因此需对每一张训练样本图片进行标注。本文采用Labelimg软件[236]对图片进行标注，采用可扩展标记语言（Extensible Markup Language,XML）来编写标记文件。在标记的过程中，针对每一张样本图片都需要框出图中的目标，并对该目标的位置、类别等进行描述。例如图4.10中一共包含1个多旋翼无人机和2个固定翼飞机，这里将其框选

出来，并分别标注其类别为“multirotor”、“fixedwing”和“fixedwing”。

![](images/8b8b86ea0d84bb6e2b128a22433d6161b622cb644c8ad5fafc3403f03bf74149.jpg)  
图4.10 采用Labelimg软件标注样本图片

标注图片完成后自动生成相应的配置文件，如图4.11所示。

![](images/8605f87145a7bd6a724e85a71e2b76e93a26458d336e3ed217dc78e3b4b2ec08.jpg)  
图4.11 标注生成的配置文件

对于缺少大数据和强大资源的应用，可采用迁移学习，通过已训练好的低层网络提取底层特征，设计并训练网络的高层结构，通过将已经学习到的模型参数分享给新模型从而加快模型的学习效率[237]。本节将YOLOv4和YOLOv4- CBAM

算法均部署于相同计算环境中，利用数据增强后的特定目标数据集对网络的高层结构进行训练。算法运行的软硬件环境如表4.3所示。

表4.3目标检测算法运行环境  

<table><tr><td>项目</td><td>配置</td></tr><tr><td>处理器</td><td>Intel Xeon E5-1620</td></tr><tr><td>主频</td><td>3.5GHz</td></tr><tr><td>显卡</td><td>NVIDIA Quadro K2200</td></tr><tr><td>内存</td><td>16GB</td></tr><tr><td>操作系统</td><td>Linux Ubuntu 18.04</td></tr><tr><td>CUDA</td><td>10.1</td></tr><tr><td>CUDNN</td><td>7.6.5</td></tr><tr><td>OpenCV</td><td>3.3.1</td></tr></table>

算法训练的参数设置如表4.4所示

表4.4网络训练参数设置  

<table><tr><td>训练参数</td><td>设置值</td><td>备注</td></tr><tr><td>image size</td><td>416×416×3</td><td>图像尺寸</td></tr><tr><td>learning rate1</td><td>0.001</td><td>初始学习率</td></tr><tr><td>learning rate2</td><td>0.00001</td><td>结束学习率</td></tr><tr><td>momentum</td><td>0.9</td><td>动量参数</td></tr><tr><td>weight decay</td><td>0.0005</td><td>权重衰减</td></tr><tr><td>epoch</td><td>300</td><td>训练轮数</td></tr><tr><td>batch size</td><td>16</td><td>单次输入数据数量</td></tr></table>

YOLOv4和YOLOv4- CBAM算法均完成训练后，通过制作的测试图片集，可采用精确率（Precision）与召回率（Recall）指标来评价YOLOv4和YOLOv4- CBAM算法的检测效果，如图4.12所示。

![](images/1b56b2e53748d1fb76153414264f46b3c5f7d29208c698f9db403d0ce28676e7.jpg)  
第110页

![](images/7e2655aedfa29e83c27ab0e72482b21c3f7f9512c614adabccce5dd287083e3d.jpg)  
图4.12对特定目标检测的P-R曲线

从计算得到的P- R曲线来看，YOLOv4- CBAM在各目标以及整体的检测效果上均好于YOLOv4。根据测试得到的船只、固定翼飞机和多旋翼无人机P- R曲线计算其平均精度（Average Precision, AP）和平均精度均值（Mean Average Precision, mAP）指标，如表4.5所示。

表4.5各目标检测精度指标统计  

<table><tr><td rowspan="2">算法</td><td colspan="3">AP/%</td><td rowspan="2">mAP/%</td></tr><tr><td>船只</td><td>固定翼飞机</td><td>多旋翼无人机</td></tr><tr><td>YOLOv4</td><td>36.17</td><td>77.92</td><td>92.14</td><td>68.74</td></tr><tr><td>YOLOv4-CBAM</td><td>52.87</td><td>84.96</td><td>94.22</td><td>77.35</td></tr></table>

通过上表统计的精度指标可以看出，在经过相同数量样本数据集训练的条件下，YOLOv4- CBAM算法在YOLOv4原算法的基础上将mAP提高了  $8.61\%$  ，并且对船只、固定翼飞机、多旋翼无人机目标的检测精度AP值方面，YOLOv4- CBAM均高于YOLOv4。特别是在船只目标检测方面，YOLOv4的AP值为  $36.17\%$  ，而YOLOv4- CBAM的AP值为  $52.87\%$  ，YOLOv4- CBAM相比YOLOv4提高了  $16.7\%$  。在固定翼飞机和多旋翼无人机目标检测方面，YOLOv4- CBAM相比YOLOv4分别提高了  $7.04\%$  和  $2.08\%$  ，也展现出了明显的提升。

将两种算法在一些背景环境复杂或者存在多个目标的测试图片上进行检测，给出五张代表性的测试图来对比其检测效果，如图4.13所示。

![](images/2b0565d213336344a510abbd9c03f728f05344bd4eeeec80b39d8d7a923415a6.jpg)

![](images/f424d82e456efdcedca477153dd70daf784094a11812040a561544e7224e9a0c.jpg)  
图4.13针对特定目标的YOLOv4与YOLOv4-CBAM检测效果对比由上图可见，YOLOv4-CBAM识别目标的置信度高于YOLOv4，预测准确度更高，例如测试图B和测试图D。而YOLOv4-CBAM在测试图A、C、D和E上

都成功预测了YOLOv4没有识别的船只和固定翼飞机，说明YOLOv4- CBAM对于复杂背景环境下的目标以及多尺度目标的检测效果有一定提升，改善了YOLOv4存在的漏检误检情况。试验结果表明，YOLOv4- CBAM检测精度相比YOLOv4有明显提升，提高了目标预测的有效性，降低了目标漏检率，证明了改进后的算法性能更优。

# 4.4 基于深度学习的并行检测与跟踪框架

# 4.4.1 应用场景分析

传统目标跟踪算法由于缺少目标的先验信息，其跟踪的目标初始框一般需要系统直接给出，再利用跟踪器连续预测目标的位置，如图4.14所示。传统目标跟踪方法运算速度非常快，满足实时跟踪的要求，但比较依赖初始框的选取，在长时间目标跟踪任务中存在漂移大、容易发散和跟踪失败等缺点。传统跟踪算法一旦出现跟踪异常后，也无法重新搜索目标，因此其在实际的应用中面临着较大问题。

![](images/8ad73a20af3dfd36514af50d73aac8ef0a503a700ffa9c1378cc85b06e111a3b.jpg)  
图4.14传统目标跟踪算法流程

与传统目标跟踪算法不同，基于深度学习的目标检测方法由于预先经过大量样本数据的训练，能够直接检测出每一帧图像中存在的所有目标的类型并输出其位置，准确性和鲁棒性都非常好，但计算耗时比较长，帧率较低，对硬件配置的要求比较高。比较简单的做法就是通过选择更高性能的硬件来运行深度学习算法，如果使目标检测的帧率能够达到30fps以上，就能基本到达实时跟踪的要求，但同时硬件的成本、重量、功耗也会大幅提升，而微小型无人机平台无法提供这样的硬件条件。

针对本文研究的微小型无人机，在有效载荷和机载端计算能力受到严格约束的条件下，为了克服传统跟踪方法的不足并提升目标跟踪效果，只能考虑从算法层面进行改进，充分运用基于深度学习的目标检测算法优点，弥补跟踪算法的不

足之处，并充分结合其各自特点。

足之处，并充分结合其各自特点。本文通过建立并行检测与跟踪算法框架，将目标跟踪任务分为检测和跟踪两个独立运行的部分。把深度学习算法的检测结果用于对跟踪器的初始化，跟踪器以较高的帧率运行，而同时检测器以低帧率运行，并对跟踪结果进行监督和校正，在目标变化较大时，利用检测算法的目标检测结果来更新跟踪器，从而在长时间跟踪过程中持续而准确地估计目标位置。

# 4.4.2 YOLO-MSKCF并行检测与跟踪算法

YOLO系列算法的特点是在保持较高精度的条件下运行速度非常快，但在计算能力相对有限的平台上仍存在不满足实时性要求的情况。例如，在NVIDIA Jetson TX2平台上运行YOLOv4算法，运行速度约为  $2\sim 5\mathrm{fps}$ ，难以到达实时跟踪的要求。但同样的平台，MSKCF算法的运行帧率可以很轻松超过30fps。因此，可以运用并行计算的思想将YOLOv4算法和MSKCF算法同时调动起来，建立图4.15所示的YOLO- MSKCF并行检测与跟踪框架，既能够保证检测结果是比较精准的，又能保持跟踪的连续性。

![](images/fc4dc1a196dbc2880101d592ef87fe04f85ce166ead225708e20697f45014a82.jpg)  
图4.15YOLO-MSKCF并行检测与跟踪框架

考虑到在无人机跟踪目标的过程中，画面中可能包含多个目标且目标处于快速移动的状态，本文采用YOLOv4进行检测并输出所有目标的类别、目标框及置信度评分，将这些目标信息快速提供给操作人员进行甄别。操作人员只需在指定目标框内点击鼠标左键，通过鼠标事件给系统反馈，从而将该指定目标的目标框传递给MSKCF跟踪器，完成跟踪器的初始化，其执行过程如算法4.1所示。这种工作模式，有利于操作人员在实际应用中快速处理多目标的信息，并且不需要手动框选运动中的目标，避免了由于目标初始框选取不准确而带来的跟踪漂移、目标丢失等问题。

算法4.1：利用YOLOv4目标检测结果初始化MSKCF跟踪器  

<table><tr><td>输入：实时采集图像
初始参数：object_existed=0;flag_track=0;m_object=0;mclicked=0;
要求：MSKCF跟踪器（tracker）完成初始化并且能持续预测目标位置和尺度
输出：跟踪器的初始目标框Tbox、初始目标位置Tpos和初始目标尺度Tssa，指定目标的类别Cpo、置信度SDo、位置Ppo、边框Bpo
算法流程：
1 使用预训练YOLOv4检测器（detector）对输入图像执行目标检测；
2 展示图像中满足置信度要求（SDO≥DT）的所有目标o(i=1,2...n）；
3 if n&amp;gt;0 then
4 object_existed=1;
5 else
6 object_existed=0;
7 end
8 if (object_existed == 0) then
9 继续执行目标检测，直到n&amp;gt;0；
10 else if (object_existed == 1) &amp;amp;&amp;amp; (flag_track == 0) then
11 检测鼠标事件，当在某目标o框内单击左键时，则m_object=1;mclicked=1;
12 if (m_object=1) &amp;amp;&amp;amp; (mClicked=1) then
13 输出此刻detector对该目标的检测结果Cio、SDo、SDi、DDi、BDi；
14 Tbox=BDi;Tpos=PD;TDsca=1;
15 初始化tracker;
16 持续预测目标框Tbox、目标位置Tpos和目标尺度Tssa；
17 else
18 继续执行detector，直到鼠标点击指定目标
19 end
20 end</td></tr></table>

由于YOLOv4检测器经过了大量目标样本的训练，因此与MSKCF跟踪器相比，YOLOv4对目标的预测结果更为可信。针对YOLOv4的运行帧率较低而无法进行实时检测的问题，本文采取隔帧检测[238]的方式让检测器与跟踪器协同工作，在有限计算资源的条件下，建立基于YOLO与MSKCF的目标并行检测与跟踪机制。

在一个跟踪时段内，先利用跟踪器对N个连续帧进行预测，随后利用检测器输出的目标框对跟踪器进行一次校正，并用新的目标样本来训练和更新跟踪器，从而完成下一个时段的跟踪任务。这种方法的优点在于，可以将检测器以较低帧率运行，减少检测器占用的计算资源，但同时又能够在间隔一段时间后将检测器

的输出结果用来校正跟踪器，从而保持跟踪器在长时间连续跟踪目标时的预测值与真实目标吻合。

本文在NVIDIA Jetson TX2平台上搭建YOLO- MSKCF检测与跟踪并行框架，在ROS环境下对该算法进行了实现，通过话题机制来完成检测与跟踪进程间的数据传递。

图4.16所示为本节YOLO- MSKCF算法的节点与话题关系，其中darknet_ros和tracker_ros节点为该算法的主节点，分别实现了检测和跟踪两个过程，其输出的/relative_position_flag与/position_diff话题包含了目标大小、位置、类别等预测信息。

![](images/7f0c480f81c60f7a9e46e19432d5f6fa637920376bb5b1674fe991db704bc7d6.jpg)  
图4.16在ROS中搭建的YOLO-MSKCF并行检测与跟踪算法框架

# 4.4.3 算法验证与结果分析

本节在ROS环境下，采用NVIDIA Jetson TX2机载电脑，对YOLO- MSKCF算法进行测试。首先，利用检测器对多个静态目标进行检测，然后由鼠标点击确定需跟踪的目标，完成跟踪器的初始化。在需要重新选择目标时，可以通过鼠标在画面中单击右键，退出对当前目标的跟踪状态，再次选择需跟踪的目标。其中YOLO检测器的检测频率为1Hz，MSKCF跟踪器的跟踪频率为  $30\mathrm{Hz}$ ，测试的具体过程如图4.17所示。

![](images/d4ab56f6e2db6e0b098a09e20a601a18e9aa469d1975ab6b32f4d79afc38f3fd.jpg)

![](images/8d90d4882c05ed55fb9923a9fe8b091f5eb46fda4120f61adeca8fe2fb189919.jpg)  
图4.17 静态多目标检测与跟踪测试

由上图所示的测试结果可见，该算法可以快速有效地提取多目标并且确定指定的目标进行跟踪，与手动框选目标相比，检测器所输出的目标框与真实目标更加吻合，避免了由于目标初始框不准确而带来的跟踪误差，同时也提高了工作效率。在需要切换目标时，只需简单操作就能够调用检测器对画面中的目标进行再次搜索，工作机制更加灵活。

为了验证该算法对运动目标的跟踪效果，将包含目标尺度变化、快速移动等情况的视频输入到YOLO- MSKCF并行检测与跟踪算法中进行测试，并其与KCF算法的测试结果进行对比。YOLO- MSKCF的测试结果如图4.18所示。

![](images/bf360b5bbc0784c64235ad6f3a3940de4a6df6253ada27cb45e43f61f3320f29.jpg)

![](images/7be2c4dd983e07c68ac4aa8158260bc001f77335ace9d09bd69e0c469f2c780a.jpg)  
图4.18 基于YOLO-MSKCF的运动目标跟踪结果

KCF跟踪器由于不能自动地提取目标，视频第一帧中的目标框是手动给出的，随后KCF跟踪器对该运动目标进行连续地预测，图4.19中的绿色框为其跟踪结果。在实际任务环境下，目标处于快速移动的状态，如果给出的目标初始框与实际目标存在一定偏差，不论是框选到目标的局部区域还是框选较多的背景区域都会导致跟踪效果变差。

![](images/04b69193e015a30e907a864279adb2701b3b018727314505f03d66282ebf7281.jpg)

![](images/4b044dc5594cf2c1f07cc1975bc6360cbd8c5c184b8f8fa46e23aa2bc0b26d66.jpg)  
图4.19 基于KCF算法的运动目标跟踪结果

由上图所示，视频初始阶段YOLO检测器对画面内所有目标进行了检测，包括待跟踪的四旋翼无人机目标。通过鼠标左键单击选中四旋翼目标，此时MSKCF跟踪器完成初始化并开始持续跟踪，而KCF跟踪器则只能通过手动框选目标来进行初始化。

跟踪初期目标在画面中较小，而随着目标运动，其在画面中的尺寸不断增大，MSKCF跟踪器所预测的目标框与实际目标的重合度降低。在机载电脑计算能力有限的条件下，YOLO检测器以较低频率运行，对MSKCF跟踪器的跟踪结果进行校正，当跟踪器输出的目标框与检测结果的偏差较大时，认为检测器的预测结果是准确的，并将MSKCF跟踪器重新初始化，从而实现对当前目标尺度的预测更准确。而无检测器校正的KCF跟踪器预测的目标框是固定尺寸的，在目标形变较大时，其输出的预测框与实际目标的偏差越来越大。

在视频末段目标已产生较大形变的情况下，YOLO- MSKCF并行检测与跟踪算法能够实现预测框与实际目标比较吻合，而KCF算法输出的目标预测框与实际目标差别十分明显。在两个算法跟踪频率均设置为  $30\mathrm{Hz}$  的条件下，运动目标跟踪测试结果证明了YOLO- MSKCF在目标尺度预测和重合度方面更好。

# 4.5 小结

本章针对长时间目标跟踪问题，对基于深度学习的非合作目标并行检测与跟踪框架进行了研究，主要研究工作及结论如下：

（1）对非合作目标的跟踪方法进行研究，深入分析了核相关滤波器（KCF）采用的岭回归、循环矩阵、核方法以及傅立叶变换等计算原理，采用公开数据集对不同跟踪算法的性能指标进行了对比分析。针对核相关滤波器不能对目标尺度进行预测的问题，提出了多尺度的核相关滤波器（MSKCF），建立了目标尺度池，通过求解最大响应值来确定最佳尺度，持续对目标位置和尺度进行预测。将KCF和MSKCF在公开数据集上进行测试，结果表明MSKCF在对运动目标跟踪时所输出的目标框随目标形状变化而相应地调整尺度，与真实目标轮廓更加吻合，成功

率指标也更好；

(2)研究了基于深度学习的目标检测算法，将卷积注意力机制引入到YOLOv4网络中进行改进，增强对重要特征的关注和学习，提出了YOLOv4-CBAM算法，利用特定目标的训练样本集对YOLOv4-CBAM和YOLOv4两种算法进行训练，并对船只等特定目标进行检测对比。试验数据证明了YOLOv4-CBAM在检测精度上相比原YOLOv4算法有明显提升，并且降低了原算法对于复杂环境中的目标以及多尺度目标的漏检率，具有更优的性能；

（3）分析了无人机目标跟踪的应用场景以及长时间目标跟踪任务待解决的问题，提出了一种基于YOLO-MSKCF的非合作目标并行检测与跟踪算法框架，利用YOLO的目标检测结果来实现跟踪器的快速初始化。在长时间目标跟踪时，将跟踪器运行速度快和检测器精度高的特点结合，采用隔帧检测方式将检测器的检测结果用于校正跟踪器，使得跟踪器输出的目标框始终接近于真实目标。在ROS环境下对该算法进行了实现，并通过对静态目标和运动目标的跟踪测试，验证了该算法的可行性和高效性。

# 第五章 基于机载视觉导引稳定平台的目标跟踪技术研究

# 5.1 引言

捷联式导引头具有质量轻、体积小、复杂度低、成本低等优点，但由于其直接固定安装于机体，在跟踪目标时，机体姿态受光学设备最大视角的限制，并且其直接测量到的仅是基于机体坐标系这一动系的目标视线角（称之为体视线角），因此需要设计复杂的解耦滤波算法根据体视线角和惯性导航系统数据来得到无人机的制导信息。相比之下，平台式导引头采用机械稳定平台对机体的姿态扰动进行隔离，从而构建了一个惯性基准，能够在惯性坐标系下直接测量目标的视线角和视线角速度。另一方面，平台式导引头还可采用将摄像机和激光测距仪均安装在平台内框上的方式，在锁定目标的同时，获得与目标的相对距离等信息，全方位地提供导引信息。基于传统动力陀螺或速率陀螺结构的稳定平台对空间尺寸要求高，且系统庞杂，而针对微小型无人机需采用结构更加轻巧的机载视觉导引稳定平台。

本章针对运动目标自主跟踪等问题，在上一章的基础上，深入研究了适用于微小型无人机的机载视觉导引稳定平台，对稳定平台的结构形式、运动学关系和动力学模型进行了分析，采用无刷电机、编码器和单片机搭建了微小型三轴稳定平台试验系统，研究了稳定平台的跟踪控制方法。基于本文搭建的四旋翼飞行试验平台，开展了空中运动目标自主跟踪飞行试验研究，提出了一种基于无人机空中多点测向测距的目标定位方法，并在户外环境下开展了实物演示试验对算法进行了验证。

# 5.2 机载视觉导引稳定平台的动力学模型与控制系统

# 5.2.1 稳定平台的结构形式

5.2.1 稳定平台的结构形式机载视觉导引稳定平台，又称为机载光电吊舱或云台，是无人机重要的侦察设备，能够在无人机飞行过程中对目标进行捕获和瞄准等。稳定平台的框架形式主要为两轴平台结构、三轴平台结构和四框架两轴稳定结构等。两轴平台与三轴平台相比，减少了一个转动机构，无法实现对载体运动的完全隔离，但同时也具有体积小、重量轻、结构精简等特点。三轴平台能够将载体运动完全隔离，补偿由于无人机姿态变化引起的瞄准线变化，可以有效消除失调角，更易于形成直接控制[239]。四框架两轴平台由内环架和外环架组成，外环主要隔离载体扰动，内环跟踪目标，内环的残余扰动和负载很小，具有更高的控制精度和瞄准线稳定裕度，

但结构较为复杂，重量较大，实际应用较少。

目前，机载视觉导引稳定平台已广泛应用于消费娱乐、影视拍摄、安防监控、空中侦察、目标指示等军民用领域，已经从早期的稳像功能逐步朝着目标跟踪、目标指示等更高层次的应用发展。图5.1所示为机载视觉导引稳定平台在各应用领域中的典型结构形式。

![](images/b3d340af4a23206639f194e269280c8f9b3d382674d655c45c634793302e869f.jpg)  
图5.1 典型的稳定平台结构形式

相比捷联式导引头，稳定平台能够完全隔离载体的扰动，使光轴相对于惯性空间保持恒定。机载视觉导引稳定平台在精确跟踪目标的同时，通过搭载红外成像仪、激光测距仪等多种设备，在利用光学设备锁定目标之外，还可以通过目标红外辐射特性来跟踪目标，也能够对目标进行测距。

# 5.2.2 坐标系定义与运动学关系

三轴稳定平台是本文主要的研究对象，由于其包括了空间三个转动自由度，可以通过转动有限视场的摄像机使其指向期望的方向，也是最典型的一种结构形式。当三轴稳定平台系统安装在无人机上时，在IMU和其他角度传感器的作用下，扭矩电机通过转动来补偿无人机飞行过程中的姿态变化，使摄像机在惯性空间中保持原有的指向。

图5.2所示为典型的三轴稳定平台系统，其中包括以下主要部分：

（1）视频流输出接口。可以提供网口、Micro USB 和 Micro HDMI 接口，其中采用 HDMI 接口输出的视频流需要经过采集才能被用于处理，这会带来一定的传输延迟，本文采用的是 Micro USB 接口。

（2）嵌入式控制器。利用传感器的测量数据，对电机进行驱动，以实现载荷的稳定。

（3）带编码器的无刷直流电机。无刷电机与编码器同轴安装，电机转动的同时反馈角度信息。

（4）减震球。进一步降低无人机抖动对稳定平台的影响。

（5）摄像机。该摄像机支持10倍光学变焦。

图5.3所示为控制器、无刷电机、编码器等部件内部结构图。

![](images/7cd267416472535fffcb53790644e82069ab4bd29f48e9aceddb9cd99fbd8bc0.jpg)  
图5.2 三轴稳定的机载视觉导引平台系统

![](images/48e45ebad23cb33a9ea39cbfffcdb5ffd80f0917c905b8c2fae9c0d07de273e5.jpg)  
图5.3 稳定平台相关部件内部结构

三轴稳定平台系统由三个无刷直流电机来带动框架旋转并实现平衡，三个旋转编码器对框架间的相对角度进行测量，同时IMU用来感知惯性空间中的光轴指

向，采用减震球对无人机飞行过程中的抖动进行隔离，最后采用一块嵌入式控制器将所有数据进行采集并输出控制信号给各个电机用来驱动整个系统。

三轴稳定平台包含外框、中框和内框三个框架，其中外框与无人机底部基座相连，摄像机与内框架固连，按照偏转、滚转和俯仰的顺序进行运动。为了描述其运动学关系，根据图5.4所示对相关坐标系进行定义，包括基座系  $F$  、外框系  $O$  、中框系  $M$  和内框系  $G$  ，框架角定义为  $\theta_{Y}$  （偏转）、  $\theta_{R}$  （滚转）和  $\theta_{P}$  （俯仰）。基座系  $F$  可通过绕轴  $z_{F}$  转动角度  $\theta_{Y}$  和平移变换得到外框系  $O$  ，外框系  $O$  可通过绕轴  $x_{O}$  转动角度  $\theta_{R}$  和平移变换得到中框系  $M$  ，中框系  $M$  可通过绕轴  $y_{M}$  转动角度  $\theta_{P}$  和平移变换得到内框系  $G$  。如图5.4所示，三轴稳定平台系统各框架平行放置的初始状态为

$$
(\theta_{Y},\theta_{R},\theta_{P}) = (0,0,0) \tag{5.1}
$$

![](images/969c6c12542397a1f151dec308b08ba3d6af326780fb72e5804366c6f18df9b1.jpg)  
图5.4三轴稳定平台相关坐标系定义

取内框系  $G$  中的任意一点  $P$  ，用向量  ${}^{G}r_{p}$  表示，则该向量可以通过旋转矩阵  ${}^{F}R_{G}$  和平移向量  ${}^{F}d_{G}$  在基座系  $F$  下表示为  ${}^{F}r_{p}$  ，其转换关系如下

$$
{}^{F}r_{p} = {}^{F}R_{G}{}^{G}r_{p} + {}^{F}d_{G} \tag{5.2}
$$

式中，  ${}^{G}r_{p}$  为

$$
{}^{G}r_{p} = \left[ \begin{array}{c}{}^{G}x_{p} \\ {}^{G}y_{p} \\ {}^{G}z_{p} \end{array} \right] \tag{5.3}
$$

处理这种变换的一种更简便的方式是采用齐次变换矩阵  $^F T_G$  ，其变换过程表示如下

$$
{}^F T_G = \left[ \begin{array}{cc}{}^F R_G & {}^F d_G\\ 0_{1\times 3} & 1 \end{array} \right] \tag{5.4}
$$

$$
\left[ \begin{array}{c}F_{r_p}\\ 1 \end{array} \right] = {}^F T_G\left[ \begin{array}{c}G_{r_p}\\ 1 \end{array} \right] \tag{5.5}
$$

其中，  $^F T_G$  可以进行如下拆分

$$
{}^F T_G = {}^F T_O{}^O T_M{}^M T_G \tag{5.6}
$$

根据图5.4中定义的参数，这些各框架间的变换矩阵可以表示如下。其中基座系  $F$  和外框系  $o$  之间的变换为

$$
{}^F R_O = \left[ \begin{array}{ccc}\cos \theta_Y & -\sin \theta_Y & 0\\ \sin \theta_Y & \cos \theta_Y & 0\\ 0 & 0 & 1 \end{array} \right] \tag{5.7}
$$

$$
{}^F d_O = \left[ \begin{array}{c} - l_1\cos \theta_Y\\ -l_1\sin \theta_Y\\ h_1 \end{array} \right] \tag{5.8}
$$

或者

$$
{}^F T_O = \left[ \begin{array}{cccc}\cos \theta_Y & -\sin \theta_Y & 0 & -l_1\cos \theta_Y\\ \sin \theta_Y & \cos \theta_Y & 0 & -l_1\sin \theta_Y\\ 0 & 0 & 1 & h_1\\ 0 & 0 & 0 & 1 \end{array} \right] \tag{5.9}
$$

外框系  $o$  和中框系  $M$  之间的变换为

$$
{}^O R_M = \left[ \begin{array}{ccc}1 & 0 & 0\\ 0 & \cos \theta_R & -\sin \theta_R\\ 0 & \sin \theta_R & \cos \theta_R \end{array} \right] \tag{5.10}
$$

$$
{}^O d_M = \left[ \begin{array}{c}l_2\\ -b_2\cos \theta_R\\ -b_2\sin \theta_R \end{array} \right] \tag{5.11}
$$

或者

$$
{}^O T_M = \left[ \begin{array}{cccc}1 & 0 & 0 & l_2\\ 0 & \cos \theta_R & -\sin \theta_R & -b_2\cos \theta_R\\ 0 & \sin \theta_R & \cos \theta_R & -b_2\sin \theta_R\\ 0 & 0 & 0 & 1 \end{array} \right] \tag{5.12}
$$

中框系  $M$  和内框系  $G$  之间的变换为

$$
^M R_G = \left[ \begin{array}{ccc}\cos \theta_P & 0 & \sin \theta_P\\ 0 & 1 & 0\\ -\sin \theta_P & 0 & \cos \theta_P \end{array} \right] \tag{5.13}
$$

$$
^M d_G = \left[ \begin{array}{c}h_3\sin \theta_P\\ b_2\\ h_3\cos \theta_P \end{array} \right] \tag{5.14}
$$

或者

$$
^M T_G = \left[ \begin{array}{cccc}\cos \theta_P & 0 & \sin \theta_P & h_3\sin \theta_P\\ 0 & 1 & 0 & b_2\\ -\sin \theta_P & 0 & \cos \theta_P & h_3\cos \theta_P\\ 0 & 0 & 0 & 1 \end{array} \right] \tag{5.15}
$$

由上面的公式可以推导基座系  $F$  和内框系  $G$  之间的旋转矩阵  $^F R_G$  和平移向量 ${}^F d_G$  为

$$
\begin{array}{r l} & {^F R_{G} = ^{F}R_{O}^{O}R_{M}^{M}R_{G} =}\\ & {\left[ \begin{array}{l l l l}{\cos \theta_{Y}\cos \theta_{P} - \sin \theta_{Y}\sin \theta_{R}\sin \theta_{P}} & {-\cos \theta_{R}\sin \theta_{Y}} & {\cos \theta_{Y}\sin \theta_{P} + \cos \theta_{P}\sin \theta_{Y}\sin \theta_{R}}\\ {\cos \theta_{Y}\sin \theta_{Y} + \cos \theta_{Y}\sin \theta_{R}\sin \theta_{P}} & {\cos \theta_{Y}\cos \theta_{R}} & {\sin \theta_{Y}\sin \theta_{P} - \cos \theta_{Y}\cos \theta_{P}\sin \theta_{R}}\\ {-\cos \theta_{R}\sin \theta_{P}} & {\sin \theta_{R}} & {\cos \theta_{R}\cos \theta_{P}} \end{array} \right]} \end{array}
$$

$$
\begin{array}{r l} & {^F d_{G} = ^{F}R_{O}^{O}R_{M}^{M}d_{G} + ^{F}R_{O}^{O}d_{M} + ^{F}d_{O} =}\\ & {\left[ \begin{array}{l}{l_{2}\cos \theta_{Y} - l_{1}\cos \theta_{Y} + h_{3}\cos \theta_{Y}\sin \theta_{P} + h_{3}\cos \theta_{P}\sin \theta_{Y}\sin \theta_{R}}\\ {l_{2}\sin \theta_{Y} - l_{1}\sin \theta_{Y} + h_{3}\sin \theta_{Y}\sin \theta_{P} - h_{3}\cos \theta_{Y}\cos \theta_{P}\sin \theta_{R}}\\ {h_{1} + h_{3}\cos \theta_{R}\cos \theta_{P}} \end{array} \right]} \end{array} \tag{5.17}
$$

设载体的角速度在基座系  $F$  下的描述如下

$$
{}^F\omega_b = \left[p\quad q\quad r\right]^T \tag{5.18}
$$

则根据平台框架的设置，在外框系  $O$  下描述的外框角速度  ${}^O \omega_O$  为

$$
{}^O\omega_O = (\mathit{\Sigma}^F R_O)^T\mathit{\Sigma}^F\omega_b + \left[ \begin{array}{c}0\\ 0\\ \dot{\theta}_Y \end{array} \right] = \left[ \begin{array}{c}p\cos \theta_Y + q\sin \theta_Y\\ -p\sin \theta_Y + q\cos \theta_Y\\ r + \dot{\theta}_Y \end{array} \right] \tag{5.19}
$$

中框系  $M$  下描述的中框角速度  ${}^M \omega_M$  为

$$
{}^M\omega_M = (\mathit{\Sigma}^O R_M)^T\mathit{\Sigma}^O\omega_O + \left[ \begin{array}{c}\dot{\theta}_R\\ 0\\ 0 \end{array} \right] = \left[ \begin{array}{c}p\cos \theta_Y + q\sin \theta_Y + \dot{\theta}_R\\ \cos \theta_R(-p\sin \theta_Y + q\cos \theta_Y) + \sin \theta_R(r + \dot{\theta}_Y)\\ \sin \theta_R(p\sin \theta_Y - q\cos \theta_Y) + \cos \theta_R(r + \dot{\theta}_Y) \end{array} \right] \tag{5.20}
$$

内框系  $G$  下描述的内框角速度  ${}^{G}\omega_{G}$  为

$$
\begin{array}{r l} & {^{\scriptscriptstyle G}\omega_{G} = (^{M}R_{G})^{T M}\omega_{M} + [ \begin{array}{l}{0}\\ {\dot{\theta}_{P}}\\ {0} \end{array} ] =}\\ & {[\cos \theta_{P}(p\cos \theta_{Y} + q\sin \theta_{Y} + \dot{\theta}_{R}) - \sin \theta_{P}(\sin \theta_{R}(p\sin \theta_{Y} - q\cos \theta_{Y}) + \cos \theta_{R}(r + \dot{\theta}_{Y}))]}\\ & {\qquad \cos \theta_{R}(-p\sin \theta_{Y} + q\cos \theta_{Y}) + \sin \theta_{R}(r + \dot{\theta}_{Y}) + \dot{\theta}_{P}}\\ & {\sin \theta_{P}(p\cos \theta_{Y} + q\sin \theta_{Y} + \dot{\theta}_{R}) + \cos \theta_{P}(\sin \theta_{R}(p\sin \theta_{Y} - q\cos \theta_{Y}) + \cos \theta_{R}(r + \dot{\theta}_{Y}))]} \end{array}
$$

# 5.2.3 稳定平台的跟踪控制

摄像机与IMU安装在稳定平台的内框上并随之转动，通过IMU敏感摄像机的姿态，从而保持摄像机在惯性空间中的稳定。为了瞄准目标，需进一步消除光轴与视线之间的偏差，使光轴与视线方向一致。而光轴与目标视线的偏差为失调角，其中包含高低和方位两个失调角。图5.5所示为基于惯性空间的失调角示意图，其中  $\theta$  是无人机俯仰角，  $\delta$  是摄像机光轴与惯性轴之间的夹角，  $\lambda$  是视线角，  $\beta$  是稳定平台的框架角，  $\epsilon$  为失调角。编码器测量得到的框架角精度较高，在摄像机IMU数据不可用的情况下，根据稳定平台的运动学模型，可以将无人机的姿态角和稳定平台的框架角用于解算摄像机光轴在惯性空间中的指向。无人机的姿态角是由组合导航系统给出的，其姿态解算精度相比摄像机IMU更高。

![](images/3d340bfb6e99e97638da716281e09295eaae7960f3c63828fb3d848ad917780c.jpg)  
图5.5惯性空间中的失调角示意图

在执行目标跟踪的实际应用中，机载视觉导引稳定平台共设置四种工作模式，即预设角度模式、搜索模式、稳定模式和跟踪模式。如图5.6所示，当稳定平台通电并初始化时，其在惯性空间中所指的方向保持在  $\lambda = 0$  。在预设角度模式下，摄像头的光轴将设置为给定角度，系统将在受到干扰的情况下保持光轴在惯性空间中的指向不变。随后，系统可切换到搜索模式，在此模式下，平台将在其最小和

最大角度之间循环旋转，以搜索更大范围的区域。当检测并选定目标后，系统将切换到跟踪模式，并将目标保持在摄像机画面的中心。图5.7主要描述了平台的控制系统以及工作状态切换，包含两个回路：跟踪回路和稳定回路。

![](images/834b24b87287f7679782a6d87027b0585b93706510240173a0d533dfb3f77a72.jpg)  
图5.6机载视觉导引稳定平台的工作流程

![](images/e9b9b6f9d3560b93a624a320dd0aba5bd607ac0244b3fa6ed96017d24c2ba124.jpg)  
图5.7机载视觉导引稳定平台的控制系统架构

图5.7所示的1、2、3、4状态分别表示预置角度模式、稳定模式、搜索模式和跟踪模式。三轴平台的稳定跟踪控制系统采用角位置和角速率双环控制结构，其中稳定回路实现的是各框架角速率稳定控制，是保证在摄像机光轴跟踪目标的前提下，由IMU敏感出光轴的扰动，采用控制律驱动电机偏转，从而隔离无人机的姿态扰动。而跟踪回路实现的是角位置跟踪及控制，根据角度传感器的测量数据和目标图像解算失调角，生成控制指令，将光轴指向目标视线方向，控制失调角趋近于零，从而实现对目标的闭环角位置跟踪。三轴平台在其三个转动自由度上，都采用这样的控制方法，利用三个电机的偏转来实现隔离扰动和跟踪目标的目的。

# 5.2.4 运动目标自主跟踪策略

空中追逃场景（Pursuit- Evasion Scenario）经常会出现在无人机执行的各种实际任务中，图5.8主要描述了无人机跟踪空中运动目标的任务场景以及无人机与目标的相对运动关系。

![](images/e8f1e31879784213dff0b4b98cfffd845a8756d2ec657a65829e735a1473588b.jpg)  
图5.8无人机与目标相对运动关系

其中，机载视觉导引稳定平台由于其将IMU和摄像机固连，从而构建了一个惯性基准，在无人机飞行过程中可以独立地控制摄像机在惯性空间中的指向。为了实现无人机对运动目标的自主跟踪，不仅仅要通过机载视觉导引稳定平台瞄准锁定目标，还需进一步解算目标视线等信息，并提供给无人机的制导控制系统，根据跟踪策略来控制无人机飞行，从而执行对目标的实时跟踪锁定。

考虑到本文所研究的四旋翼无人机能直接跟踪包括空间位置和偏航角的四个期望指令，可通过四个电机的转速差产生扭矩差来实现较为灵敏的航向控制，因此可以将四旋翼无人机对运动目标的跟踪分为纵向和横向运动两部分。无人机航向的跟踪采用纯追踪法，直接控制无人机的航向角与目标横向视线角一致，在水平面内无人机向前飞行的速度与机体轴保持一致，并且始终指向目标，其表达式如下。

$$
\psi_{des} = \lambda_{\chi} \tag{5.22}
$$

式中，  $\psi_{des}$  为无人机的期望航向角，  $\lambda_{\chi}$  为目标横向视线角。

无人机纵向的跟踪采用比例导引律，在稳定平台实现对目标的锁定后，将目标的视线信息提供给无人机制导控制系统，从而完成飞行控制，导引律的表达式如下

$$
\ddot{u}_{des} = NL\times \dot{\lambda}_{\gamma} \tag{5.23}
$$

式中， $\dot{\lambda}_{\gamma}$  为纵向视线角速度， $N$  为导引比， $\ddot{u}_{des}$  为末制导律产生的期望加速度指令， $L$  为加速度指令的法向向量，可以按以下不同方式计算

$$
L_{TPNG} = u_T - u \tag{5.24}
$$

$$
L_{PPNG} = -\dot{u} \tag{5.25}
$$

式中， $u_{T}$  和  $u$  分别为目标和无人机的位置， $\dot{u}_{T}$  和  $\dot{u}$  分别为目标和无人机的速度， $L_{TPNG}$  、 $L_{PPNG}$  分别对应的是真比例导引律和纯比例导引律（Pure Proportional Navigation Guidance, PPNG）[240]。

纵向视线角速度  $\dot{\lambda}_{\gamma}$  可以表示为

$$
\dot{\lambda}_{\gamma} = \frac{(u_{T} - u)\times(\dot{u}_{T} - \dot{u})}{|u_{T} - u|^{2}} \tag{5.26}
$$

![](images/eb68b93234b3b6998b709f899fb9d9816a3f9890bf66e271e60ef0f05a1e2c18.jpg)  
图5.9所示为本文采用的横向与纵向跟踪策略示意图 图5.9无人机自主跟踪策略

无人机的纵向跟踪策略采用真比例导引律，也就是期望的加速度指令垂直于视线方向。在解算跟踪指令时，考虑机载视觉导引稳定平台已成功锁定目标，认为光轴与视线方向一致。

根据第四章算法，在跟踪过程中可以得到与真实目标轮廓一致的目标预测框。设首次检测到目标时的目标预测框长和宽分别为  $ap_{0}$  、 $bp_{0}$ ， $t$  时刻跟踪系统输出的目标预测框长和宽分别为  $ap(t)$  、 $bp(t)$ ，根据目标的尺度变化，设计沿视线方向的

加速度指令  $\ddot{u}_{los}^{des}$  以控制无人机与目标的相对距离，从而使画面中的目标大小保持固定，其表达式如下

$$
\ddot{u}_{los}^{des} = \frac{(u_T - u)}{|u_T - u|}\cdot (k_1^{so}e_{so} + k_2^{so}\dot{e}_{so} + k_3^{so}\int e_{so}) \tag{5.27}
$$

式中，  $k_{1}^{so}$  、  $k_{2}^{so}$  、  $k_{3}^{so}$  为比例、微分、积分系数，  $e_{so}$  由画面中目标当前尺寸与首次检测到的目标尺寸的偏差计算得到，其表达式如下

$$
e_{so} = \sqrt{\frac{a_0b_0}{ap(t)bp(t)}} -1 \tag{5.28}
$$

当目标在画面中变小时，认为目标在远离无人机，为了避免目标过小而导致跟踪失败，则无人机产生沿视线方向向前的加速指令以持续接近目标。而当目标在画面中增大时，认为目标在向无人机靠近，则产生沿视线方向向后的加速度指令，以增大相对距离。

# 5.3 运动目标自主跟踪系统与飞行试验验证

# 5.3.1 基于三轴稳定平台的运动目标跟踪测试

为了验证本文搭建的三轴稳定平台系统对运动目标的跟踪性能，在户外利用三脚架和无人机挂载平台，并与机载计算机相连，实时地执行算法。分别利用四旋翼无人机、行人等运动目标进行跟踪测试，测试过程如图5.10和图5.11所示。

![](images/d4c346454c1612617984dffa920cbb4303d3e3e39a985760645974e841207985.jpg)  
图5.10 地面支架搭载稳定平台对四旋翼无人机目标进行跟踪测试

![](images/ce813daa07ad0e638b4acc5f631d30120352a9c6af28c9221d2e83febe5ce9b1.jpg)  
图5.11利用无人机搭载稳定平台对慢速移动的行人目标进行跟踪测试画面左上角为目标的实时偏差值，测试结果如图5.12、图5.13和图5.14所示。

![](images/031cc9b7b38bd6e90d8d21c0d1791261206da0d8262b7133cea920ed5d2c6528.jpg)  
图5.12间歇性快速移动目标跟踪测试数据

![](images/3d5c96c35889d9cb287a15fc7076cda4a8f75e943f9c70a4d85d121c8b09a62f.jpg)  
图5.13慢速移动行人目标跟踪测试数据

![](images/b17a8b3f28b9b993249c3ff93fe1aea486a769ae9db50260d444ef13c03c973a.jpg)  
图5.14四旋翼无人机目标跟踪测试数据

由图5.12可知，尽管目标位置在较短时间内产生较大变化，在  $\mathbf{X}$  方向拉偏约200像素点，但平台快速回到稳定跟踪状态，稳态误差大约为  $\pm 3$  像素。这里的  $\mathbf{X}$  方向像素偏差指的是由目标在画面中左右移动带来的偏差，而y方向像素偏差指的是由目标在画面中上下移动带来的偏差。

由图5.13所示的行人跟踪结果可以看出，无人机挂载稳定平台飞行时会产生一定程度的抖动，在长达约250s的跟踪测试过程中，行人以  $0.9\sim 2\mathrm{m / s}$  的速度朝任意方向运动，而  $\mathbf{X}$  方向的跟踪精度约为  $\pm 9.34$  像素，y方向的跟踪精度约为  $\pm 5.07$  像素。

图5.14为采用四旋翼无人机搭载稳定平台在空中跟踪另一个小型四旋翼的测试结果，偏差主要是由目标快速机动移动所造成的。另外，当前系统存在约  $200\mathrm{ms}$  的图像传输延迟，主要是由试验系统硬件设备决定的，这也会影响跟踪性能并带来一定的偏差。

不同测试场景下的跟踪数据统计如表5.1所示

表5.1不同测试环境下稳定平台跟踪目标的均方根误差  

<table><tr><td>均方根误差（Pixel）</td><td>x方向</td><td>y方向</td><td>2D</td></tr><tr><td>间歇性快速移动目标</td><td>31.57</td><td>4.83</td><td>31.94</td></tr><tr><td>慢速移动行人目标</td><td>9.34</td><td>5.07</td><td>10.63</td></tr><tr><td>四旋翼无人机目标</td><td>21.50</td><td>19.28</td><td>28.88</td></tr></table>

跟踪行人时，目标高度变化可以基本忽略，并且行人的机动能力相对有限，因此跟踪误差较小。而跟踪一个飞行中的四旋翼无人机则复杂一些，四旋翼可以快速机动并且在三维空间中朝任意方向运动，画面中目标在短时间内进行转向或者改变飞行高度等都操作都会给当前的系统带来一定的偏差。若要进一步地提高对这类机动运动目标的跟踪效果，则需要从硬件配置、算法等多方面着手进行改进。

# 5.3.2 飞行试验系统搭建

如图 5.15 所示，本文自主设计并搭建了四旋翼无人机飞行试验平台，起飞重量约  $5.1\mathrm{kg}$ ，轴距  $650\mathrm{mm}$ ，所搭载的三轴稳定平台重量约为  $409\mathrm{g}$ 。

![](images/580125d7e7ee1315674ea208eb94fcf35d750ce73fdf341d182d71baef408025.jpg)  
图5.15本文搭建的全自主目标跟踪飞行试验平台

为了实现对运动目标的自主跟踪，无人机采用NVIDIA Jetson TX2作为机载计算机用于处理目标的实时图像，并将目标信息提供给Pixhawk控制器用于无人机的制导控制，本文采用的Pixhawk控制器固件版本为1.9.2，机载计算机与飞行控制器之间采用串口进行数据传输。

机载视觉导引稳定平台、飞行控制器、机载计算机等无人机的主要机载设备如图5.16所示[241]。

![](images/96bf8a3c494476d1d31649051bbf2e3b01ba3c32ab9a3f494b3ba4f97dd50067.jpg)  
图5.16自主跟踪飞行试验平台主要机载设备

用于运动目标自主跟踪试验的全部飞行程序都在ROS环境中搭建，并采用了mavros节点对所有MAVLink数据包进行处理，无人机的控制频率设置为  $30\mathrm{Hz}$ 。无人机启动时可以采用遥控器进行控制，当遥控器将系统切换至外部控制模式时，

无人机则进入完全自主状态，自动地执行程序，搜索并跟踪目标。本文在ROS中搭建的飞行程序框架如图5.17所示。

![](images/80bb0d361be68230803a7b5f7af5fd8a842061c7e194c814562757bf4d38f65a.jpg)  
图5.17无人机目标跟踪程序的ROS节点与话题关系

其中，tracker_ros节点负责对图像进行处理并跟踪指定目标，serial_node节点负责给出稳定平台的控制指令并通过串口进行输出，target_tracking节点主要包含一些逻辑判断并用于控制整个目标跟踪的流程，px4_pos_controller节点则负责解算无人机的位置控制和姿态控制指令，mavros节点主要用于接收和发送MAVLink数据包。

# 5.3.3 空中运动目标自主跟踪飞行试验

取某小型四旋翼无人机为空中运动目标，其轴距约  $410\mathrm{mm}$ ，并且以  $1.2\mathrm{m / s}$  左右的速度朝任意方向运动，在目标无人机飞行状态完全未知的情况下，利用本文搭建的试验系统对目标进行自主跟踪。当目标无人机出现在机载摄像机的画面中并且被成功识别后，稳定平台始终保持对目标的锁定，试验无人机进入自主跟踪状态，跟随目标运动。整个试验在户外空旷环境中进行，无人机进入自主跟踪状态的持续时间约90s，在自主跟踪飞行过程中，无人机始终紧跟目标，并保持在一定的相对距离范围内。

本文在地面端和机载端都对飞行试验过程进行了视频记录，并把机载端和地面端的视频进行同步处理，使其能够完整地展示飞行全过程，相关视频已上传至公开网络[242]。图5.18所示为本文在外场记录下的试验过程。

![](images/89b7dd3aede3eb9e99527ae05780bbaa6910d073313b4bb73aec67806d55a35b.jpg)  
图5.18 针对空中运动目标的无人机全自主跟踪飞行试验

试验过程记录的视频以及图片资料表明，当跟踪算法运行后，目标框随着目标的变化而不断调整。与真实目标吻合度较好，在机载视觉导引稳定平台控制系统的作用下，实现了对该四旋翼无人机的实时锁定，跟踪偏差较小。按照所提出的跟踪策略，试验无人机实现了对目标的自主跟随。无人机的姿态角、摄像机IMU所测的摄像机姿态角、机载视觉导引稳定平台的框架角数据如图5.19所示，无人机与目标在北东地坐标系下的飞行轨迹如图5.20所示。

![](images/2f88dc667036a05ce4ffc7005edfc1008aaf49118788e39efff75bac4025d81a.jpg)  
图5.19 无人机与稳定平台姿态、无人机飞行速度相关数据

![](images/c2b522e91be730a09fab62a3d8b7b224d959a9e620dd0dff4b5e9bc909242a6a.jpg)

![](images/5bd87b6fd4826585aca13278a7febc97f12f82cd5a7db59ba5455e108164f738.jpg)  
图5.20无人机跟踪目标的飞行轨迹

由于在搭建飞行试验平台时，飞行控制器和机载视觉导引稳定平台均采用了较低成本的传感器，在姿态和位置数据方面均存在一定的抖动和偏差，但根据试验结果来看，通过对比机载视觉导引稳定平台角度数据以及飞行过程中的摄像机画面，能够证明系统较为稳定地实现了对目标的锁定，无人机跟踪目标的飞行轨迹数据也能够印证跟踪策略以及飞行控制方法的有效性。如果能进一步提升图像处理能力、传感器精度以及控制系统性能，则会有利于提高跟踪的效果，而在当前硬件设备条件下所实现的响应速度和跟踪精度已经具有一定的吸引力。

# 5.4 基于机载视觉导引稳定平台的目标定位技术

# 5.4.1 基于无人机多点测向与测距的目标定位方法

三角测量法广泛应用于航海、建筑、路桥等领域，如图5.21所示，其主要原理是利用两个位置已知的点  $A$  、  $B$  与目标点  $T$  形成一个空间三角形，通过两个已知点之间的相对距离以及与目标点的方位信息，利用三角关系来确定目标点的空间坐标。

![](images/092b5311f902d763e8114fa99d91998b96befaca012d2570158517f5edcf916e.jpg)  
图5.21 基线长度对三角测量的影响

在实际应用中，由于图像背景噪声大等因素，很可能存在摄像机对目标点测向偏差，导致目标点的位置求解出现误差。在双目视觉中， $l$  也被称之为基线长度，当摄像机角度测量存在  $\delta \beta$  偏差时，测量到的目标深度值会产生  $\delta d$  的变化，当  $l$  较小时， $\delta d$  较大，而当  $l$  较大时， $\delta d$  将明显减小。也就是说，使用同样分辨率的摄像机，必须在较长的基线下才能使得三角测量的不确定性减小，并且至少需要两个以上的测量点才能输出目标位置。

与三角测量法不同，本节在利用机载视觉导引稳定平台对目标进行锁定和测向的基础上，通过激光测距仪测量得到相对距离，则依靠单个测量点即可实现对目标的位置估计。在无人机飞行过程中，可以通过选择多个点的测量数据进行融合，进一步地减小测量误差。图5.22所示是以无人机抓取地面目标为背景的多点测向与测距目标定位方法应用过程。

![](images/c34070a3db64529da55102c5d43c98a6658c4cd4b67e072b8eebcc54e6a1c651.jpg)  
图5.22 利用机载视觉导引稳定平台对目标进行搜索、锁定、定位和抓取的应用场景[243]

无人机首先利用机载视觉导引稳定平台对目标进行搜索，并对其进行检测和分类，在锁定待抓取目标后，利用多点测向和测距的方式对目标进行定位，随后根据初步解算的位置抵进待抓取目标的上方，再利用图像信息对机械臂进行引导，操纵机械臂抓取物体。

在对目标进行初步定位的过程中，无人机的位置可以由卫星导航系统提供，而在机载视觉导引稳定平台对目标进行锁定后，利用多架无人机对同一目标进行测量，获取多个测量点与目标之间的方位角、仰角以及距离等信息，对测量得到的数据进行融合，就能够实现对目标位置的估计。

![](images/0d4f219042d8b771d08ecb1a31a54f85fc58ee58f91404e5a0a16af0509104ea.jpg)  
图5.23基于机载视觉导引稳定平台多点测向与测距的目标定位原理

如图5.23所示，在北东地坐标系中，无人机机载视觉导引稳定平台瞄准目标  $T$  时的测量点  $O_{i}$  位置坐标已知为  $(x_{i},y_{i},z_{i})$  ，在点  $O_{i}$  对目标测向所得的方位角和仰角分别为  $\lambda_{\chi}^{i}$  和  $\lambda_{\gamma}^{i}$  ，对目标测距得到的距离估计值为  $R_{i}$  ，则利用点  $O_{i}$  所估计得到的目标点为  $T_{i}$  ，其坐标  $(t_{x}^{i},t_{y}^{i},t_{z}^{i})$  可由如下表达

$$
\left\{ \begin{array}{l}t_{x}^{i} = x_{i} + R_{i}\cos \lambda_{\gamma}^{i}\cos \lambda_{\chi}^{i} \\ t_{y}^{i} = y_{i} + R_{i}\cos \lambda_{\gamma}^{i}\sin \lambda_{\chi}^{i} \\ t_{z}^{i} = z_{i} + R_{i}\sin \lambda_{\gamma}^{i} \end{array} \right. \tag{5.29}
$$

在实际工作中，由于测向和测距数据存在误差，因此目标定位结果与实际的目标点是不一致的。针对当前位置的目标，假设一共获取了  $n$  组单点测量数据，取其中组  $m$  ，通过选取某点使其满足与这  $m$  个目标定位结果的距离之和最小，可以得到一个目标位置的估计值  $(\tilde{t}_x,\tilde{t}_y,\tilde{t}_z)$  。

$$
\min \sum_{i = 1}^{m}\sqrt{(\tilde{t}_x - t_x^i)^2 + (\tilde{t}_y - t_y^i)^2 + (\tilde{t}_z - t_z^i)^2} \tag{5.30}
$$

按照上述求解方法，由  $n$  组单点测量数据可以产生  $C_n^m$  个目标位置的估计值。为了进一步地提高目标位置估计精度，采用离散卡尔曼滤波器来实现对目标位置的最优估计。

首先，选取状态变量为  $X_{k} = \left\lfloor x_{k}\right.$ $\nu x_{k}$ $y_{k}$ $\nu y_{k}$ $z_{k}$ $\nu z_{k}$  』，其中  $x_{k}$  、  $y_{k}$  、  $z_{k}$  表示  $k$  时刻目标的位置，  $\nu x_{k}$  、  $\nu y_{k}$  、  $\nu z_{k}$  表示  $k$  时刻目标的速度，状态方程和观测方程表示如下

$$
X_{k + 1} = \Phi_k X_k + \Gamma_k W_k \tag{5.31}
$$

$$
Y_{k + 1} = H_k Y_k + V_k \tag{5.32}
$$

式中，  $\Phi_k$  为状态转移矩阵，  $\Gamma_k$  为噪声驱动矩阵，  $W_k$  为系统噪声序列，  $H_k$  为观测矩阵，  $V_k$  为观测噪声。

取采样时间间隔为  $\Delta t$ ，对于匀速直线运动的目标，其状态转移矩阵的表达式为

$$
\Phi_{k} = \left[ \begin{array}{cccccc}1 & \Delta t & 0 & 0 & 0 & 0\\ 0 & 1 & 0 & 0 & 0 & 0\\ 0 & 0 & 1 & \Delta t & 0 & 0\\ 0 & 0 & 0 & 1 & 0 & 0\\ 0 & 0 & 0 & 0 & 1 & \Delta t\\ 0 & 0 & 0 & 0 & 0 & 1 \end{array} \right] \tag{5.33}
$$

对于静止目标，其状态转移矩阵为单位矩阵。首先利用机载视觉导引稳定平台获得目标位置的观测值，根据卡尔曼滤波算法进行计算，可以得到目标定位的最优估计。为了获得无人机与目标的距离信息，机载视觉导引稳定平台需要将摄像机与激光测距仪均安装在稳定平台内框上，在锁定目标后，给出测距数据。为使平台结构更加简化，在缺少目标测距数据的情况下，也可以采用多架无人机同时对目标进行测向，利用三角测量法来解算目标位置。

# 5.4.2 地面移动目标跟踪与定位模拟仿真

在仿真环境下，针对某地面运动目标，选择用两架无人机分别位于目标的左后方和右后方对目标进行自主跟踪，同时对目标进行测向与测距，将两架无人机获得的目标单点定位数据进行融合，得到目标位置的最优估计。

在北东地坐标系下，设目标初始时刻的位置为  $(150\mathrm{m},0, - 4m)$ ，并以  $8\mathrm{m / s}$  的速度朝北偏东  $51.3^{\circ}$  方向做匀速直线运动。另外，取1号无人机的初始时刻位置为  $(0,0, - 154m)$ ，取2号无人机的初始时刻位置为  $(300\mathrm{m},0, - 154m)$ 。设目标状态估计的初值为  $X_0 = \left\lfloor 400 \begin{array}{lllll}2 & 30 & 3 & - 4 & 0 \end{array} \right\rfloor^T$ ，无人机的定位误差和机载视觉导引稳

定平台的测量误差如表5.2所示。

表5.2与目标定位相关的主要传感器测量误差  

<table><tr><td>观测量</td><td>测量误差（标准差）</td></tr><tr><td>无人机正北方向位置（m）</td><td>1</td></tr><tr><td>无人机正东方向位置（m）</td><td>1</td></tr><tr><td>无人机海拔高度（m）</td><td>1.5</td></tr><tr><td>方位角（°）</td><td>3</td></tr><tr><td>仰角（°）</td><td>0.5</td></tr><tr><td>相对距离（m）</td><td>1.5</td></tr></table>

仿真时间设置为180s，根据本章所提出的自主跟踪方法和第三章建立的无人机动力学模型，在Matlab环境下针对地面运动目标进行跟踪与定位的模拟仿真。整个飞行过程中，无人机始终保持对目标的自主跟踪，同时1号无人机和2号无人机利用机载视觉导引稳定平台的测向与测距数据，分别对目标的位置进行解算，将融合后的目标位置估计值与目标真实位置进行对比。

仿真结果如图5.24所示。

![](images/9242ad844eb5910274948359d94afbcc542a4c8078f3942c94e64b3ffd373c0c.jpg)

![](images/9ed69d6485bb7ff4181568e76c8142eb448d4fcf9cd038b0b237c0e5a22b52cc.jpg)  
图5.24 地面运动目标跟踪与定位结果

对单点目标定位方法和基于多点测向与测距的目标位置估计方法的估计误差进行统计，并取43.8s至180.0s稳定跟踪阶段的测量数据，结果显示1号机单点定位精度为  $14.44\mathrm{m}$ ，2号机单点定位精度为  $7.41\mathrm{m}$ ，基于多点测向与测距的目标位置估计精度为  $2.13\mathrm{m}$ 。由图5.24也可以看出，当目标的状态估计初值与真实的目标初始状态存在较大误差的情况下，该方法能够较快收敛，并有效地提高了目标定位精度。

# 5.4.3 实物演示试验与结果分析

本文自主设计并制作了如图5.25所示的微小型机载视觉导引稳定平台实物演示系统，在户外环境下，在机载计算机中运行目标跟踪算法，能够控制摄像机锁定并指向地面某个设定目标。实物演示试验中，共选取7个测量点，利用稳定平台对该目标进行测向，获得方位角和仰角，同时采用手持激光测距仪的方式对目标进行测距，得到稳定平台与目标之间的相对距离测量值，每个测量点之间相隔数米。测量时捕获目标的画面如图5.26所示。

![](images/9a8eeb9df54e767e57deda62d1bb2981c51ae32f3e248b8075ebe642f8fbf298.jpg)  
图5.25 自主搭建的微小型机载视觉导引稳定平台

![](images/aa160f5e8ab4d1c9c91428e2a4da559f2b6d834b00a336f44793ccbeb8ef607b.jpg)  
图5.26多点测向与测距户外试验

检测到目标后，摄像机在稳定平台的驱动下自动锁定目标，输出目标的方位角和仰角，通过激光测距仪获得稳定平台与目标的相对距离，同时在每一个测量点由无人机的导航系统记录下位置数据，7组测量数据如表5.3所示。

表5.3多点测向测距数据以及目标位置估计值  

<table><tr><td>测量点</td><td>测量位置（m）</td><td>方位角（°）</td><td>仰角（°）</td><td>测距（m）</td><td>目标位置估计（m）</td><td>目标实测位置（m）</td></tr><tr><td>1</td><td>(6.64,9.23,-1.09)</td><td>218.18</td><td>5.06</td><td>14.42</td><td>(-4.65,0.35,0.18)</td><td></td></tr><tr><td>2</td><td>(7.29,1.92,-1.03)</td><td>176.24</td><td>6.56</td><td>10.64</td><td>(-3.26,2.61,0.19)</td><td></td></tr><tr><td>3</td><td>(8.43,-1.59,-1.18)</td><td>155.96</td><td>6.72</td><td>10.48</td><td>(-1.08,2.65,0.05)</td><td></td></tr><tr><td>4</td><td>(10.91,-3.77,-1.07)</td><td>141.82</td><td>5.70</td><td>12.07</td><td>(1.47,3.65,0.13)</td><td>(-1.56,0.03,0.12)</td></tr><tr><td>5</td><td>(11.27,-5.56,-0.99)</td><td>133.10</td><td>4.82</td><td>14.36</td><td>(1.49,4.89,0.22)</td><td></td></tr><tr><td>6</td><td>(11.86,-10.11,-0.93)</td><td>126.40</td><td>3.74</td><td>17.71</td><td>(1.38,4.11,0.23)</td><td></td></tr><tr><td>7</td><td>(12.67,-16.43,-1.00)</td><td>123.16</td><td>2.98</td><td>22.6</td><td>(0.32,2.46,0.17)</td><td></td></tr></table>

无人机组合导航系统的位置估计精度为  $2\mathrm{m}$  左右，而激光测距仪的标称精度为1cm，由上表分析可知无人机的定位数据和方位角度测量数据存在一定偏差，这里的方位角度由磁罗盘给出，在磁场环境变化的情况下，磁罗盘数据会出现较大的误差。7组测量数据中，最大的目标定位误差达到了  $4.73\mathrm{m}$  。

针对7个目标定位数据，取一个点满足其到所有估计点的距离之和最小，经过计算可得其坐标为  $(0.01m,3.01m,0.16m)$  。在7组测量数据中任意选取两组目标定位数据，并解算两个估计点连线的中点坐标，由此可以生成21组对目标位置的观测数据。选取目标位置初始值为  $(0.01m,3.01m,0.16m)$ ，采用卡尔曼滤波器对目

标位置进行估计，计算结果如图 5.27 所示。

![](images/b9277dea3b875d3ad79127664cbc2f3c1a3fca61970c91ca29c0f6ae1457e259.jpg)  
图5.27 地面运动目标跟踪与定位结果

从观测数据可以看出，正东方向的目标定位误差较大，最大值接近  $5\mathrm{m}$ ，正北方向的目标位置误差在  $- 4\sim 2\mathrm{m}$  之间变化。经过多点测量数据融合之后的目标位置估计误差明显减小，最大定位误差在  $3.2\mathrm{m}$  以内，且估计结果比较稳定，在多个观测数据误差较大的情况下，目标位置估计数据没有出现太大波动。实物演示系统的测试结果验证了该方法的可行性，在传感器精度受限的条件下，该方法能够有

效地提高目标定位精度。

本文所采用的激光测距仪精度是比较高的，而稳定平台测向数据和无人机定位数据的偏差较大是造成位置估计存在误差的主要原因。在未来的工作中，为了进一步提高目标定位精度，在搭建机载视觉导引稳定平台时可考虑选择高精度的磁罗盘，保证测向准确度。此外，采用差分GPS也可以将无人机的定位精度提高至厘米级，有利于提高目标定位精度。

# 5.5 小结

本章基于机载视觉导引稳定平台，对三轴稳定平台的结构、控制系统、无人机的跟踪策略以及目标定位方法进行了研究，主要研究工作及结论如下：

（1）介绍了机载视觉导引稳定平台主要的结构形式，分析了三轴稳定平台的运动学关系、稳定与跟踪控制系统，搭建了机载视觉导引稳定平台试验系统，对不同的运动目标在地面和空中分别进行了跟踪测试，实现了较好的跟踪效果，并对测试结果进行分析，提出了改进的方向。

（2）针对运动目标的自主跟踪问题，研究并提出了基于真比例导引律和纯追踪法的无人机自主跟踪策略，自主搭建了用于验证该方法的飞行试验平台，在ROS系统下编写了无人机的飞行控制程序，开展了针对空中运动目标的全自主跟踪飞行试验，试验全程目标状态完全未知，通过试验数据和所记录的飞行视频可以验证该系统的有效性和可行性；

（3）分析了三角测量法的基本原理和不足之处，针对实际应用中的目标定位问题，提出了基于机载视觉导引稳定平台的多点测向与测距目标定位方法，对地面匀速直线运动目标进行了跟踪与定位模拟仿真，利用自主设计和制作的小型机载视觉导引稳定平台系统，在户外环境下对地面目标进行了定位试验，对试验数据进行了整理和分析，证明了该方法能够有效降低目标位置估计值与目标实测位置的偏差，最后针对定位误差产生的原因进行了分析，并提出了改进目标定位精度的方法。

# 第六章 微小型无人机编队飞行控制系统研究

# 6.1 引言

单个微小型无人机的巡航时间、任务载荷和侦察范围是非常有限的，而通过多无人机编队协同的方式可增强系统的灵活性和任务成功率等，能够完成一些更为复杂的实际任务。根据第二章所提出的无人机低空精确打击地面目标任务的要求，微小型无人机首先对目标进行搜索、跟踪和定位，在解算得到目标位置后，则指派多个微小型无人机打击该目标。在对目标进行打击之前，负责打击任务的多架无人机需要在预定区域进行集结，并按照一定的队形进行编队飞行，在收到打击指令和目标位置后，保持编队队形并飞向目标所在的区域，最后进入末制导阶段。鉴于田嘉懿[244]在其博士论文中已经对打击地面运动目标的末制导律进行了深入研究，本章主要针对微小型无人机的编队飞行控制技术展开研究。

与多旋翼无人机相比，固定翼无人机在有效载荷、续航性能、末端打击速度等方面均具有一定优势，在起飞重量相同的情况下，采用固定翼无人机对目标进行打击造成的毁伤效果更大。除此之外，多旋翼无人机可处于准静止状态并在特定点进行悬停，有充足的时间去等待编队系统完成调度，而固定翼无人机必须以一定的速度向前飞行，在组成编队后需要实时地对编队成员进行控制，系统更为复杂。目前，固定翼无人机编队飞行控制技术的相关研究还比较少，在实际飞行任务中的应用还不成熟，因此其研究意义更大。

本章基于一致性理论，提出了一种五机编队队形及其通信拓扑，设计了五机编队协同控制算法，通过仿真试验，证明了该算法可以使得编队实现预期的目标。将微小型固定翼无人机编队队形保持问题视为目标跟踪问题，提出了基于总能量控制的纵向跟踪策略和基于L1制导律的横向跟踪策略。本章采用低成本的模型飞机和飞行控制器，搭建了编队飞行试验系统，开展了三机和五机编队飞行试验，验证了该方法的可行性。

# 6.2 基于一致性理论的无人机编队飞行系统

# 6.2.1 图论的相关知识

无人机编队中各成员间的信息交换关系类似于一个结构化的图形，采用“节点”和“边”的方式来表示编队中的成员以及相互之间的联系。基于图论的相关知识，可以通过无向图有向图来描述各成员无人机之间的通信拓扑结构，如图6.1所示。

![](images/17ad896b7fbbf4aecb607d0dfe973bccbde59615feab62622af8c3208f2f56c6.jpg)  
图6.1图论中的有向图和无向图

对于图6.1中的有向图，可以描述为  $G_{1} = (\nu_{1},\epsilon_{1})$  ，其中  $\nu_{1} = \{1,2,\therefore$ $\epsilon_{1} = \{(1,2),(1,3),(2,3),(3,2)\}$  。而无向图  $G_{2} = (\nu_{2},\epsilon_{2})$  的节点集  $\nu_{2}$  和边集  $\epsilon_{2}$  分别描述为 $\nu_{2} = \{a,b,c\}$  和  $\epsilon_{2} = \{(a,b),(a,c),(b,c)\}$  。

邻接矩阵是表示图中各节点之间关系的矩阵[245]，有向图  $G = (\nu ,\epsilon)$  的邻接矩阵 $A$  表达式为

$$
A = [a_{ij}]\in R^{p\times p},a_{ii} = 0,a_{ij} > 0,(i\neq j) \tag{6.1}
$$

拉普拉斯矩阵  $L$  的表达式为

$$
L = [l_{ij}]\in R^{p\times p},l_{ii} = \sum_{i\neq j}a_{ij},l_{ij} = -a_{ij} > 0,(i\neq j) \tag{6.2}
$$

在上式中，当  $(j,i)\in \epsilon$  时，  $a_{ij}$  是一个正权值，本文设定  $a_{ij}$  等于1。而当  $(j,i)\notin \epsilon$  则  $a_{ij} = 0$  。

根据矩阵的定义可以推导出矩阵  $L$  满足

$$
l_{ij}< 0,(i\neq j) \tag{6.3}
$$

$$
\sum_{j = 1}^{p}l_{ij} = 0,(i = 1,2,\ldots ,p) \tag{6.4}
$$

# 6.2.2 无人机编队飞行运动学模型

本章考虑  $p$  个无人机组成的编队，假设编队内各成员无人机的飞行高度保持相同，则只需考虑无人机在水平面内的运动，因此可以在二维平面内建立无人机编队的运动学关系。在无人机飞行过程中，根据得到的控制指令，通过自动驾驶仪对指令进行响应，完成各分系统的动作执行。本章假设编队中所有无人机成员均采用参数相同的一阶速度保持器和一阶航向保持器所组成的自动驾驶仪，如下所示

$$
\dot{\nu}_{i} = -\frac{1}{\tau_{\nu}} \nu_{i} + \frac{1}{\tau_{\nu}} \nu_{i}^{c} \tag{6.5}
$$

$$
\dot{\phi}_{i} = -\frac{1}{\tau_{\phi}} \phi_{i} + \frac{1}{\tau_{\phi}} \phi_{i}^{c} \tag{6.6}
$$

则编队中第  $i$  个无人机的运动模型为

$$
\left[ \begin{array}{c}x_{i} \\ \dot{y}_{i} \\ \dot{\nu}_{i} \\ \dot{\phi}_{i} \end{array} \right] = \left[ \begin{array}{c} \nu_{i} \cos \phi_{i} \\ \nu_{i} \sin \phi_{i} \\ -\frac{\nu_{i}}{\tau_{\nu}} \\ -\frac{\phi_{i}}{\tau_{\phi}} \end{array} \right] + \left[ \begin{array}{c} 0 & 0 \\ 0 & 0 \\ \frac{1}{\tau_{\nu}} & 0 \\ 0 & \frac{1}{\tau_{\phi}} \end{array} \right] \tag{6.7}
$$

式中，无人机的状态为  $X_{i} = \left[x_{i} \quad y_{i} \quad \nu_{i} \quad \phi_{i}\right]^{T}$ ，控制输入为  $U_{i} = \left[\nu_{i}^{c} \quad \phi_{i}^{c}\right]^{T}$ ，其中  $(x_{i}, y_{i})$  为第  $i$  个无人机在二维平面中的位置，  $\nu_{i}$  为飞行速度，  $\phi_{i}$  为航向角，根据笛卡尔坐标系，当速度指向  $x$  轴正向时，航向角为 0，逆时针旋转为正。  $\tau_{\nu}$  和  $\tau_{\phi}$  分别为自动驾驶仪中的速度时间常数和航向角时间常数。  $\nu_{i}^{c}$  和  $\phi_{i}^{c}$  分别为无人机自动驾驶仪的速度和航向角输入指令[246]。

# 6.2.3 编队目标

本文为五架无人机在巡航状态下编队飞行设计了一种期望的队形，并采用如图6.2所示的通信方式实现成员无人机之间的信息交互，使其最终能够保持固定的巡航速度和编队队形。

![](images/41d543df4c57f55f10152de3136eaad29d88fcd3cc608fccfdb6c759c24cff3e.jpg)  
图6.2 五架无人机组成的通信拓扑图

编队控制目标具体如下

$$
\lim_{t\to T}(x_0 - x_i) = h_{ix0} \qquad i = 1,2,3,4 \tag{6.8}
$$

$$
\lim_{t\to T}(y_0 - y_i) = h_{iy0} \qquad i = 1,2,3,4 \tag{6.9}
$$

$$
\lim_{t\to T}(\nu_0 - \nu_i) = 0 \qquad i = 1,2,3,4 \tag{6.10}
$$

$$
\lim_{t\to T}(\phi_0 - \phi_i) = 0 \qquad i = 1,2,3,4 \tag{6.11}
$$

其中，0号机为领航者，  $h_{ix0}$  和  $h_{ix0}$  表示第  $i$  架无人机与领机的期望位置偏差，  $T$  表示有限时间的最大值。控制的目标是使得跟随者（1、2、3、4号机）与领航者（0号机）保持固定的相对位置，并使跟随者的速度和航向角与领航者保持一致。

假设整个系统中，跟随者之间的信息交互矩阵为  $A = [a_{ij}]^{m\times n}$ ，领航者与跟随者之间的信息交互矩阵为  $B = diag\{b_1,b_2,\ldots ,b_n\}$ ，采用图6.2所示的拓扑结构，假设每边权重为1，则

$$
A = \left[ \begin{array}{llll}0 & 0 & 1 & 0\\ 0 & 0 & 0 & 1\\ 1 & 0 & 0 & 0\\ 0 & 1 & 0 & 0 \end{array} \right]\\ B = \left[ \begin{array}{llll}1 & 0 & 0 & 0\\ 0 & 1 & 0 & 0\\ 0 & 0 & 0 & 0\\ 0 & 0 & 0 & 0 \end{array} \right] \tag{6.12}
$$

定义第  $i$  架无人机在  $x$  与  $y$  方向编队误差  $e_{ix}$  和  $e_{iy}$  为

$$
e_{ix} = \sum_{j = 1}^{n}a_{ij}(x_i - x_j - h_{ijx}) + b_i(x_i - x_0 - h_{ix0}) \tag{6.14}
$$

$$
e_{iy} = \sum_{j = 1}^{n}a_{ij}(y_i - y_j - h_{ijy}) + b_i(y_i - y_0 - h_{iy0}) \tag{6.15}
$$

其中，  $h_{ix0}$  、  $h_{iy0}$  、  $h_{ijx}$  和  $h_{ijy}$  均根据期望队形而事先给定，  $h_{ijx}$  和  $h_{ijy}$  表示第  $i$  和  $j$  架跟踪者之间的期望位置偏差。

本文设计了一种由五架无人机组成的编队队形，其中0号机位于编队最前方，1号机与2号机分别位于0号机的左后方和右后方，三架无人机组成了边长为  $l_{f}$  的等边三角形。3号机和4号机分别位于1号机和2号机的正后方，1、2、3、4号机共同组成一个边长为  $l_{f}$  的正方形。根据此队形图可以计算得到上述控制器中的相关参数。

图6.3中所示为该五机编队队形的示意图。

![](images/32362a01ea2bcc4501010ecb0b8a81f9e2ef4d7e784b09a672de9bb2fa6c1e29.jpg)  
图6.3 无人机编队期望队形示意图

# 6.2.4 控制器设计

将系统模型转换为

$$
\left\{ \begin{array}{l}\dot{x}_i = \nu_i\cos \phi_i\\ \dot{\nu}_i = \nu_i\sin \phi_i\\ \dot{\nu}_i = -\frac{\nu_i}{\tau_\nu} +\frac{1}{\tau_\nu}\nu_i^c\\ \dot{\phi}_i = -\frac{\phi_i}{\tau_\phi} +\frac{1}{\tau_\phi}\phi_i^c \end{array} \right. \tag{6.16}
$$

定义无人机的位置为  $\xi_{i} = [x_{i},y_{i}]^{T}$ ，将  $\dot{x}_i$ 、 $\dot{y}_i$  对时间求导，则第  $i$  个无人机的动力学模型可表示为

$$
\ddot{\xi}_i = u_i \tag{6.17}
$$

其中， $u_{i} = [u_{xi},u_{yi}]^{T}$  为虚拟控制输入，将  $\dot{\nu}_i$ 、 $\dot{\phi}_i$  的表达式带入后，可得  $u_{i}$  和实际控制量  $\nu_{i}^{c}$ 、 $\phi_{i}^{c}$  之间的关系为

$$
\left\{ \begin{array}{l}\nu_{i}^{c} = \tau_{\nu}(u_{xi}\cos \phi_{i} + u_{yi}\sin \phi_{i}) + \nu_{i}\\ \phi_{i}^{c} = \frac{\tau_{\phi}}{\nu_{i}} (-u_{xi}\sin \phi_{i} + u_{yi}\cos \phi_{i}) + \phi_{i} \end{array} \right. \tag{6.18}
$$

设计控制输入  $u_{xi}$  和  $u_{yi}$  为

$$
\left\{ \begin{array}{l}u_{xi} = -k_{1x}e_{ix} - k_{2x}\dot{e}_{ix}\\ u_{yi} = -k_{1y}e_{iy} - k_{2y}\dot{e}_{iy} \end{array} \right. \tag{6.19}
$$

式中， $k_{1x}$ 、 $k_{2x}$ 、 $k_{1y}$  和  $k_{2y}$  为控制器参数。

# 6.2.5 仿真试验

为了证明系统能够收敛至编队目标，并保持固定队形巡航，对该无人机编队飞行系统在Matlab环境下进行仿真试验，五架无人机的初始位置、速度和航向角如表6.1所示。

表6.1五架无人机的初始状态  

<table><tr><td rowspan="2">无人机编号</td><td colspan="4">参数</td></tr><tr><td>x i /m</td><td>y i /m</td><td>v i / (m/s)</td><td>φ i / (°)</td></tr><tr><td>0</td><td>0</td><td>0</td><td>30</td><td>45</td></tr><tr><td>1</td><td>-45</td><td>-50</td><td>2</td><td>30</td></tr><tr><td>2</td><td>-5</td><td>-30</td><td>8</td><td>36</td></tr><tr><td>3</td><td>-40</td><td>-45</td><td>7</td><td>45</td></tr><tr><td>4</td><td>-15</td><td>-55</td><td>4</td><td>60</td></tr></table>

设置自动驾驶仪的速度和航向时间常数分别为  $\tau_{\nu} = 1$  和  $\tau_{\phi} = 0.5$  ，取  $l_{f} = 10m$  控制器参数为  $k_{1x} = 2.5$  、  $k_{2x} = 0.9$  、  $k_{1y} = 2.5$  和  $k_{2y} = 0.9$  。在约10s的仿真试验过程中，五架无人机的位置、速度、航向角以及最终保持的编队队形如图6.4所示。

![](images/7d7a2cd946b50d24ed778c2c827e1f48dc7b5e260bb7d71c604c94092cc19988.jpg)

![](images/cd7be3db9efc54008ef2393960cc92525974c2c4a48dbadf7b0d62730d1e21cb.jpg)  
图6.4 五架无人机编队飞行仿真试验结果

由于跟随者（1、2、3、4号机）的初始速度较低且初始位置与领航者（0号机）相距较远，仿真初始阶段跟踪者（1、2、3、4号机）的飞行速度和航向角变化较大。经过数秒钟后，五架无人机组成的编队系统在位置、速度和航向上均能逐步收敛到期望值，并且能够保持预定的编队队形进行巡航飞行，实现了预期目标。

# 6.3 无人机编队队形保持控制方法

# 6.3.1 无人机质心运动模型

研究无人机质心运动时，一般把运动分为在铅垂平面内的运动和在水平面内的运动两个部分，也就是纵向运动和横向运动。图6.5所示为无人机在只考虑纵向运动时的运动状态和受力分析示意图。

![](images/0d40a087279944ca35f935725a8d8392b7f7704723a145cb9c09ae289bc42fa2.jpg)  
图6.5 无人机纵向运动受力分析

如图6.5所示，无人机受到作用于质心的多个外力，其中  $T$  为发动机产生的推力，  $W$  为无人机重力，推力与机身轴线之间有一定的安装角  $\phi$  ，  $L$  为升力垂直于

速度矢量  $V$  ,  $D$  为阻力与速度矢量方向相反,  $\gamma$  为航迹倾角,  $\alpha$  为迎角。

根据动量定理, 可建立无人机质心运动方程, 如下所示

$$
\left\{ \begin{array}{l l}{m\frac{d V}{d t} = T\cos (\alpha +\phi)\cos \beta -D - m g\sin \gamma}\\ {m V\cos \gamma \frac{d\chi}{d t} = T[\sin (\alpha +\phi)\sin \mu -\cos (\alpha +\phi)\sin \beta \cos \mu ] + C\cos \mu +L\sin \mu}\\ {-m V\frac{d\gamma}{d t} = T[-\sin (\alpha +\phi)\cos \mu -\cos (\alpha +\phi)\sin \beta \sin \mu ] + C\sin \mu -L\cos \mu +m g\cos \gamma} \end{array} \right.
$$

式中,  $\beta$  为侧滑角,  $\chi$  为航迹偏角,  $\mu$  为速度滚转角,  $D$  为无人机所受的飞行阻力[247]。

无人机对称飞行所需满足的条件为

$$
\left\{ \begin{array}{l l}{\phi = 0}\\ {\mu = 0}\\ {\beta = 0}\\ {d\chi /d t = 0} \end{array} \right. \tag{6.21}
$$

只考虑无人机纵向运动的情况时, 质心运动方程可简化为

$$
\left\{ \begin{array}{l} m \frac{dV}{dt} = T \cos (\alpha + \phi) - D - m g \sin \gamma \\ -m V \frac{d\gamma}{dt} = -T \sin (\alpha + \phi) - L + m g \cos \gamma \end{array} \right. \tag{6.22}
$$

当飞行迎角和发动机的安装角较小时, 考虑有  $\cos (\alpha + \phi) \approx 1$ ,  $\sin (\alpha + \phi) \approx 0$ , 则上述方程可进一步简化为

$$
\left\{ \begin{array}{l} m \frac{dV}{dt} = T - D - m g \sin \gamma \\ -m V \frac{d\gamma}{dt} = -L + m g \cos \gamma \end{array} \right. \tag{6.23}
$$

若无人机在铅垂面内做非定常直线运动, 则  $\frac{d\gamma}{dt} = 0$ , 上述方程可进一步简化为

$$
\left\{ \begin{array}{l} m \frac{dV}{dt} = T - D - m g \sin \gamma \\ L = m g \cos \gamma \end{array} \right. \tag{6.24}
$$

本文的无人机编队中各成员都在固定高度飞行, 因此队形保持控制只考虑水平面内的运动, 如图 6.6 所示。

![](images/2bebe575791debdbd446cd84d755675c97de96ffa7ce615fb8e32955be6ccd93.jpg)  
图6.6 领航者与跟随者相对运动状态

无人机的导航系统可以提供其在导航坐标系下的位置，在实际飞行过程中，把编队队形保持问题视为目标跟踪问题，跟随者可以通过通信系统接收领航者的飞行数据，从而实现对领航者与跟随者相对运动关系的控制。以领航者为原点建立动坐标系，则跟随者  $E_{i}$  的期望位置由  $h_{Li}^{\lambda}$  和  $h_{Li}^{\gamma}$  两个量来描述，而跟随者与期望位置之间存在纵向偏差  $\Delta L$  和侧向偏差  $\Delta L$ ，则可以把跟随者的位置控制分为纵向和横向两部分来考虑，根据导航系统解算得到的纵向和侧向偏差数据来计算无人机的制导控制指令。

# 6.3.2 无人机纵向运动控制

传统的无人机纵向运动控制一般是采用对升降舵进行操纵来改变机体俯仰角，从而在短周期内控制其上升或下降，而油门则用来控制飞行速度，但实际上，这两种控制之间存在着较大的耦合作用。通过升降舵的拉杆操作在使得无人机上升的同时，随着高度的增加，飞行速度会明显降低。而通过油门拉高螺旋桨转速在使得无人机速度提升的同时，随着动能的增加，无人机也会产生明显的爬升趋势。因此，本文在设计纵向控制器时，采用总能量控制系统（Total Energy Control System, TECS），对无人机的纵向运动进行解耦控制[248]。

由受力分析可知，无人机在飞行过程中的需用推力  $T_{R}$  为

$$
T_{R} = W(\gamma + \frac{\dot{V}}{g}) + D \tag{6.25}
$$

其中，考虑了航迹倾角  $\gamma$  较小，可以用  $\sin \gamma \approx \gamma$  来简化方程。

在初始时刻的巡航状态下, 可以假设推力与阻力相平衡, 即  $\gamma = \dot{V} = 0$  。考虑无人机的纵向机动较小, 速度变化较为缓慢, 因此认为短周期运动中, 阻力几乎不变, 则扰动后的推力增量与航迹倾角、切向加速度和无人机的重量相关

$$
\Delta T \approx W(\gamma + \frac{\dot{V}}{g}) \tag{6.26}
$$

考虑无人机单位重量的总能量  $E$  为

$$
E = h + \frac{V^2}{2g} \tag{6.27}
$$

式中,  $h$  和  $V$  为无人机当前的飞行高度和速度, 对  $E$  进行微分, 并考虑航迹倾角较小, 则有

$$
\dot{E} = V(\gamma + \frac{\dot{V}}{g}) \tag{6.28}
$$

$$
\frac{\dot{E}}{V} = \gamma + \frac{\dot{V}}{g} = \frac{\Delta T}{W} \tag{6.29}
$$

由上式可以看出, 推力的变化是改变总能量的决定性因素, 而通过升降舵的偏转能够在短时间内改变  $\gamma$  。当推力恒定时, 则无人机的总能量不会改变, 而升降舵的作用可以使无人机的动能和势能相互转化, 从而实现能量分配的效果。因此总能量控制方法的基本思路就是用推力来控制总能量的变化率, 用升降舵来控制总能量的分配率。其中, 能量分配率定义为动能与势能的变化率之差, 表达式如下

$$
\dot{L} = \frac{\dot{V}}{g} - \gamma \tag{6.30}
$$

总能量的期望值可以由期望高度  $h_{des}$  和期望速度  $V_{des}$  计算得到, 期望高度可以由航线高度决定, 而期望的速度则根据纵向位置误差和目标无人机的速度可以计算得到, 表达式如下

$$
V_{des} = V_{trim} + k_{vp} \Delta L + k_{vi} \int_{0}^{t} \Delta L dt + k_{vd} (V_{obj} - V) \tag{6.31}
$$

式中,  $V_{trim}$  为巡航时的飞行速度,  $\Delta L$  为无人机与目标无人机的纵向位置偏差,  $V_{obj}$  为目标无人机的飞行速度,  $k_{vp} \sim k_{vi} \sim k_{vd}$  为可调节的系数。

取总能量变化率与期望值的偏差为  $\dot{E}_e$ , 能量分配率与期望值的偏差为  $\dot{L}_e$ , 期望推力  $T_c$  和期望俯仰角指令  $\theta_{ec}$  为

$$
\left\{ \begin{array}{l} T_c = k_{tp} \dot{E}_e + k_{ti} \int_0^t \dot{E}_e dt \\ \theta_{ec} = k_{ep} \dot{L}_e + k_{ei} \int_0^t \dot{L}_e dt \end{array} \right. \tag{6.32}
$$

式中， $k_{tp}$ 、 $k_{ti}$ 、 $k_{ep}$  和  $k_{ei}$  均为控制器参数。

# 6.3.3 无人机横向运动控制

无人机按照航线自主飞行时，其跟踪的目标路径一般是一条复杂的曲线，而控制目标则是希望无人机更为紧密地接近目标路径。当无人机与目标路径产生偏差时，利用L1制导律来产生一个垂直于速度方向的加速度指令，使得无人机在动态飞行过程中，受控制指令作用不断接近目标路径，从而实现自主航线飞行，如图6.7所示。

![](images/56bea3ff53783b1a4183fbe162a1972c90d675bd888e7604a5722b719da873e5.jpg)  
图6.7 L1制导律示意图

上图中，无人机与参考点的距离向量为  $L_{1}$ ， $L_{1}$  与速度向量  $V$  之间的夹角为  $\eta$ ，理想条件下无人机沿圆弧接近参考点， $R$  为圆弧的半径，则横向加速度指令  $a_{s}$  可以通过下面的式子来计算。

$$
a_{s} = 2\frac{V^{2}}{L_{1}}\sin \eta \tag{6.33}
$$

$a_{s}$  的方向由  $L_{1}$  与  $V$  的指向决定，如果  $L_{1}$  指向  $V$  的左侧，则  $a_{s}$  向左作用，反之则  $a_{s}$  向右作用，总的来说就是使无人机的速度方向始终趋近于  $L_{1}$  的方向并与之对齐。

在给定  $L_{1}$  长度的条件下，当无人机与目标路径横向偏差较大时，则夹角  $\eta$  也较大，此时产生的横向加速度指令也较大，则控制律会使无人机旋转并使其速度方向以一个较大的角度接近目标路径。反之，当无人机与目标的横向偏差较小时，则夹角  $\eta$  也较小，此时横向加速度指令较小，控制律会使无人机旋转并使其速度方向以一个较小的角度接近目标路径[249]。

在实际的飞行过程中，目标路径或者预定航线一般是由多个期望的航点来确定的，进而可以根据无人机的当前位置和前后航点位置来计算制导指令，如图6.8所示。

![](images/a82ac3ddda4f741d2fa7e38507ef40f50b7c434bb3e39918f9f740a2fe221f75.jpg)  
图6.8 根据前后航点计算制导指令

在实际的路径跟踪过程中， $L_{1}$  是一个可调节的系数， $d$  是无人机与期望路径 $\overline{AB}$  之间的侧向偏差，夹角  $\eta_{1}$  和  $\eta_{2}$  则可以由无人机的位置、速度方向和期望航点A和B位置等已知量计算得到，从而能够解算出加速度指令  $a_{s}$ 。

当夹角很小时，可以假设

$$
\sin \eta \approx \eta = \eta_{1} + \eta_{2} \tag{6.34}
$$

根据运动关系可以求出  $\eta_{1}$  和  $\eta_{1}$  为

$$
\eta_{1} \approx \frac{d}{L_{1}} \tag{6.35}
$$

$$
\eta_{2} \approx \frac{\dot{d}}{V} \tag{6.36}
$$

则加速度指令  $a_{s}$  可以简化为

$$
a_{s} = 2 \frac{V^{2}}{L_{1}} \sin \eta \approx 2 \frac{V}{L_{1}} \left(\dot{d} + \frac{V}{L_{1}} d\right) \tag{6.37}
$$

# 6.4 编队飞行试验与结果分析

# 6.4.1 编队飞行试验系统搭建

本文采用了多架小型固定翼航模飞机来搭建飞行试验平台，基于Pixhawk控制器进行二次开发，对编队控制算法进行编程。每架试验无人机搭载一个Pixhawk控制器、两套数传模块和一个机载端RTK模块。两套数传模块分别连接Pixhawk控制器的TELEM1和TELEM2接口，其中与TELEM1接口连接的数传模块用于无人机和地面站之间的通信，而与TELEM2接口连接的数传模块则用于无人机之间的数据传输，本文采用的数传电台通信频率为900MHz。机载端RTK模块与飞行控制器相连，另外设置地面端RTK模块为基站，通过地面站与无人机间的数传链路，将基站的定位数据发送给飞行控制器，通过实时差分计算，可以对无人机的定位数据进行修正，实现无人机的高精度定位。图6.9所示为本文搭建的试验机（1号机、2号机、3号机），图6.10所示为编队飞行试验系统的硬件框架。

![](images/03a26184ba5872de14c912da257b7deec5274761203423977b0eaf10584b8248.jpg)  
图6.9 用于编队飞行试验的小型固定翼无人机

![](images/c06b03bb8db7405c88ed1a9567c4dea939530e1545c8583969230ddb86b3af19.jpg)  
图6.10 编队飞行试验系统硬件框架

验证机可以在飞行控制器的自主模式下起飞，同时操作人员可根据试验情况使用遥控器将飞行模式切换到手动、增稳、返航等模式，当试验机进入编队时，则切换至外部控制模式，无人机根据编队控制程序保持预定队形飞行。地面站与数传模块相连，实时地接收所有无人机的飞行数据，在地图界面上显示无人机的飞行状态，同时也可以通过地面站来更改无人机的航线或者切换无人机的飞行模式等。

编队飞行试验对三机和五机两种构型分别进行测试。三机编队飞行试验中，1号机作为领航者，起飞后按预定航线围绕试验场地飞行，2号机和3号机作为跟随者，起飞后根据地面站指令加入飞行编队，紧跟在1号机的后方并与1号机保持纵队队形，随后按照指令进行队形变换，紧跟在1号机的左后方和右后方呈三角队形，如图6.11所示。三机编队飞行试验中，飞行编队在纵队队形和三角队形之间需进行数次变换。

![](images/a32996220c68ea7373ad7328b0b9380b2f221e8d6c02d39b36e3c12bebba7cd3.jpg)  
图6.11 编队飞行队形变换示意图

在五机编队飞行试验中，1号机先起飞进入航线，2、3、4、5号机按照一定顺序进入编队，五架无人机保持预定队形飞行一段时间后，按顺序退出编队并返航。

# 6.4.2 三机编队飞行试验与数据分析

三机编队飞行试验过程如图6.12所示

![](images/84da18cada161bcd6cd3c7654eaa40d7d5f8605192574efd94f489b1e496be93.jpg)  
图6.12 三机编队飞行试验过程

以1号机开机时的位置为原点建立本地北东地坐标系，三机编队飞行试验数据如图6.13所示。

![](images/c3cafce1a2e4a2ffcb86a0f050c7468e5f7297e1e3caa7ac921611a5ea4fee75.jpg)

![](images/1363a815bb4386d2495312cd374748572f02d3012d8baded93c1c2b32e98dd49.jpg)

![](images/ec639ba8842ec4562202fba0b5c2090ec79436d527a370ab2bd70f213109c5ef.jpg)  
图6.13 三机编队飞行试验结果

由图6.13可知，1号机起飞后进入预定航线自主飞行，航线高度  $58\mathrm{m}$  （海拔 $598\mathrm{m}$ ），2号机在第99秒加入编队，航线高度  $68\mathrm{m}$  （海拔  $608\mathrm{m}$ ），3号机在第139秒加入编队，航线高度  $72\mathrm{m}$  （海拔  $612\mathrm{m}$ ）。1、2、3号机首先组成纵队队形，1号机领航，2号机紧跟1号机（纵向保持  $10\mathrm{m}$  距离），3号机紧跟2号机（纵向保持  $10\mathrm{m}$  距离）。第532秒变成三角队形，1号机领航，2、3号机分别作为僚机在1号机左右两侧伴飞。其中2号机移动到1号机左后方，并与1号机保持纵向距离  $10\mathrm{m}$ ，侧向距离  $10\mathrm{m}$ ；3号机移动到1号机右后方，3号机与1号机保持纵向距离  $10\mathrm{m}$ ，侧向距离  $10\mathrm{m}$ 。第745秒变换成纵队，第980秒再次变成三角队形。第1283秒，2号机退出编队进入降落航线；第1294秒，3号机退出编队进入降落航线；1号机按照预定航线自主降落[250]。三机编队飞行时的队形保持位置偏差如图6.14所示。

![](images/fa2facba3d176438db81fbd3d3f2885098778648035423d74d43d5b1b6d4de96.jpg)

![](images/b0d49c3cdce365d80de00d73f5ef97b0381a933cf6d6a54e41ab1b09474a2220.jpg)

由上图可知，在1号机进行转弯时，2、3号机与期望位置的偏差最大，其最大的纵向和侧向位置偏差达到  $7 \sim 12\mathrm{m}$  。而当1号机沿直线段航线飞行时，2、3号机的位置偏差明显减小，其侧向和纵向的位置偏差均小于  $5\mathrm{m}$  。由于2、3号机是通过接收1号机的位置数据来控制自身在编队中的位置，在1号机转弯初期，其位置数据变化量并不大，导致生成的控制量较小，而2、3号机会沿着之前的方向飞行，因而带来了较大的队形偏差。而在1号机的机动过程中，航向角、滚转角等姿态数据的改变相比位置数据能够更快地反映其运动趋势。针对存在机动的无人机编队队形保持问题时，可以考虑引入领航者的姿态与速度数据来改进控制系统，从而进一步减小无人机成员与期望位置之间的偏差。

# 6.4.3 五机编队飞行试验与数据分析

五机编队飞行试验过程如图6.15所示。

![](images/df811330ffcecbaca26216e15d5be27ffce57713835c77a25c16a19621ce5f77.jpg)  
图6.15 五机编队飞行试验过程

基于本地北东地坐标系，五机编队飞行试验数据如图6.16所示。

![](images/39045fcf6a29c70e4e0425d5d3fd9217b42a11e1fca35d946504f27fb57f6be2.jpg)  
(a)X方向位移随时间变化曲线

![](images/07258941a645477cbdb35fd5805cfa67b29d17f96cd9172b345c218be0d2689b.jpg)

![](images/8f35170c2856db4f7c38d4e20ef222d33313c5b64e6797ba6b85bedf5dc30dfd.jpg)  
(e)X 方向飞行速度随时间变化曲线

![](images/90733256b46f1415f48ce9d262d9ffc7e635772c1c2614d057044da6ec574f91.jpg)  
(f)Y 方向速度随时间变化曲线

![](images/251c24d5e9fb18642939456ab60eb90fb088e1c15fe91467bf21cc0a049e12f9.jpg)  
图6.16 五机编队飞行试验结果

由图 6.16 可知，1 号机于第 699 秒起飞后进入预定航线自主飞行，航线高度  $60\mathrm{m}$ （海拔  $701\mathrm{m}$ ）。2、3、4、5 号机依次于第 753、800、903、990 秒起飞进入各自设定的航线飞行，其航线高度分别保持在  $55\mathrm{m}$ （海拔  $696\mathrm{m}$ ）、 $60\mathrm{m}$ （海拔  $701\mathrm{m}$ ）、 $65\mathrm{m}$ （海拔  $706\mathrm{m}$ ）和  $70\mathrm{m}$ （海拔  $711\mathrm{m}$ ）。4 号机在第 928 秒加入编队，5 号机在第 1048 秒加入编队，2 号机在第 1176 秒加入编队，3 号机在第 1180 秒加入编队，五架无人机组成预定编队队形飞行约 85 秒后，按照 2、3、5、4 号机的顺序依次退出编队。基于本章所提出的五机编队队形，在五机编队飞行试验过程中对纵向和侧向距离进行了调整，其中 1 号机在最前方，2、3 号机紧跟 1 号机分别位于其左后方和右后方，纵向和侧向均保持  $10\mathrm{m}$  距离，4 号机跟随 2 号机并位于其正后方  $10\mathrm{m}$  处，5 号机跟随 3 号机并位于其正后方  $10\mathrm{m}$  处，试验中五架无人机的实际编队效果如图 6.15 所示。五机编队飞行时的队形保持位置偏差如图 6.17 所示。

![](images/8392b877f440924cfdc1d96f8f366ead5602e8ccb5104a61f334fbaf01192804.jpg)  
图 6.17 五机编队飞行试验时的编队位置误差

由上图可知，在1号机进行转弯时，2、3、4、5号机队形保持的位置偏差最大，而1号机沿直线段飞行时，位置偏差较小。为了对比转弯段和直线段飞行的队形保持效果，取  $1190\sim 1209$  秒转弯段位置偏差数据和  $1230\sim 1241$  秒直线段位置偏差数据进行分析，如表6.2所示。

表6.2编队队形保持的位置控制精度  

<table><tr><td rowspan="2">无人机编号</td><td colspan="2">转弯段位置偏差（m）</td><td colspan="2">直线段位置偏差（m）</td></tr><tr><td>纵向</td><td>侧向</td><td>纵向</td><td>侧向</td></tr><tr><td>2</td><td>5.8</td><td>4.9</td><td>1.5</td><td>0.7</td></tr><tr><td>3</td><td>6.4</td><td>6.0</td><td>2.8</td><td>2.6</td></tr><tr><td>4</td><td>5.2</td><td>4.4</td><td>2.0</td><td>1.7</td></tr><tr><td>5</td><td>4.3</td><td>4.1</td><td>1.8</td><td>1.7</td></tr></table>

根据上表统计的直线段和转弯段位置偏差数据，可以更直接地反映由于1号机进行转弯使得2、3、4、5号机的位置偏差比较大，3号机纵向和侧向偏差均为最大值，分别达到了  $6.4\mathrm{m}$  和  $6.0\mathrm{m}$  。直线段的位置偏差明显减小，3号机纵向和侧向偏差均为最大值，分别为  $2.8\mathrm{m}$  和  $2.6\mathrm{m}$  ，而2号机的纵向和侧向偏差均为最小值，分别为  $1.5\mathrm{m}$  和  $0.7\mathrm{m}$  。以上数据统计的结果只针对于所选取时间范围内的位置偏差数据，选择的数据范围不同，结果会存在细微差异。根据对飞行试验数据的分析可以得出，本章建立的五机编队飞行控制系统按照预定任务航线飞行时的队形保持效果较好，所实现的位置控制精度满足实际任务需要。为了进一步减小转弯飞行阶段的编队位置偏差，可以从优化控制算法、降低通信延时等方面进行改进。

# 6.5 小结

本章以无人机低空精确打击地面目标任务为背景，提出了一种无人机编队队形及其通信拓扑结构，研究了微小型无人机编队队形保持控制技术，提出了无人机纵向和横向跟踪策略，建立了自主编队飞行试验系统，开展了飞行试验，并对飞行试验数据进行了分析。主要研究工作及结论如下：

（1）提出了一种典型的多无人机机编队飞行框架，基于一致性理论，建立了多无人机动力学模型，设计了多无人机编队控制器，实现了在Matlab环境下的飞行仿真，仿真试验数据表明该系统能够收敛并保持期望的队形；

（2）分析了无人机的动力学模型，将无人机编队队形保持问题视为目标跟踪问题，将无人机的运动划分为纵向和横向两部分，并分别基于L1制导律和总能量控制方法提出了无人机的横向和纵向跟踪策略；

（3）搭建了基于Pixhawk飞行控制器的微小型固定翼无人机编队飞行系统，开展了三机和五机自主编队飞行试验，并具体分析了由Pixhawk飞行控制器采集

的飞行试验数据，记录的视频显示无人机按照预定队形较好地完成了飞行任务，实现了较高的队形保持精度，整个队形变换过程比较稳定，飞行试验结果证明了该编队飞行控制方法的可行性。

# 第七章 结论与展望

# 7.1 论文主要研究成果

1. 阐述了微小型无人机的特点以及面临的主要问题，介绍了目前主流的目标跟踪方法，总结了机载视觉导引系统的应用现状，介绍了基于深度学习的目标检测方法，分析了低成本飞行控制器的发展和应用情况，总结了无人机编队飞行系统研究概况，总结了相关内容的国内外研究现状并给出论文研究的主要思路。

2. 以无人机低空精确打击地面目标任务为背景，针对微小型无人机目标跟踪任务的特点，提出了微小型无人机目标跟踪与飞行控制系统的基本框架，设计了基于多无人机协同的低空精确打击任务流程以及各子系统之间的协调关系。描述了摄像机成像原理，分析了摄像机畸变模型、内参数和外参数模型，介绍了主要图像传感器的工作原理，完成了低成本摄像机的选型和标定，得到了摄像机的内参数模型和畸变系数。分析了DSP、FPGA、ARM等主流的嵌入式图像处理系统架构，建立了基于ARM/Linux的嵌入式图像处理系统框架，对NVIDIA Jetson系列AI计算平台进行对比分析，最终选择NVIDIA Jetson TX2和Pixhawk4作为本文的机载计算机和飞行控制器，并建立了机载视觉目标跟踪与飞行控制系统硬件和软件框架，搭建了一套微小型多旋翼无人机飞行平台。基于Gazebo可视化仿真平台，对本文在机载计算机以及飞行控制器中建立的目标跟踪、飞行控制等主要算法进行测试，通过无人机穿越圆环算例验证了该方法的可行性。

3. 针对车载式微小型无人机自主降落问题，分析了ArUco二维码的检测和识别原理，基于嵌套二维码设计了一种多层级图案的合作降落板标志，在地面测试了该标志在不同距离处的检测效果。测试结果表明，该降落板标志相比单个二维码标志在近距离下的检测成功率更高，并且拓展了其可检测范围。分析了共面P4P问题的求解过程，对多个二维码标志测量数据进行融合，提出了无人机与该降落板的相对位姿估计方法，并在机载计算机中部署该算法，基于pixhawk控制器搭建了车载式微小型多旋翼无人机自主降落飞行控制系统，并通过室内环境下的飞行试验证明了该降落系统的可行性，同时也证明了该位姿估计方法得到的估计值较单个二维码标志的测量值更接近于真实值。分析了四旋翼无人机的布局，建立了螺旋桨拉力和力矩的理论计算模型，与实验数据进行对比，在常用的  $2000\sim 8000$  转速区间，理论计算得到的螺旋桨拉力值和力矩值的最大相对误差分别为  $11.5\%$  和  $5.4\%$  。对无人机动力系统进行建模，分析了从遥控器输出指令信号到螺旋桨产生拉力与力矩的过程。在Matlab/Simulink中建立了四旋翼无人机飞行控制系统框架，设计了位置控制器和姿态控制器，搭建了六自由度动力学仿真模型。针对末制导

阶段的视角和落角约束问题，对经典制导方法的不足进行了分析，提出了一种带视角和落角约束的偏置比例导引律，并与同类末制导方法进行了对比分析，利用四旋翼动力学模型进行仿真试验。仿真结果表明，该方法对视角的控制满足要求，终端落角相比同类方法更接近于期望落角，验证了该方法的实用性和高效性。

4. 针对非合作目标的长时间跟踪问题，对非合作目标跟踪方法进行研究，深入分析了核相关滤波器（KCF）采用的岭回归、循环矩阵、核方法以及傅立叶变换等计算原理，采用公开数据集对不同跟踪算法的性能指标进行了对比分析。针对核相关滤波器不能对目标尺度进行预测的问题，提出了多尺度的核相关滤波器（MSKCF），建立了目标尺度池，通过求解最大响应值来确定最佳尺度，从而实现对目标位置和尺度的连续预测。将KCF和MSKCF在公开数据集视频上进行测试，结果表明MSKCF在对运动目标跟踪时所输出的目标框随目标形状变化而相应地调整尺度，与KCF相比，MSKCF的预测结果与真实目标轮廓更加吻合，跟踪性能更优。研究了基于深度学习的目标检测算法，对YOLO系列算法的网络结构进行了深入分析，将卷积注意力机制引入到YOLOv4网络中进行改进，增强了网络对重要特征的关注和学习，提出了YOLOv4-CBAM算法，利用特定目标的训练样本集对YOLOv4-CBAM和YOLOv4两种算法进行训练，并对船只等特定目标进行检测对比。试验数据证明了YOLOv4-CBAM在检测精度上相比原YOLOv4算法有明显提升，并且降低了原算法对于复杂环境中的目标以及多尺度目标的漏检率，具有更优的性能。分析了长时间目标跟踪任务中传统跟踪方法存在的不足之处，提出了一种基于YOLO-MSKCF的非合作目标并行检测与跟踪算法框架，利用YOLO的目标检测结果来实现跟踪器的快速初始化，在长时间目标跟踪时，将跟踪器运行速度快和检测器精度高的特点相结合，采用隔帧检测方式将检测器的检测结果用于校正跟踪器，使得跟踪器输出的目标框更接近于真实目标，通过对静态和动态目标的检测试验，验证了该方法的可行性和高效性。

5. 针对运动目标自主跟踪、目标定位等问题，分析了机载视觉导引稳定平台的结构形式和运动学关系，设计了平台的稳定与跟踪控制系统，搭建了微小型无人机目标跟踪飞行试验系统，对行人、四旋翼无人机等运动目标分别进行了空中和地面的跟踪测试，验证了该系统的可行性。将真比例导引律和纯追踪法相结合，将目标尺度的预测值引入无人机跟踪控制系统中进行解算，提出了运动目标自主跟踪策略，开展了针对空中运动目标的全自主跟踪飞行试验，通过试验数据和所记录的飞行视频可以验证该系统的有效性和可行性。根据无人机低空精确打击目标任务的要求，针对实际任务中的地面目标定位问题，提出了基于机载视觉导引稳定平台的多点测向与测距目标定位方法，自主搭建了一套微小型视觉导引稳定平台实物演示系统，通过仿真试验和阶段性的实物演示试验对该方法进行验证，

仿真结果证明了该方法的可行性。在户外环境下，开展了对地面静止目标的实物演示试验，得到了目标定位数据，对工程应用中的定位误差产生原因进行了分析，并提出了提高该实物演示系统定位精度的措施。

6. 以无人机低空精确打击地面目标任务为背景，针对微小型无人机编队飞行控制问题，提出了一种典型的五架无人机编队队形及其通信拓扑结构，设计了编队协同控制律，在Matlab环境下进行了飞行仿真试验，仿真结果说明该方法能够实现收敛并让五架无人机保持期望的队形。建立了固定翼无人机质心运动模型，将无人机编队队形保持问题视为目标跟踪问题，并分别基于L1制导律和总能量控制方法设计了无人机横向和纵向跟踪策略。基于Pixhawk控制器搭建了微小型固定翼无人机编队飞行试验系统，开展了三机和五机自主编队飞行试验。飞行试验结果验证了该系统的可行性，无人机在转弯段的编队位置偏差为6m左右，直线段飞行时的编队位置偏差较小，为2m左右，地面和空中拍摄记录下的编队队形变换过程非常稳定。

# 7.2 论文主要创新点

本文的创新性工作主要体现在以下几个方面：

1. 提出了一种基于多层级嵌套标志的合作目标跟踪方法和考虑视角与落角约束的微小型无人机自主降落偏置比例导引律，解决了传统人工视觉标志存在距离盲区、缺乏方向性、特征信息少等问题，在传感器精度受限的条件下提高了自主降落精度。针对微小型无人机在实际任务中的引导回收问题，首先把微小型无人机的自主降落过程分为末制导阶段、跟踪阶段和会合阶段，设计了车载式微小型多旋翼无人机的自主降落系统框架，提出了一种将传统图案与ArUco二维码相结合的合作降落板目标设计思路，通过大、小二维码标志的特殊嵌套方式构建了多层级的内部特征，解决了传统图案在自主降落任务中存在距离盲区、方向性不足、不能被唯一识别等问题，提高了微小型无人机的落点精度。针对微小型无人机与该降落板目标的相对位姿估计问题，对多尺度的共面二维码标志进行实时检测，提出了一种基于多二维码数据融合的位姿估计方法，该方法相比传统的二维码标志位姿估计方法在估计精度上有明显的提升，通过开展微小型无人机自主降落飞行试验，验证了该方法的可行性和高效性。针对末制导阶段的微小型无人机视角和落角约束问题，提出了一种适用于微小型多旋翼无人机自主降落任务的偏置比例导引律，建立了多旋翼无人机动力学模型，并在Matlab环境下针对地面非机动运动平台和机动运动平台两种情况进行模拟仿真，将该方法和同类型制导律进行对比，结果表明该方法满足视角约束，并且其计算得到的无人机终端落角最接近于-90期望落角。

2. 提出了一种基于深度学习的非合作目标并行检测与跟踪方法，解决了传统目标跟踪方法漂移大、故障率高等问题，实现了对目标位置和尺度的实时估计，显著提高了目标预测框与真实目标的重合率。根据长时间目标跟踪任务的要求，分析了目标在跟踪过程中存在形变的情况，借鉴了深度学习的思想，把检测器与跟踪器同时引入目标跟踪任务，提出了一种基于YOLO-MSKCF的非合作目标并行检测与跟踪方法，采用隔帧检测方式将检测器的检测结果用于对跟踪器的初始化和校正，既发挥了检测器高精度和低漂移的优势，又利用跟踪器计算速度快和连续预测的特性，解决了传统跟踪方法在长时间跟踪任务中存在漂移大、故障率高等问题，在有限的计算资源条件下，对跟踪性能进行了有效提升，并且对目标尺度进行实时预测。基于机载计算机对该方法进行了实现，并针对静止和运动目标两种情况进行了跟踪试验，结果显示该方法预测的目标框与实际的目标轮廓非常吻合，跟踪结果优于KCF等传统跟踪方法，从而验证了该方法的可行性。

3. 提出了一种基于机载视觉导引稳定平台的运动目标自主跟踪方法和基于多点测向与测距的目标定位方法，解决了利用视觉信息进行目标参数估算、制导控制解算等问题，实现了对空中机动目标的自动搜索和跟踪。对比分析了机载视觉导引稳定平台与捷联式导引头的特性，针对运动目标自主跟踪任务，建立了三轴稳定平台的运动学模型，在摄像机内置磁罗盘数据出现较大偏差的情况下，将稳定平台的框架角和无人机的姿态角用于解算摄像机光轴在惯性空间中的指向，从而实现平台的稳定与跟踪控制。搭建了一套基于非合作目标跟踪算法的机载视觉导引稳定平台试验系统，根据视觉导引稳定平台解算得到的目标视线角以及目标尺度等信息，提出了针对运动目标的微小型无人机跟踪策略，利用多旋翼无人机的横向运动特性，采用纯追踪法对其横向运动进行控制，可以将无人机的机头快速指向目标。基于真比例导引律对无人机的纵向运动进行控制，将目标跟踪方法得到的目标尺度预测值引入制导控制指令解算过程，在跟随目标运动的过程中，通过控制无人机的接近速度使得目标在图像中的大小保持不变。通过对地面和空中运动目标的自主跟踪飞行试验，验证了该方法的可行性。针对单目视觉难以获得目标距离信息的问题，本文提出了一种基于无人机多点测向与测距的目标定位方法，将激光测距仪与摄像机安装在稳定平台的内框上，锁定目标后，对目标进行测向的同时得到摄像机与目标的相对距离，实现了对目标地理位置的解算，在跟踪过程中通过多点测量数据融合，对目标定位进行估计，自主搭建了一套微小型视觉导引稳定平台实物演示系统，通过仿真试验和阶段性的实物演示试验验证了该方法的可行性。

4. 提出了一种基于目标跟踪策略的微小型无人机编队队形保持控制方法，解决了任务航线自主飞行阶段的编队纵向和横向位置控制，基于低成本飞行控制系

统实现了较高的编队队形保持精度。根据无人机低空精确打击地面目标任务的要求，提出了一种可扩展的无人机编队队形及其通信拓扑结构，将微小型无人机编队队形保持问题视为目标跟踪问题，分别对微小型无人机的纵向和横向跟踪控制方法进行了深入研究，设计了基于L1制导律的无人机横向位置控制律，可根据领航无人机与跟随无人机之间的侧向位置偏差解算跟随者的横向加速度指令。根据领航无人机与跟随无人机之间的纵向位置偏差和速度差，设计了跟随者无人机期望速度的表达式，基于总能量控制理论，提出了无人机的纵向位置控制律。建立了微小型无人机编队飞行试验系统，开展了三机和五机编队飞行试验，对编队队形切换、编队队形保持以及编队转弯过程进行了测试，试验结果验证了该编队飞行控制方法的可行性，试验数据显示该编队飞行系统具有较高的稳定性和可靠性，队形保持精度满足实际任务需要，具有一定的工程应用价值。

# 7.3 下一步工作展望

本文取得了一定进展，但还存在尚待完善的部分，所提出的算法有一定的工程实际应用价值，但在精度和效率方面还需要进一步提升。在后续的研究工作中，需要进一步展开深入研究的方向主要包括：

1. 有待进一步研究复杂环境中的车载式无人机自主降落导引问题。

目前，车载式无人机系统诸多领域有广泛的应用价值，极大提升了无人机作业效率。但车辆在行驶中的实际路面状况复杂多样，存在车辆快速运动、地面坡度大、障碍物遮挡、通信受干扰等情况。在上述环境下，如何实现无人机的精准起降，仍需进一步探索。低成本摄像头所采集的实时图像存在模糊、噪声大、延迟高、易受光照影响等缺点，如何从算法层面对成像品质进行优化，仍需要进一步研究。捷联式导引头直接与无人机固定安装，没有稳定平台来隔离无人机姿态运动，其直接测量得到的是无人机机体坐标系下的体视线角信息，为了得到惯性系下的视线信息，还需要设计复杂的解耦滤波算法，融合体视线角信息和惯导数据，从而提取制导系统所需的惯性制导信息，这个过程还需要进一步研究。

2. 仍需进一步研究更加高效、快速和精确的目标检测算法

目前深度学习算法对机载计算机的性能要求较高，许多算法部署后运行速度较低，无法满足实时跟踪任务的需要，因此需进一步优化算法框架，提升检测效果的同时，减少计算时间，提高计算效率。YOLO算法在检测准确度方面还有待提高，特别是针对小目标的检测效果较差，在未来的工作中还需要进一步优化其算法的网络框架，提高检测精度。

# 3. 无人机分布式集群控制技术有待进一步研究

自主化、智能化、集群化将是无人机发展的趋势，而分布式的框架更适合大

规模的无人机集群控制，是未来的重要研究领域。本文并没有考虑通信网络存在时间延迟的问题，为了更贴近实际应用的情况，未来应该研究各成员无人机之间存在非统一系统时延的编队控制算法，并且应该进一步考虑当编队通信拓扑结构发生变化时的编队系统保持与编队重构问题。此外，应该进一步研究大规模无人机编队集群的近距离防碰撞技术，考虑如何对即将发生的碰撞进行预测，并提前控制相关成员以使其避免相撞，从而降低无人机集群在飞行过程中发生碰撞的概率。本文所开展的编队飞行试验中，转弯时的编队位置误差较大，需要进一步研究无人机机动飞行时的队形保持控制方法。

4. 复杂战场环境下多目标搜索与跟踪技术的进一步研究

无人机集群形成规模优势，在战场中通过群体感知能够获得更广阔的侦察范围。本文将无人机目标跟踪系统和编队飞行控制系统分为两个部分进行研究，只对单一无人机跟踪单个目标的跟踪问题进行分析，缺乏对复杂战场环境下的多目标搜索与跟踪问题的进一步深入探索。在未来的研究工作中，应以多无人机协同目标跟踪任务为应用背景，充分考虑地理环境复杂、目标不确定、目标机动、通信能力有限等问题，对多无人机协同搜索路径进行优化，建立多无人机协同目标跟踪的任务管理机制。

# 参考文献

[1] Yanushevsky R. Guidance of Unmanned Aerial Vehicles (无人机制导)[M]. 牛轶峰等译. 北京: 国防工业出版社, 2015. [2] Valavanis K P. Advances in Unmanned Aerial Vehicles[M]. The Netherlands: Springer, 2007. [3] 徐忠国. 未来空战利器——无人机[J]. 中国人民防空. 2020, (2): 71- 71. [4] Royal Flying Corps in the First World War - Imperial War Museum. May 2020. https://www.iwm.org.uk/collections/item/object/205315299. [5] 付强, 等. 精确制导武器技术应用向导[M]. 北京: 国防工业出版社, 2014. [6] 苗昊春, 等. 智能化弹药[M]. 北京: 国防工业出版社, 2014. [7] 袁健全, 等. 飞航导弹[M]. 北京: 国防工业出版社, 2013. [8] 张明亮, 赵全习, 王还乡. 美国无人战斗机的发展现状及趋势[J]. 飞航导弹, 2011, (9): 25- 28. [9] 王保存. 世界新军事变革新论[M]. 北京: 中国人民解放军出版社, 2003. [10] 熊光楷. 论世界新军事变革趋势和中国新军事变革[J]. 外交学院学报. 2004, (2): 8- 16. [11] 金一南. 影响深远的世界新军事变革[J]. 求是. 2006, (16): 59- 61. [12] 王桂芝, 沈卫. 国外战术无人机系统与技术发展分析[J]. 飞航导弹. 2020, (9): 71- 74. [13] 秦明, 朱会, 李国强. 军用无人机的发展趋势[J]. 飞航导弹. 2007, (6): 36- 38. [14] Judy J W. Microelectromechanical systems (MEMS): Fabrication, design and applications[J]. Smart Materials and Structures. 2001, 10(6): 1115- 1134. [15] Li X, Ge M, Dai X, et al. Accuracy and reliability of multi- GNSS real- time precise positioning: GPS, GLONASS, BeiDou, and Galileo[J]. Journal of Geodesy. 2015, 89(6): 607- 635. [16] Rocca R. Photogrammetry with a Drone “DJI Phantom 2 Vision Plus”: 3D Model of an Area Deformed by Neotectonics in the Venezuelan Andes[J]. Earth Sciences. 2017, 6(5): 63- 68. [17] Poterek Q, et al. Deep Learning for Automatic Colorization of Legacy Grayscale Aerial Photographs[J]. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. 2020, 13: 2899- 2915. [18] Nex F, Remondino F. UAV for 3D mapping applications: a review[J]. Applied Geomatics, 2014, 6(1): 1- 15. [19] Ezequiel C A F, Cua M, Libatique N C, et al. UAV Aerial Imaging

Applications for Post- Disaster Assessment, Environmental Management and Infrastructure Development[C]. International Conference on Unmanned Aircraft Systems. Orlando, USA, 27- 30 May 2014: 274- 283. [20] Silvagni M, Tonoli A, Zenerino E, et al. Multipurpose UAV for search and rescue operations in mountain avalanche events[J]. Geomatics Natural Hazards and Risk. 2017, 8(1): 18- 33. [21] Bejiga M B, Zeggada A, Nouffidj A, Melgani F. A Convolutional Neural Network Approach for Assisting Avalanche Search and Rescue Operations with UAV Imagery[J]. Remote Sensing. 2017; 9(2): 100. [22] Laliberte A S, Jeffrey E H, et al. Acquisition, Orthorectification, and Object- based Classification of Unmanned Aerial Vehicle (UAV) Imagery for Rangeland Monitoring[J]. Photogrammetric Engineering and Remote Sensing. 2010, 76(6): 661- 672. [23] Li Z, Liu Y, Walker R, et al. Towards automatic power line detection for a UAV surveillance system using pulse coupled neural filter and an improved Hough transform[J]. Machine Vision and Applications. 2010, 21(5): 677- 686. [24] Chow, J Y J. Dynamic UAV- based traffic monitoring under uncertainty as a stochastic arc- inventory routing policy[J]. International Journal of Transportation Science and Technology, 2016, 5(3): 167- 185. [25] Sawadsitang S, Niyato D, Tan P S, et al. Joint Ground and Aerial Package Delivery Services: A Stochastic Optimization Approach[J]. IEEE Transactions on Intelligent Transportation Systems. 2019, 20(6): 2241- 2254. [26] 赵南. 无人机系统在森林防火方面的应用及其发展[J]. 农业与技术. 2019, 39(1): 110- 111. [27] Russell S J, et al. Artificial Intelligence: A Modern Approach (人工智能——一种现代方法)[M]. 北京: 人民邮电出版社, 2002. [28] Jain R, Kasturi R, Schunck B G. Machine Vision[M]. New York: McGraw- Hill, 1995. [29] 刘浩敏, 等. 基于单目视觉的同时定位与地图构建方法综述[J]. 计算机辅助设计与图形学学报. 2016. 28(6): 855- 868. [30] Saska M, Vonášek V, Chudoba J, et al. Swarm Distribution and Deployment for Cooperative Surveillance by Micro- Aerial Vehicles[J]. Journal of Intelligent & Robotic Systems. 2016, 84(1- 4): 469- 492. [31] Dong Z, Zhang R, et al. Study on UAV Path Planning Approach Based on Fuzzy Virtual Force[J]. Chinese Journal of Aeronautics. 2010, 23(3): 341- 350. [32] 牛轶峰, 沈林成, 戴斌, 徐昕, 相晓嘉. 无人作战系统发展[J]. 国防科技. 2009, 30(5): 1- 11. [33] 胡高辰. 美国击杀苏莱曼尼事件后的伊朗核问题形势[J]. 世界知识. 2020,

(4): 38- 39.

[34] 兰顺正. 美军对非核心地区国家行动模式向"轻量级"转变[J]. 世界知识. 2020, (3): 34- 35.

[35] 白云湖. 箭指首领——解读美军刺杀苏莱曼尼行动[J]. 轻兵器. 2020, (3): 62- 68.

[36] 鲁亚飞, 陈清阳, 吴岸平. 中空长航时察打一体无人机运用特点分析[J]. 飞航导弹. 2020, (9): 75- 79.

[37] 符成山, 吴雅诚, 雷东. 美军无人机装备现状及发展趋势[J]. 飞航导弹. 2019, (9): 46- 52.

[38] 魏祥灰, 唐超颖, 王彪. 基于空间两点的视觉自主着陆导引算法设计[J]. 北京航空航天大学学报. 2019, 45(2): 357- 365.

[39] 蒋鸿翔. 无人直升机视觉导引着陆研究[D]. 南京: 南京航空航天大学, 2008.

[40] Gui Y, Guo P, Zhang H, et al. Airborne Vision- Based Navigation Method for UAV Accuracy Landing Using Infrared Lamps[J]. Journal of Intelligent & Robotic Systems. 2013, 72(2): 197- 218.

[41] 熊丹. 基于视觉的微小型无人机地面运动目标跟踪[D]. 长沙: 国防科技大学, 2017.

[42] 王晓光, 章卫国, 刘洋. 伴飞诱饵干扰下的自杀式无人机攻击策略[J]. 航空学报. 2015, 36(9): 3137- 3146.

[43] 黄长强. 未来空战过程智能化关键技术研究[J]. 航空兵器. 2019, 26(1): 15- 23.

[44] 崔秀敏, 王维军, 方振平. 小型无人机发展现状及其相关问题分析[J]. 飞行力学. 2005, 23(1): 14- 18.

[45] 刘辉邦, 褚金奎, 支炜, 李晓雨. 基于STM32的无人机姿态测量系统设计[J]. 传感器与微系统. 2013, 32(8): 108- 110.

[46] 刘广斌, 罗卫兵, 严斌亨. 美军微小型无人机及关键技术探析[J]. 飞航导弹. 2016, (5): 43- 47.

[47] 葛明锋. 基于轻小型无人机的高光谱成像系统研究[D]. 上海: 中国科学院上海技术物理研究所, 2015.

[48] 陈世适, 姜臻, 等. 微小型飞行器发展现状及关键技术浅析[J]. 无人系统技术. 2018, 1(1): 38- 53.

[49] Percin M, Oudheusden B W V, et al. Force generation and wing deformation characteristics of a flapping- wing micro air vehicle 'DelFly IF' in hovering flight[J]. Bioinspiration & Biometrics, 2016, 11(3): 036014.

[50] MetaFly. May 2020. https://bionicbird.com/world/shop/metafly.

[51] Breuer K. Flight of the RoboBee[J]. Nature. 2019, 570(7762): 448- 449. [52] Ramezani A, Chung S J, Hutchinson S. A biomimetic robotic platform to study flight specializations of bats[J]. Science Robotics. 2017, 2(3): eaal2505. [53] PD- 100 Black Hornet Nano Unmanned Air Vehicle. September 2020. https://www.army- technology.com/projects/pd100- black- hornet- name/.[54] Kroo I, Prinz F, et al. The Mesicopter: A Miniature Rotorcraft Concept Phase II Interim Report[Z]. USA, Stanford University, 2000. [55] Aeryon Scout - Wikipedia. September 2020. https://en.jinzhao.wiki/wiki/Aeryon_Scout.[56] Elbit Systems Introduces MAGNI, a Vehicle- Launched Multi- Rotor Micro- Drone. October 2020. https://elbitsystems.com/pr- new/elbit- systems- introduces- magni- a- vehicle- launched- multi- rotor- micro- drone/.[57] Goodnight R D. An Innovative Approach for Data Collection and Handling to Enavle Advancements in Micro Air Vehicle Persistent Surveillance[D]. College Station: Texas A&M University, 2009. [58] 李煜, 陆强, 白丕绩. 非制冷红外成像系统在陆军装备中的应用现状及趋势[J]. 红外技术. 2017, 39(7): 581- 593. [59] SenseFly. May 2020. https://www.sensefly.com/drones/compare- drones/.[60] Lehmann Aviation. September 2021. https://lehmannaviation.com/.[61] Pandit V, Poojari A. A Study on Amazon Prime Air for Feasibility and Profitability —A Graphical Data Analysis[J]. IOSR Journal of Business and Management. 2014, 16(11): 6- 11. [62] 京东X事业部. May 2020. https://x.jdwl.com/drone/index.[63] 顺丰科技. May 2020. https://www.sf- tech.com.cn/product/uav.[64] 复亚智能. May 2020. http://www.foiadrone.com/.[65] Goldfarb N, et al. Open source Vicon Toolkit for motion capture and Gait Analysis[J]. Computer Methods and Programs in Biomedicine. 2021, 212: 106414. [66] Liu B, Paquin N. Viconmavlink: A software tool for indoor positioning using a motion capture system[Z]. ArXiv, 2018. [67] Ritz R, D'Andrea R. Carrying a flexible payload with multiple flying vehicles[C]. IEEE/RSJ International Conference on Intelligent Robots and Systems. Tokyo, Japan, 3- 7 November 2013: 3465- 3471. [68] Mohamed A, Yang C, Cangelosi A. Stereo Vision based Object Tracking Control for a Movable Robot Head[J]. IFAC- PapersOnline. 2016, 49(5): 155- 162. [69] Intel RealSense T265. September 2020. https://www.intelrealsense.com/zh- hans/tracking- camera- t265/#Products.[70] Shen S, Michael N, Kumar V. Autonomous multi- floor indoor navigation with a computationally constrained MAV[C]. IEEE International Conference on Robotics

and Automation. Shanghai, China, 9- 13 May 2011: 20- 25. [71] Carrillo L R G, López A E D, Lozano R, et al. Combining Stereo Vision and Inertial Navigation System for a Quad- Rotor UAV[J]. Journal of Intelligent & Robotic Systems, 2012, 65(1- 4): 373- 387. [72] 柏席峰. 美国大兵手中又一把小利刃——“弹簧刀”巡飞弹[J]. 兵器知识. 2012, (8): 51- 54. [73] 唐明军. 亚阿冲突双方武库中都有什么装备[J]. 太空探索. 2020, (11): 58- 61. [74] 苏润丛, 向文豪, 缪国春, 王栋. 纳卡冲突中无人机的作战应用与分析[J]. 飞航导弹. 2021, (1): 65- 70. [75] 河豚A2. January 2021. http://www.ziyanuav.com/PufferfishA2/index.aspx.[76] 徐晨华. 美国MQ- 9无人机的新发展与技术性能[J]. 飞航导弹. 2018, (7): 48- 53. [77] BENO V, ADAMCIK F. Unmanned Combat Air Vehicle: MQ- 9 Reaper[C]. International Conference of Scientific Paper - AFASES. Brasov, 22- 24 May 2014. [78] How Qassem soleimanni targeted 230mph laser guided hellfire mission fired drone. January 2021. https://www.dailymail.co.uk.uk/news/article- 7850453. [79] Duray J D. Remotely piloted aircraft: implications for future warfare[J]. Air Force Magazine. 2020, 103(1).[80] 王晖娟, 魏国福. 美海军X- 47B无人机首上航母测试[J]. 飞航导弹. 2013, (1): 9. [81] 孔维玮. 基于多传感器的无人机自主着舰引导与控制系统研究[D]. 长沙: 国防科技大学, 2017. [82] 高文, 朱明, 贺柏根, 吴笑天. 目标跟踪技术综述[J]. 中国光学. 2014, 7(3): 365- 375. [83] Patruno C, et al. Helipad Detection for Accurate UAV Pose Estimation by Means of a Visual Sensor[J]. International Journal of Advanced Robotic Systems. 2017, 14(5): 1- 15. [84] Saripalli S, Montgomery J F, Sukhatme G S. Visually- Guided Landing of an Unmanned Aerial Vehicle[J]. IEEE Transactions on Robotics and Automation. 2003, 19(3): 371- 380. [85] Baca T, Stepan P, Saska M. Autonomous landing on a moving car with unmanned aerial vehicle[C]. IEEE European Conference on Mobile Robots. Paris, France, 6- 8 September 2017. [86] 朱建明. 无人驾驶直升机自主降落的算法研究与实现[D]. 成都: 电子科技大学, 2009. [87] Jung Y, Lee D, Bang H. Study on Ellipse Fitting Problem for Vision- based

Autonomous Landing of an UAV[C]. International Conference on Control, Automation and Systems. Seoul, Korea, 22- 25 October 2014: 1631- 1634.

[88] Yang S, Scherer S A, Zell A. An Onboard Monocular Vision System for Autonomous Takeoff, Hovering and Landing of a Micro Aerial Vehicle[J]. Journal of Intelligent & Robotic Systems. 2013, 69(1- 4): 499- 515.

[89] Lange S, Sunderhauf N, Protzel P. A Vision Based Onboard Approach for Landing and Position Control of an Autonomous Multirotor UAV in GPS- Denied Environments[C]. IEEE International Conference on Advanced Robotics. Munich, Germany, 22- 26 June 2009.

[90] Chen X, Phang S K, Shan M, et al. System Integration of a Vision- Guided UAV for Autonomous Landing on Moving Platform[C]. IEEE International Conference on Control and Automation. Kathmandu, Nepal, 1- 3 June 2016: 761- 766.

[91] QR code - Wikipedia. May 2020. https://en.jinzhao.wiki/wiki/QR_code.

[92] Rekimoto J. Matrix: A Realtime Object Identification and Registration Method for Augmented Reality[C]. 3rd Asia Pacific Computer Human Interaction. Shonan Village Center, Japan, 15- 17 July 1998: 63- 68.

[93] Fiala M. ARTag, a fiducial marker system using digital techniques[C]. IEEE Conference on Computer Vision and Pattern Recognition. San Diego, USA, 20- 25 June 2005: 590- 596.

[94] Khan D, et al. Robust Tracking Through the Design of High Quality Fiducial Markers: An Optimization Tool for ARToolKit[J]. IEEE Access. 2018, 6: 22421- 22433.

[95] Wagner D, Schmalstieg D. ARToolKitPlus for Pose Tracking on Mobile Devices[C]. Proceedings of 12th Computer Vision Winter Workshop. St. Lambrecht, Australia, 6- 8 February 2007: 139- 146.

[96] Olson E. AprilTag: A robust and flexible visual fiducial system[C]. IEEE International Conference on Robotics and Automation. Shanghai, China, 9- 13 May 2011: 3400- 3407.

[97] Garrido- Jurado S, et al. Automatic generation and detection of highly reliable fiducial markers under occlusion[J]. Pattern Recognition. 2014, 47(6): 2280- 2292.

[98] Borowczyk A, et al. Autonomous Landing of a Multirotor Micro Air Vehicle on a High Velocity Ground Vehicle[J]. IFAC- PapersOnline. 2017, 50(1): 10488- 10494.

[99] Sanchez- Lopez J L, Arellano- Quintana V, Tognon M, et al. Visual Marker based Multi- Sensor Fusion State Estimation[J]. IFAC- PapersOnline. 2017, 50(1): 16003- 16008.

[100] Wenzel K E, Rosset P, Zell A. Low- Cost Visual Tracking of a Landing Place and Hovering Flight Control with a Microcontroller[J]. Journal of Intelligent & Robotic Systems. 2010, 57(1- 4): 297- 311.

[101] 徐贵力, 倪立学, 程月华. 基于合作目标和视觉的无人飞行器全天候自

动着陆导引关键技术[J]. 航空学报. 2008, 29(2): 437- 442. [102] 桂阳. 基于机载视觉的无人机自主着舰引导关键技术研究[D]. 长沙: 国防科技大学, 2013. [103] 于起峰, 尚洋. 摄像测量学简介与展望[J]. 科技导报. 2008, 26(24): 84- 88. [104] 尹宏鹏, 陈波, 柴毅, 刘兆栋. 基于视觉的目标检测与跟踪综述[J]. 自动化学报. 2016, 42(10): 1466- 1489. [105] Comaniciu D, Meer P. Mean Shift: A Robust Approach Toward Feature Space Analysis[J]. IEEE Transactions Pattern on Analysis and Machine Intelligence. 2002, 24(5): 603- 619. [106] Comaniciu D, Ramesh V, Meer P. Kernel- Based Object Tracking[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence. 2003, 25(5): 564- 575. [107] Cheng Y. Mean shift, Mode Seeking, and Clustering[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence. 1995, 17(8): 790- 799. [108] Bradski G R. Computer Vision Face Tracking For Use in a Perceptual User Interface[C]. Fourth IEEE Workshop on Applications of Computer Vision. Princeton, USA, 19- 21 October 1998: 214- 219. [109] Levy A, Lindenbaum M. Sequential Karhunen- Loeve Basis Extraction and its Application to Images[J]. IEEE Transactions on Image Processing. 2000, 9(8): 1371- 1374. [110] Brand M. Incremental singular value decomposition of uncertain data with missing values[C]. The 7th European Conference on Computer Vision. Copenhagen, Denmark, 28- 31 May 2002: 707- 720. [111] Khan Z H, Gu I Y H. Nonlinear Dynamic Model for Visual Object Tracking on Grassmann Manifolds With Partial Occlusion Handling[J]. IEEE Transactions on Cybernetics. 2013, 43(6): 2005- 2019. [112] Mei X, Ling H. Robust Visual Tracking using L1 Minimization[C]. IEEE International Conference on Computer Vision. Kyoto, Japan, 27 September- 4 October 2009: 1662- 1674. [113] Li H, Shen C, Shi Q. Real- time Visual Tracking Using Compressive Sensing[C]. IEEE Conference on Computer Vision and Pattern Recognition. Colorado, USA, 20- 25 June 2011: 1305- 1312. [114] Zhang T, et al. Structural Sparse Tracking[J]. IEEE Conference on Computer Vision and Pattern Recognition, Boston, USA, 7- 12 June 2015: 150- 158. [115] Grabner H, Bischof H. On- line Boosting and Vision[C]. IEEE Conference on Computer Vision and Pattern Recognition. New York, USA, 17- 22 June 2006: 260- 267. [116] Hare S, Saffari A, Torr P H S. Struck: Structured Output Tracking with Kernels[C]. International Conference on Computer Vision. Barcelona, Spain, 6- 13

November 2011: 263- 270.

[117] Babenko B, Yang M, Belongie S. Robust Object Tracking with Online Multiple Instance Learning[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence. 2011, 33(8): 1619- 1632.

[118] Kalal Z, Mikolajczyk K, Matas J. Tracking- Learning- Detection[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence. 2011, 34(7): 1409- 1422.

[119] Bolme D S, Beveridge J R, et al. Visual Object Tracking using Adaptive Correlation Filters[C]. IEEE Conference on Computer Vision and Pattern Recognition. San Francisco, USA, 13- 18 June 2010: 2544- 2550.

[120] Henriques J F, Caseiro R, Martins P, et al. Exploiting the Circulant Structure of Tracking- by- Detection with Kernels[C]. The 12th European Conference on Computer Vision. Florence, Italy, 7- 13 October 2012: 702- 715.

[121] Henriques J F, Caseiro R, Martins P, Batista J. High- Speed Tracking with Kernelized Correlation Filters[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence. 2015, 37(3): 583- 596.

[122] Wu Y, Lim J, Yang M. Online Object Tracking: A Benchmark[C]. IEEE Conference on Computer Vision and Pattern Recognition. Portland, USA, 23- 28 June 2013: 2411- 2418.

[123] Li Y, Zhu J. A Scale Adaptive Kernel Correlation Filter Tracker with Feature Integration[C]. The 13th European Conference on Computer Vision. Zurich, Switzerland, 5- 12 September 2014: 254- 265.

[124] Montero A S, Lang J, Laganière R. Scalable Kernel Correlation Filter with Sparse Feature Integration[C]. 2015 IEEE International Conference on Computer Vision. Santiago, Chile, 7- 13 December 2015: 587- 594.

[125] Cortes C, Vapnik, V. Support- Vector Networks[J]. Machine Learning. 1995, 20(3): 273- 297.

[126] ImageNet - Wikipedia. May 2020. https://en.jinzhao.wiki/wiki/ImageNet.

[127] 赵永强, 饶元, 董世鹏, 等. 深度学习目标检测方法综述[J]. 中国图象图形学报. 2020, 25(4): 629- 654.

[128] Girshick R, Donahue J; Darrell T; Malik J. Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation[C]. IEEE Conference on Computer Vision and Pattern Recognition. Columbus, USA, 23- 28 June 2014: 580- 587.

[129] Girshick R, Fast R- CNN[C]. IEEE International Conference on Computer Vision. Santiago, Chile, 7- 13 December 2015: 1440- 1448.

[130] Ren S, He K, Girshick R, et al. Faster R- CNN: Towards Real- Time Object Detection with Region Proposal Networks[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence. 2017, 39(6): 1137- 1149.

[131] Redmon J, et al. You Only Look Once: Unified, Real- Time Object Detection[C]. IEEE Conference on Computer Vision and Pattern Recognition. Las Vegas, USA, 27- 30 June 2016: 779- 788. [132] Redmon J, Farhadi A. YOLO9000: Better, Faster, Stronger[C]. IEEE Conference on Computer Vision and Pattern Recognition. Honolulu, USA, 21- 26 July 2017: 6517- 6525. [133] Redmon J, Farhadi A. YOLOv3: An Incremental Improvement[Z]. ArXiv, 2018. [134] Liu W, et al. SSD: Single Shot MultiBox Detector[C]. The 14th European Conference on Computer Vision. Amsterdam, Netherlands, 11- 14 October 2016: 21- 37. [135] 张邦楚, 廖剑, 匡宇, 张敏, 周绍磊, 康宇航. 美国无人机集群作战的研究现状与发展趋势[J]. 航空兵器. 2020, 27(6): 7- 12. [136] 牛轶峰, 肖湘江, 柯冠岩. 无人机集群作战概念及关键技术分析[J]. 国防科技. 2013, 34(5): 37- 43. [137] Kalman R E. A New Approach To Linear Filtering and Prediction Problems[J]. Journal of Basic Engineering. 1960, 82D: 35- 45. [138] Bucy R S, et al. Digital Synthesis of Non- linear Filters[J]. Automatica. 1971, 7(3): 287- 298. [139] Carlson N A. Federated Filter for Fault- Tolerant Integrated Navigation Systems[C]. IEEE Position Location and Navigation Symposium. Orlando, USA, 29 November- 2 December 1988: 110- 119. [140] Julier S, et al. A New Method for the Nonlinear Transformation of Means and Covariances in Filters and Estimators[J]. IEEE Transactions on Automatic Control. 2000, 45(3): 477- 482. [141] Mahony R, Hamel T, et al. Complementary filter design on the special orthogonal group SO(3)[C]. Proceedings of the 44th IEEE Conference on Decision and Control. Seville, Spain, December 12- 15 2005: 1477- 1484. [142] Hamel T, Mahony R. Attitude estimation on SO(3) based on direct inertial measurements[C]. IEEE International Conference on Robotics and Automation. Orlando, USA, 15- 19 May 2006: 2170- 2175. [143] Mahony R, Hamel T, Pflimlin J. Nonlinear Complementary Filters on the Special Orthogonal Group[J]. IEEE Transactions on Automatic Control. 2008, 53(5): 1203- 1218. [144] Khosravian A, Trumpf J, Mahony R, Hamel T. Recursive Attitude Estimation in the Presence of Multi- rate and Multi- delay Vector Measurements[C]. 2015 American Control Conference. Chicago, USA, 1- 3 July 2015: 3199- 3205. [145] Kumar N S, Jann T. Estimation of attitudes from a low- cost miniaturized inertial platform using Kalman Filter- based sensor fusion algorithm[J]. Sadhana. 2004,

29(2): 217- 235. [146] Higgins W T. A Comparison of Complementary and Kalman Filtering[J]. IEEE Transactions on Aerospace and Electronic Systems. 1975, 11(3): 321- 325. [147] Chowdhary G, Jategaonkar R. Aerodynamic Parameter Estimation from Flight Data Applying Extended and Unscented Kalman Filter[J]. Aerospace & Technology, 2010, 14(2): 106- 117. [148] Marina H G, Pereda F J, et al. UAV Attitude Estimation Using Unscented Kalman Filter and TRIAD[J]. IEEE Transactions on Industrial Electronics, 2012, 59(11): 4465- 4474. [149] Open- source hardware - Wikipedia. May 2020. https://en.jinzhao.wiki/wiki/Open- source_hardware.[150] Open- source software - Wikipedia. May 2020. https://en.jinzhao.wiki/wiki/Open- source_software.[151] 李大伟, 杨炯. 开源飞控知多少[J]. 机器人产业. 2015, (3): 83- 93. [152] Vanjare A M, et al. Development of Paparazzi Autopilot System for UAV: A Hardware Approach[J]. International Journal for Scientific Research & Development. 2015, 3(9): 455- 461. [153] GitHub - openpilot. March 2021. https://github.com/openpilot/OpenPilot.[154] MikrokopterWiki. March 2020. http://wiki.mikrokopter.de/en/FlightCtrl.[155] MultiWii. March 2021. http://www.multiwii.com/.[156] 杨庭晖, 等. 一款"KK"板单轴飞行器[J]. 航空模型. 2012, (3): 50- 53. [157] ArduPilot Mega. March 2021. http://www.ardupilot.co.uk/.[158] Pixhawk. March 2021. https://pixhawk.org.[159] 杨小川, 等. Pixhawk开源飞控项目概述及其航空应用展望[J]. 飞航导弹. 2018, (4): 25- 32. [160] 牟勇飚. 无人机编队中的气动祸合问题研究[D]. 西安: 西北工业大学, 2006. [161] Lissaman P B S, Shollenberger C A. Formation Flight of Birds[J]. Science. 1970, 168(3934): 1003- 1005. [162] Perdix (drone) - Wikipedia. September 2021. https://en.jinzhao.wiki/wiki/Perdix_(drone).[163] 贾高伟, 王建峰. 无人机集群任务规划方法研究综述[J]. 系统工程与电子技术. 2021, 43(1): 99- 111. [164] 林倩玉. 多无人机协同编队控制算法研究[D]. 哈尔滨: 哈尔滨工业大学, 2018. [165] 秦昂, 张登成, 魏扬. 多无人机编队队形保持优化控制仿真研究[J]. 飞行力学. 2017, 35(6): 44- 48.

[166] 朱旭. 基于信息一致性的多无人机编队控制方法研究[D]. 西安: 西北工业大学, 2014.

[167] 焦林冠, 石鹏飞, 魏文领. 基于模糊PID的无人机编队控制[J]. 计算机仿真. 2015, 32(9): 66- 71.

[168] Challa V R, Ratnoo A. Analysis of UAV Kinematic Constraints for Rigid Formation Flying[C]. AIAA Guidance, Navigation, and Control Conference. San Diego, USA, 4- 8 January 2016.

[169] 嵇亮亮. 无人机的导引及协同编队飞行控制技术研究[D]. 南京: 南京航空航天大学, 2008.

[170] Seanor B, Gu Y, Napolitano M R, Campa G, Gururajan S, Rowe L. 3- Aircraft Formation Flight Experiment[C]. 14th Mediterranean Conference on Control and Automation. Ancona, Italy, 28- 30 June 2006.

[171] Gu Y, Seanor B, Campa G, et al. Design and Flight Testing Evaluation of Formation Control Laws[J]. IEEE Transactions on Control Systems Technology. 2006, 14(6): 1105- 1112.

[172] Li N H M, Liu H H T. Formation UAV Flight Control using Virtual Structure and Motion Synchronization[C]. 2008 American Control Conference. Seattle, USA, 11- 13 June 2008: 1782- 1787.

[173] 邵壮, 祝小平, 周洲, 王彦雄. 无人机编队机动飞行时的队形保持反馈控制[J]. 西北工业大学学报. 2015, 33(1): 26- 33.

[174] Kim S, Kim Y. Three Dimensional Optimum Controller for Multiple UAV Formation Flight Using Behavior- based Decentralized Approach[C]. International Conference on Control, Automation and Systems. Seoul, Korea, 17- 20 October 2007: 1387- 1392.

[175] 邱华鑫, 段海滨, 范彦铭. 基于鸽群行为机制的多无人机自主编队[J]. 控制理论与应用. 2015, 32(10): 1298- 1304.

[176] 薛瑞彬. 多无人机分布式协同编队飞行控制技术研究[D]. 北京: 北京理工大学, 2016.

[177] Ren W. Consensus Based Formation Control Strategies for Multi- vehicle Systems[C]. 2006 American Control Conference. Minneapolis, USA, 14- 16 June 2006: 4237- 4242.

[178] Olfati S R, Murray R M. Consensus Problems in Networks of Agents with Switching Topology and Time- Delays[J]. IEEE Transactions on Automatic Control. 2004, 49(9): 1520- 1533.

[179] Ni W, Cheng D. Leader- following Consensus of Multi- Agent Systems under Fixed and Switching Topologies[J]. Systems & Control Letters. 2010, 59(3- 4): 209- 217.

[180] Ren W, Beard R W, Mclain T W. Coordination Variables and Consensus

Building in Multiple Vehicle Systems[J]. Lecture Notes in Control and Information Sciences. 2004, 309: 171- 188.

[181] Seo J, et al. Controller Design for UAV Formation Flight Using Consensus based Decentralized Approach[C]. AIAA Aerospace Conference. Seattle, USA, 6- 9 April 2009.

[182] Kuriki Y, Namerikawa T. Formation Control with Collision Avoidance for a Multi- UAV System Using Decentralized MPC and Consensus- Based Control[J]. SICE Journal of Control, Measurement, and System Integration. 2015, 8(4): 285- 294.

[183] Silva T B D, et al. Consensus- based navigation of a UAV formation[C]. 2015 Workshop on Research, Education and Development of Unmanned Aerial Systems. Cancun, Mexico, 23- 25 November 2015: 219- 224.

[184] Dong X, Yu B, Shi Z, et al. Time- Varying Formation Control for Unmanned Aerial Vehicles: Theories and Applications[J]. IEEE Transactions on Control Systems Technology. 2014, 23(1): 340- 348.

[185] 窦立谦, 杨闯, 等. 基于状态观测器的多无人机编队跟踪控制[J]. 天津大学学报(自然科学与工程技术版). 2019, 52(1): 94- 101.

[186] 吴宇, 梁天骄. 基于改进一致性算法的无人机编队控制[J]. 航空学报. 2020, 41(9): 323848- 323848.

[187] 朱旭, 张逊逊, 尤谨语, 等. 基于信息一致性的无人机紧密编队集结控制[J]. 航空学报, 2015, 36(12): 3919- 3929.

[188] 郭伟强. 基于一致性理论的无人机编队控制器设计[D]. 哈尔滨: 哈尔滨工业大学, 2013.

[189] Yan M, et al. Consensus- based three- dimensional multi- UAV formation control strategy with high precision[J]. Frontiers of Information Technology & Electronic Engineering, 2017, 18(7): 968- 977.

[190] 白福忠, 等. 视觉测量技术基础[M]. 北京: 电子工业出版社, 2013.

[191] 徐德, 等. 机器人视觉测量与控制[M]. 北京: 国防工业出版社, 2011.

[192] CCD - Wikipedia. May 2021. https://en.jinzhao.wiki/wiki/CCD.

[193] CMOS - Wikipedia. May 2021. https://en.jinzhao.wiki/wiki/CMOS.

[194] Zhang Z. A Flexible New Technique for Camera Calibration[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence. 2000, 22(11): 1330- 1334.

[195] Camera Link - Wikipedia. May 2021. https://en.jinzhao.wiki/wiki/Camera_Link.

[196] IEEE 1394 - Wikipedia. May 2021. https://en.jinzhao.wiki/wiki/IEEE_1394.

[197] Gigabit Ethernet - Wikipedia. September 2021. https://en.jinzhao.wiki/wiki/Gigabit_Ethernet.

[198] USB - Wikipedia. May 2021. https://en.jinzhao.wiki/wiki/USB.

[199] Serial port - Wikipedia. May 2021. https://en.jinzhao.wiki/wiki/Serial_port.[200] OpenCV - Github. December 2021. https://github.com/opencv.[201] 李素芬. 嵌入式图像识别硬件平台研究[D]. 武汉: 武汉理工大学, 2008. [202] 蒋立丰. 嵌入式图像处理系统的设计与研究[D]. 上海: 东华大学, 2013. [203] 吴健. 基于ARM的嵌入式USB图像采集与处理系统[D]. 合肥: 合肥工业大学, 2012. [204] 鲁亚飞, 吴岸平, 陈清阳. 无人机对地目标多帧融合定位与误差收敛特性分析[J]. 国防科技大学学报. 2021, 43(2): 66- 73. [205] 陈新, 彭科举, 周东翔, 等. 基于优选数据准则的空基多平台协同定位方法[J]. 信号处理. 2010, 26(10): 1466- 1472. [206] 王林. 多无人机协同目标跟踪问题建模与优化技术研究[D]. 长沙: 国防科技大学, 2011. [207] ROS. May 2021. https://www.ros.org/.[208] mavlink/mavros. May 2021. https://github.com/mavlink/mavros.[209] MAVLINK Common Message Set. May 2021. https://mavlink.io/en/messages/common.html.[210] Gazebo. May 2021. http://gazebosim.org/.[211] Liu X, Zhang S, Tian J, Liu L. An Onboard Vision- Based System for Autonomous Landing of a Low- Cost Quadrotor on a Novel Landing Pad[J]. Sensors. 2019; 19(21): 4703. [212] Fitzgibbon A W, Pilu M, Fisher R B. Direct Least Squares Fitting of Ellipses[C]. Proceedings of 13th International Conference on Pattern Recognition. Vienna, Austria, 25- 19 August 1996: 253- 257. [213] Shahar D J. Minimizing the Variance of a Weighted Average[J]. Open Journal of Statistics. 2017, 7(2): 216- 224. [214] ECEF - Wikipedia. March 2021. https://en.jinzhao.wiki/wiki/ECEF.[215] 全权. 多旋翼飞行器设计与控制[M]. 北京: 电子工业出版社, 2018. [216] Supplementary Material (sensors- 19- 04703- s001). September 2021. https://www.mdpi.com/1424- 8220/19/21/4703/s1. [217] Ghose D. True Proportional Navigation with Maneuvering Target[J]. IEEE Transactions on Aerospace and Electronic Systems. 1994, 30(1): 229- 237. [218] Kim B S, Lee J G, Han H S. Biased PNG Law for Impact with Angular Constraint[J]. IEEE Transactions on Aerospace and Electronic Systems. 1998, 34(1): 277- 288. [219] 杨俊鹏, 祝小平, 周洲. 电视制导无人机导引律研究[J]. 西北工业大学学报. 2005, 23(4): 479- 482. [220] 覃天, 陈万春, 邢晓岚. 一种带落角约束的精确导引方法[J]. 宇航学报,

2012, 33(5): 570- 576. [221] Gray R M. Toeplitz and Circulant Matrices: A review[J]. Foundations and Trends in Communications and Information Theory. 2006, 2(3): 155- 239. [222] Rifkin R, Yeo G, et al. Regularized Least- Squares Classification[J]. NATO Science Series Sub Series III Computer and Systems Sciences. 2003, 190: 131- 154. [223] 王耀辉. 基于机载视觉的空中目标跟踪及相对姿态估计[D]. 长沙: 国防科技大学, 2016. [224] Bao C, Wu Y, et al. Real Time Robust L1 Tracker Using Accelerated Proximal Gradient Approach[C]. IEEE Conference on Computer Vision and Pattern Recognition. Providence, USA, 16- 21 June 2012: 1830- 1837. [225] Wu Y, et al. Online Robust Image Alignment via Iterative Convex Optimization[C]. IEEE Conference on Computer Vision and Pattern Recognition. Providence, USA, 16- 21 June 2012: 1808- 1814. [226] Zhang K, Zhang L, et al. Real- Time Compressive Tracking[C]. The 12th European conference on Computer Vision. Florence, Italy, 7- 13 October 2012: 866- 879. [227] 焦昌勇. 内河航道目标的特征识别与分类算法研究与实现[D]. 武汉: 武汉理工大学, 2013. [228] 阮激扬. 基于YOLO的目标检测算法设计与实现[D]. 北京: 北京邮电大学, 2019. [229] Simonyan K, Zisserman A. Very Deep Convolutional Networks for Large- Scale Image Recognition[C]. International Conference on Learning Representations. San Diego, USA, 7- 9 May 2015. [230] Jia D, Wei D, Socher R, et al. ImageNet: A Large- Scale Hierarchical Image Database[C]. IEEE Conference on Computer Vision and Pattern Recognition. Miami, USA, 20- 25 June 2009: 248- 255. [231] He K, Zhang X, et al. Deep Residual Learning for Image Recognition[C]. IEEE Conference on Computer Vision and Pattern Recognition. Las Vegas, USA, 27- 30 June 2016: 770- 778. [232] Lin T, Maire M, Belongie S, et al. Microsoft COCO: Common Objects in Context[C]. The 13th European Conference on Computer Vision. Zurich, Switzerland, 6- 12 September 2014. [233] Bochkovskiy A, Wang C, et al. YOLOv4: Optimal Speed and Accuracy of Object Detection[Z]. ArXiv, 2020. [234] Wang C, et al. CSPNet: A New Backbone that can Enhance Learning Capability of CNN[Z]. ArXiv, 2019. [235] Lan Y, Peng K, Zhang W, Liu X. Image Terminal Guidance Based on YOLO V2 Framework[C]. 4th International- Academy- of- Astronautics Conference on

Dynamics and Control of Space Systems. Changsha, China, 21- 23 May 2018: 651- 666. [236] GitHub - labeling. May 2021. https://github.com/tzutalin/labelImg.[237] 兰奕星. 基于深度学习的船只目标检测与视觉测量研究[D]. 长沙: 国防科技大学, 2018. [238] 傅小明. 复杂天气下飞行目标物识别跟踪技术的研究[D]. 成都: 电子科技大学, 2020. [239] 张平, 等. 机载/弹载视觉导引稳定平台的建模与控制[M]. 北京: 国防工业出版社, 2011. [240] Tan R, Kumar M. Tracking of Ground Mobile Targets by Quadrotor Unmanned Aerial Vehicles[J]. Unmanned Systems. 2014, 2(2): 157- 173. [241] Liu X, Yang Y, Ma C, Li J, Zhang S. Real- Time Visual Tracking of Moving Targets Using a Low- Cost Unmanned Aerial Vehicle with a 3- Axis Stabilized Gimbal System[J]. Applied Sciences. 2020, 10(15): 5064. [242] Supplementary Material (applsci- 10- 05064- s001). September 2021. https://www.mdpi.com/2076- 3417/10/15/5064/s1. [243] 刘轩岑, 吴航劲, 杨跃能, 张士峰, 江振宇, 刘龙斌. 一种用于四旋翼无人机的机械臂[P]. 中国专利: ZL 202110072191.2, 2021- 04- 02. [244] 田嘉懿. 低成本全捷联微型导弹制导控制技术研究[D]. 长沙: 国防科技大学, 2019. [245] Ren W. Distributed Consensus in Multi- Vehicle Cooperative Control: Theory and Applications (多航行体协同控制中的分布式一致性——理论与应用)[M]. 吴晓锋译. 北京: 电子工业出版社, 2014. [246] 薛瑞彬. 多无人机分布式协同编队飞行控制技术研究[D]. 北京: 北京理工大学, 2016. [247] 方振平, 等. 航空飞行器飞行动力学[M]. 北京: 北京航空航天大学出版社, 2005. [248] 吴树范, 等. 飞机总能量控制系统的研究I——原理分析与系统设计[J]. 航空学报. 1993, 14(7): 335- 361. [249] Park S, et al. A New Nonlinear Guidance Logic for Trajectory Tracking[C]. AIAA Guidance, Navigation, and Control Conference and Exhibit. Providence, USA, 16- 19 August 2004. [250] Liu X, et al. A Low- Cost Solution for Leader- Follower Formation Control of Multi- UAV System Based on Pixhawk[C]. 3rd International Symposium on Power Electronics and Control Engineering. Chongqing, China, 27- 29 November 2020: 012081.